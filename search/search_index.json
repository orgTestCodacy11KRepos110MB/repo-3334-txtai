{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Build AI-powered semantic search applications</p> <p> </p> <p>txtai executes machine-learning workflows to transform data and build AI-powered semantic search applications.</p> <p></p> <p>Traditional search systems use keywords to find data. Semantic search applications have an understanding of natural language and identify results that have the same meaning, not necessarily the same keywords.</p> <p> </p> <p>Backed by state-of-the-art machine learning models, data is transformed into vector representations for search (also known as embeddings). Innovation is happening at a rapid pace, models can understand concepts in documents, audio, images and more.</p> <p>Summary of txtai features:</p> <ul> <li>\ud83d\udd0e Large-scale similarity search with multiple index backends (Faiss, Annoy, Hnswlib)</li> <li>\ud83d\udcc4 Create embeddings for text snippets, documents, audio, images and video. Supports transformers and word vectors.</li> <li>\ud83d\udca1 Machine-learning pipelines to run extractive question-answering, zero-shot labeling, transcription, translation, summarization and text extraction</li> <li>\u21aa\ufe0f\ufe0f Workflows that join pipelines together to aggregate business logic. txtai processes can be microservices or full-fledged indexing workflows.</li> <li>\u2699\ufe0f Build with Python or YAML. API bindings available for JavaScript, Java, Rust and Go.</li> <li>\u2601\ufe0f Cloud-native architecture that scales out with container orchestration systems (e.g. Kubernetes)</li> </ul> <p>Applications range from similarity search to complex NLP-driven data extractions to generate structured databases. Semantic workflows transform and find data driven by user intent.</p> <p> </p> <p>The following applications are powered by txtai.</p> <p></p> Application Description paperai Semantic search and workflows for medical/scientific papers codequestion Semantic search for developers tldrstory Semantic search for headlines and story text neuspo Fact-driven, real-time sports event and news site <p>txtai is built with Python 3.7+, Hugging Face Transformers, Sentence Transformers and FastAPI</p>"},{"location":"cloud/","title":"Cloud","text":"<p>Scalable cloud-native applications can be built with txtai. The following container runtimes are supported.</p> <ul> <li>Container Orchestration Systems (i.e. Kubernetes)</li> <li>Docker Engine</li> <li>Serverless Compute</li> </ul> <p>Images for txtai are available on Docker Hub for CPU and GPU installs. The CPU install is recommended when GPUs aren't available given the image is half the size.</p> <p>The base txtai images have no models installed and models will be downloaded each time the container starts. Caching the models is recommended as that will significantly reduce container start times. This can be done a couple different ways.</p> <ul> <li>Create a container with the models cached</li> <li>Set the transformers cache environment variable and mount that volume when starting the image     <pre><code>docker run -v &lt;local dir&gt;:/models -e TRANSFORMERS_CACHE=/models --rm -it &lt;docker image&gt;\n</code></pre></li> </ul>"},{"location":"cloud/#build-txtai-images","title":"Build txtai images","text":"<p>The txtai images found on Docker Hub are configured to support most situations. This image can be locally built with different options as desired.</p> <p>Examples build commands below.</p> <pre><code># Get Dockerfile\nwget https://raw.githubusercontent.com/neuml/txtai/master/docker/base/Dockerfile\n\n# Build Ubuntu 18.04 image running Python 3.7\ndocker build -t txtai --build-arg BASE_IMAGE=ubuntu:18.04 --build-arg PYTHON_VERSION=3.7 .\n\n# Build image with GPU support\ndocker build -t txtai --build-arg GPU=1 .\n\n# Build minimal image with the base txtai components\ndocker build -t txtai --build-arg COMPONENTS= .\n</code></pre>"},{"location":"cloud/#container-image-model-caching","title":"Container image model caching","text":"<p>As mentioned previously, model caching is recommended to reduce container start times. The following commands demonstrate this. In all cases, it is assumed a config.yml file is present in the local directory with the desired configuration set.</p>"},{"location":"cloud/#api","title":"API","text":"<p>This section builds an image that caches models and starts an API service. The config.yml file should be configured with the desired components to expose via the API.</p> <p>The following is a sample config.yml file that creates an Embeddings API service.</p> <pre><code># config.yml\nwritable: true\n\nembeddings:\npath: sentence-transformers/nli-mpnet-base-v2\ncontent: true\n</code></pre> <p>The next section builds the image and starts an instance.</p> <pre><code># Get Dockerfile\nwget https://raw.githubusercontent.com/neuml/txtai/master/docker/api/Dockerfile\n\n# CPU build\ndocker build -t txtai-api .\n\n# GPU build\ndocker build -t txtai-api --build-arg BASE_IMAGE=neuml/txtai-gpu .\n\n# Run\ndocker run -p 8000:8000 --rm -it txtai-api\n</code></pre>"},{"location":"cloud/#service","title":"Service","text":"<p>This section builds a scheduled workflow service. More on scheduled workflows can be found here.</p> <pre><code># Get Dockerfile\nwget https://raw.githubusercontent.com/neuml/txtai/master/docker/service/Dockerfile\n\n# CPU build\ndocker build -t txtai-service .\n\n# GPU build\ndocker build -t txtai-service --build-arg BASE_IMAGE=neuml/txtai-gpu .\n\n# Run\ndocker run --rm -it txtai-service\n</code></pre>"},{"location":"cloud/#workflow","title":"Workflow","text":"<p>This section builds a single run workflow. Example workflows can be found here.</p> <pre><code># Get Dockerfile\nwget https://raw.githubusercontent.com/neuml/txtai/master/docker/workflow/Dockerfile\n\n# CPU build\ndocker build -t txtai-workflow . # GPU build\ndocker build -t txtai-workflow --build-arg BASE_IMAGE=neuml/txtai-gpu .\n\n# Run\ndocker run --rm -it txtai-workflow &lt;workflow name&gt; &lt;workflow parameters&gt;\n</code></pre>"},{"location":"cloud/#serverless-compute","title":"Serverless Compute","text":"<p>One of the most powerful features of txtai is building YAML-configured applications with the \"build once, run anywhere\" approach. API instances and workflows can run locally, on a server, on a cluster or serverless.</p> <p>Serverless instances of txtai are supported on frameworks such as AWS Lambda, Google Cloud Functions, Azure Cloud Functions and Kubernetes with Knative.</p>"},{"location":"cloud/#aws-lambda","title":"AWS Lambda","text":"<p>The following steps show a basic example of how to build a serverless API instance with AWS SAM.</p> <ul> <li>Create config.yml and template.yml</li> </ul> <pre><code># config.yml\nwritable: true\n\nembeddings:\npath: sentence-transformers/nli-mpnet-base-v2\ncontent: true\n</code></pre> <pre><code># template.yml\nResources:\ntxtai:\nType: AWS::Serverless::Function\nProperties:\nPackageType: Image\nMemorySize: 3000\nTimeout: 20\nEvents:\nApi:\nType: Api\nProperties:\nPath: \"/{proxy+}\"\nMethod: ANY\nMetadata:\nDockerfile: Dockerfile\nDockerContext: ./\nDockerTag: api\n</code></pre> <ul> <li> <p>Install AWS SAM</p> </li> <li> <p>Run following</p> </li> </ul> <pre><code># Get Dockerfile and application\nwget https://raw.githubusercontent.com/neuml/txtai/master/docker/aws/api.py\nwget https://raw.githubusercontent.com/neuml/txtai/master/docker/aws/Dockerfile\n\n# Build the docker image\nsam build\n\n# Start API gateway and Lambda instance locally\nsam local start-api -p 8000 --warm-containers LAZY\n\n# Verify instance running (should return 0)\ncurl http://localhost:8080/count\n</code></pre> <p>If successful, a local API instance is now running in a \"serverless\" fashion. This configuration can be deployed to AWS using SAM. See this link for more information.</p>"},{"location":"cloud/#kubernetes-with-knative","title":"Kubernetes with Knative","text":"<p>txtai scales with container orchestration systems. This can be self-hosted or with a cloud provider such as Amazon Elastic Kubernetes Service, Google Kubernetes Engine and Azure Kubernetes Service. There are also other smaller providers with a managed Kubernetes offering.</p> <p>A full example covering how to build a serverless txtai application on Kubernetes with Knative can be found here.</p>"},{"location":"examples/","title":"Examples","text":"<p>The examples directory has a series of notebooks and applications giving an overview of txtai. See the sections below.</p>"},{"location":"examples/#semantic-search","title":"Semantic Search","text":"<p>Build semantic/similarity/vector/neural search applications.</p> Notebook Description Introducing txtai \u25b6\ufe0f Overview of the functionality provided by txtai Build an Embeddings index with Hugging Face Datasets Index and search Hugging Face Datasets Build an Embeddings index from a data source Index and search a data source with word embeddings Add semantic search to Elasticsearch Add semantic search to existing search systems Similarity search with images Embed images and text into the same space for search Distributed embeddings cluster Distribute an embeddings index across multiple data nodes What's new in txtai 4.0 Content storage, SQL, object storage, reindex and compressed indexes Anatomy of a txtai index Deep dive into the file formats behind a txtai embeddings index Custom Embeddings SQL functions Add user-defined functions to Embeddings SQL Model explainability Explainability for semantic search Query translation Domain-specific natural language queries with query translation Build a QA database Question matching with semantic search Embeddings components Composable search with vector, SQL and scoring components Semantic Graphs Explore topics, data connectivity and run network analysis Topic Modeling with BM25 Topic modeling backed by a BM25 index"},{"location":"examples/#pipelines","title":"Pipelines","text":"<p>Transform data with NLP-backed pipelines.</p> Notebook Description Extractive QA with txtai Introduction to extractive question-answering with txtai Extractive QA with Elasticsearch Run extractive question-answering queries with Elasticsearch Extractive QA to build structured data Build structured datasets using extractive question-answering Apply labels with zero shot classification Use zero shot learning for labeling, classification and topic modeling Building abstractive text summaries Run abstractive text summarization Extract text from documents Extract text from PDF, Office, HTML and more Text to speech generation Generate speech from text Transcribe audio to text Convert audio files to text Translate text between languages Streamline machine translation and language detection Generate image captions and detect objects Captions and object detection for images Near duplicate image detection Identify duplicate and near-duplicate images API Gallery Using txtai in JavaScript, Java, Rust and Go"},{"location":"examples/#workflows","title":"Workflows","text":"<p>Efficiently process data at scale.</p> Notebook Description Run pipeline workflows \u25b6\ufe0f Simple yet powerful constructs to efficiently process data Transform tabular data with composable workflows Transform, index and search tabular data Tensor workflows Performant processing of large tensor arrays Entity extraction workflows Identify entity/label combinations Workflow Scheduling Schedule workflows with cron expressions Push notifications with workflows Generate and push notifications with workflows Pictures are a worth a thousand words Generate webpage summary images with DALL-E mini Run txtai with native code Execute workflows in native code with the Python C API"},{"location":"examples/#model-training","title":"Model Training","text":"<p>Train NLP models.</p> Notebook Description Train a text labeler Build text sequence classification models Train without labels Use zero-shot classifiers to train new models Train a QA model Build and fine-tune question-answering models Train a language model from scratch Build new language models Export and run models with ONNX Export models with ONNX, run natively in JavaScript, Java and Rust Export and run other machine learning models Export and run models from scikit-learn, PyTorch and more"},{"location":"examples/#applications","title":"Applications","text":"<p>Series of example applications with txtai. Links to hosted versions on Hugging Face Spaces also provided.</p> Application Description Basic similarity search Basic similarity search example. Data from the original txtai demo. \ud83e\udd17 Book search Book similarity search application. Index book descriptions and query using natural language statements. Local run only Image search Image similarity search application. Index a directory of images and run searches to identify images similar to the input query. \ud83e\udd17 Summarize an article Summarize an article. Workflow that extracts text from a webpage and builds a summary. \ud83e\udd17 Wiki search Wikipedia search application. Queries Wikipedia API and summarizes the top result. \ud83e\udd17 Workflow builder Build and execute txtai workflows. Connect summarization, text extraction, transcription, translation and similarity search pipelines together to run unified workflows. \ud83e\udd17"},{"location":"faq/","title":"FAQ","text":"<p>Below is a list of frequently asked questions and common issues encountered.</p> <p>Issue</p> <p>Embeddings query errors like this:</p> <pre><code>SQLError: no such function: json_extract\n</code></pre> <p>Solution</p> <p>Upgrade Python version as it doesn't have SQLite support for json_extract</p> <p>Issue</p> <p>Segmentation faults and similar errors on macOS</p> <p>Solution</p> <p>Downgrade PyTorch to &lt;= 1.12. See issue #377 for more on this issue. </p> <p>Issue</p> <p><code>ContextualVersionConflict</code> exception when importing certain libraries while running one of the examples notebooks on Google Colab</p> <p>Solution</p> <p>Restart the kernel. See issue #409 for more on this issue. </p>"},{"location":"further/","title":"Further reading","text":"<ul> <li>Introducing txtai, AI-powered semantic search built on Transformers</li> <li>Tutorial series on Hashnode | dev.to</li> <li>What's new in txtai 5.0 | 4.0</li> <li>Getting started with semantic search</li> <li>Run machine-learning workflows to transform data and build AI-powered semantic search applications with txtai</li> <li>Semantic search on the cheap</li> <li>Serverless vector search with txtai</li> <li>Insights from the txtai console</li> </ul>"},{"location":"install/","title":"Installation","text":"<p>The easiest way to install is via pip and PyPI</p> <pre><code>pip install txtai\n</code></pre> <p>Python 3.7+ is supported. Using a Python virtual environment is recommended.</p>"},{"location":"install/#optional-dependencies","title":"Optional dependencies","text":"<p>txtai has the following optional dependencies that can be installed as extras. The patterns below are supported in setup.py install_requires sections.</p> <p>Note: Extras are provided for convenience. Alternatively, individual packages can be installed to limit dependencies.</p>"},{"location":"install/#all","title":"All","text":"<p>Install all dependencies.</p> <pre><code>pip install txtai[all]\n</code></pre>"},{"location":"install/#api","title":"API","text":"<p>Serve txtai via a web API.</p> <pre><code>pip install txtai[api]\n</code></pre>"},{"location":"install/#cloud","title":"Cloud","text":"<p>Interface with cloud compute.</p> <pre><code>pip install txtai[cloud]\n</code></pre>"},{"location":"install/#console","title":"Console","text":"<p>Command line index query console.</p> <pre><code>pip install txtai[console]\n</code></pre>"},{"location":"install/#database","title":"Database","text":"<p>Additional content storage options.</p> <pre><code>pip install txtai[database]\n</code></pre>"},{"location":"install/#graph","title":"Graph","text":"<p>Topic modeling, data connectivity and network analysis.</p> <pre><code>pip install txtai[graph]\n</code></pre>"},{"location":"install/#model","title":"Model","text":"<p>Additional non-standard models.</p> <pre><code>pip install txtai[model]\n</code></pre>"},{"location":"install/#pipeline","title":"Pipeline","text":"<p>All pipelines - default install comes with most common pipelines.</p> <pre><code>pip install txtai[pipeline]\n</code></pre> <p>More granular extras are available for pipeline categories: <code>pipeline-audio</code>, <code>pipeline-data</code>, <code>pipeline-image</code>, <code>pipeline-text</code>, and <code>pipeline-train</code>.</p>"},{"location":"install/#similarity","title":"Similarity","text":"<p>Word vectors, support for sentence-transformers models not on the HF Hub and additional ANN libraries.</p> <pre><code>pip install txtai[similarity]\n</code></pre>"},{"location":"install/#workflow","title":"Workflow","text":"<p>All workflow tasks - default install comes with most common workflow tasks.</p> <pre><code>pip install txtai[workflow]\n</code></pre>"},{"location":"install/#combining-dependencies","title":"Combining dependencies","text":"<p>Multiple dependencies can be specified at the same time.</p> <pre><code>pip install txtai[pipeline,workflow]\n</code></pre>"},{"location":"install/#environment-specific-prerequisites","title":"Environment specific prerequisites","text":"<p>Additional environment specific prerequisites are below.</p>"},{"location":"install/#linux","title":"Linux","text":"<p>Optional audio transcription requires a system library to be installed</p>"},{"location":"install/#macos","title":"macOS","text":"<p>Run <code>brew install libomp</code> see this link</p>"},{"location":"install/#windows","title":"Windows","text":"<p>Optional dependencies require C++ Build Tools</p> <p>The txtai build workflow occasionally has work arounds for other known but temporary dependency issues. The FAQ also has a list of common problems, including common installation issues.</p>"},{"location":"install/#install-from-source","title":"Install from source","text":"<p>txtai can also be installed directly from GitHub to access the latest, unreleased features.</p> <pre><code>pip install git+https://github.com/neuml/txtai\n</code></pre>"},{"location":"install/#conda","title":"Conda","text":"<p>A community-supported txtai package is available via conda-forge.</p> <pre><code>conda install -c conda-forge txtai\n</code></pre>"},{"location":"install/#run-with-containers","title":"Run with containers","text":"<p>Docker images are available for txtai. See this section for more information on container-based installs.</p>"},{"location":"why/","title":"Why txtai?","text":"<p>In addition to traditional search systems, a growing number of semantic search solutions are available, so why txtai?</p> <ul> <li>Up and running in minutes with pip or Docker <pre><code># Get started in a couple lines\nfrom txtai.embeddings import Embeddings\n\nembeddings = Embeddings({\"path\": \"sentence-transformers/all-MiniLM-L6-v2\"})\nembeddings.index([(0, \"Correct\", None), (1, \"Not what we hoped\", None)])\nembeddings.search(\"positive\", 1)\n#[(0, 0.2986203730106354)]\n</code></pre></li> <li>Build applications in your programming language of choice via the API <pre><code># app.yml\nembeddings:\npath: sentence-transformers/all-MiniLM-L6-v2\n</code></pre> <pre><code>CONFIG=app.yml uvicorn \"txtai.api:app\"\ncurl -X GET \"http://localhost:8000/search?query=positive\"\n</code></pre></li> <li>Connect machine learning models together to build intelligent data processing workflows</li> <li>Works with both small and big data - scale when needed</li> <li>Low footprint - install additional dependencies when you need them</li> <li>Learn by example - notebooks cover all available functionality</li> </ul>"},{"location":"api/","title":"API","text":"<p>txtai has a full-featured API, backed by FastAPI, that can optionally be enabled for any txtai process. All functionality found in txtai can be accessed via the API.</p> <p>The following is an example configuration and startup script for the API.</p> <p>Note: This configuration file enables all functionality. For memory-bound systems, splitting pipelines into multiple instances is a best practice.</p> <pre><code># Index file path\npath: /tmp/index\n\n# Allow indexing of documents\nwritable: True\n\n# Enbeddings index\nembeddings:\npath: sentence-transformers/nli-mpnet-base-v2\n\n# Extractive QA\nextractor:\npath: distilbert-base-cased-distilled-squad\n\n# Zero-shot labeling\nlabels:\n\n# Similarity\nsimilarity:\n\n# Text segmentation\nsegmentation:\nsentences: true\n\n# Text summarization\nsummary:\n\n# Text extraction\ntextractor:\nparagraphs: true\nminlength: 100\njoin: true\n\n# Transcribe audio to text\ntranscription:\n\n# Translate text between languages\ntranslation:\n\n# Workflow definitions\nworkflow:\nsumfrench:\ntasks:\n- action: textractor\ntask: url\n- action: summary\n- action: translation\nargs: [\"fr\"]\nsumspanish:\ntasks:\n- action: textractor\ntask: url\n- action: summary\n- action: translation\nargs: [\"es\"]\n</code></pre> <p>Assuming this YAML content is stored in a file named config.yml, the following command starts the API process.</p> <pre><code>CONFIG=config.yml uvicorn \"txtai.api:app\"\n</code></pre> <p>uvicorn is a full-featured production ready server with support for SSL and more. See the uvicorn deployment guide for details.</p>"},{"location":"api/#connect-to-api","title":"Connect to API","text":"<p>The default port for the API is 8000. See the uvicorn link above to change this.</p> <p>txtai has a number of language bindings which abstract the API (see links below). Alternatively, code can be written to connect directly to the API. Documentation for a live running instance can be found at the <code>/docs</code> url (i.e. http://localhost:8000/docs). The following example runs a workflow using cURL.</p> <pre><code>curl \\\n-X POST \"http://localhost:8000/workflow\" \\\n-H \"Content-Type: application/json\" \\\n-d '{\"name\":\"sumfrench\", \"elements\": [\"https://github.com/neuml/txtai\"]}'\n</code></pre>"},{"location":"api/#local-instance","title":"Local instance","text":"<p>A local instance can be instantiated. In this case, a txtai application runs internally, without any network connections, providing the same consolidated functionality. This enables running txtai in Python with configuration.</p> <p>The configuration above can be run in Python with:</p> <pre><code>from txtai.app import Application\n\n# Load and run workflow\napp = Application(config.yml)\napp.workflow(\"sumfrench\", [\"https://github.com/neuml/txtai\"])\n</code></pre> <p>See this link for a full list of methods.</p>"},{"location":"api/#run-with-containers","title":"Run with containers","text":"<p>The API can be containerized and run. This will bring up an API instance without having to install Python, txtai or any dependencies on your machine!</p> <p>See this section for more information.</p>"},{"location":"api/#supported-language-bindings","title":"Supported language bindings","text":"<p>The following programming languages have bindings with the txtai API:</p> <ul> <li>JavaScript</li> <li>Java</li> <li>Rust</li> <li>Go</li> </ul> <p>See the link below for a detailed example covering how to use the API.</p> Notebook Description API Gallery Using txtai in JavaScript, Java, Rust and Go"},{"location":"api/cluster/","title":"Distributed embeddings clusters","text":"<p>The API supports combining multiple API instances into a single logical embeddings index. An example configuration is shown below.</p> <pre><code>cluster:\nshards:\n- http://127.0.0.1:8002\n- http://127.0.0.1:8003\n</code></pre> <p>This configuration aggregates the API instances above as index shards. Data is evenly split among each of the shards at index time. Queries are run in parallel against each shard and the results are joined together. This method allows horizontal scaling and supports very large index clusters.</p> <p>This method is only recommended for data sets in the 1 billion+ records. The ANN libraries can easily support smaller data sizes and this method is not worth the additional complexity. At this time, new shards can not be added after building the initial index.</p> <p>See the link below for a detailed example covering distributed embeddings clusters.</p> Notebook Description Distributed embeddings cluster Distribute an embeddings index across multiple data nodes"},{"location":"api/configuration/","title":"Configuration","text":"<p>Configuration is set through YAML. In most cases, YAML keys map to fields names in Python. The example in the previous section gave a full-featured example covering a wide array of configuration options.</p> <p>Each section below describes the available configuration settings.</p>"},{"location":"api/configuration/#embeddings","title":"Embeddings","text":"<p>The configuration parser expects a top level <code>embeddings</code> key to be present in the YAML. All embeddings configuration is supported.</p> <p>The following example defines an embeddings index.</p> <pre><code>path: index path\nwritable: true\n\nembeddings:\npath: vector model\ncontent: true\n</code></pre> <p>Three top level settings are available to control where indexes are saved and if an index is a read-only index.</p>"},{"location":"api/configuration/#path","title":"path","text":"<pre><code>path: string\n</code></pre> <p>Path to save and load the embeddings index. Each API instance can only access a single index at a time.</p>"},{"location":"api/configuration/#writable","title":"writable","text":"<pre><code>writable: boolean\n</code></pre> <p>Determines if the input embeddings index is writable (true) or read-only (false). This allows serving a read-only index.</p>"},{"location":"api/configuration/#cloud","title":"cloud","text":"<p>Cloud storage settings can be set under a <code>cloud</code> top level configuration group.</p>"},{"location":"api/configuration/#pipeline","title":"Pipeline","text":"<p>Pipelines are loaded as top level configuration parameters. Pipeline names are automatically detected in the YAML configuration and created upon startup. All pipelines are supported.</p> <p>The following example defines a series of pipelines. Note that entries below are the lower-case names of the pipeline class.</p> <pre><code>caption:\n\nextractor:\npath: model path\n\nlabels:\n\nsummary:\n\ntabular:\n\ntranslation:\n</code></pre> <p>Under each pipeline name, configuration settings for the pipeline can be set.</p>"},{"location":"api/configuration/#workflow","title":"Workflow","text":"<p>Workflows are defined under a top level <code>workflow</code> key. Each key under the <code>workflow</code> key is the name of the workflow. Under that is a <code>tasks</code> key with each task definition.</p> <p>The following example defines a workflow.</p> <pre><code>workflow:\nsumtranslate:\ntasks:\n- action: summary\n- action: translation\n</code></pre>"},{"location":"api/configuration/#schedule","title":"schedule","text":"<p>Schedules a workflow using a cron expression.</p> <pre><code>workflow:\nindex:\nschedule:\ncron: 0/10 * * * * *\nelements: [\"api params\"] tasks:\n- task: service\nurl: api url\n- action: index\n</code></pre>"},{"location":"api/configuration/#tasks","title":"tasks","text":"<pre><code>tasks: list\n</code></pre> <p>Expects a list of workflow tasks. Each element defines a single workflow task. All task configuration is supported.</p> <p>A shorthand syntax for creating tasks is supported. This syntax will automatically map task strings to an <code>action:value</code> pair.</p> <p>Example below.</p> <pre><code>workflow:\nindex:\ntasks:\n- action1\n- action2\n</code></pre> <p>Each task element supports the following additional arguments.</p>"},{"location":"api/configuration/#action","title":"action","text":"<pre><code>action: string|list\n</code></pre> <p>Both single and multi-action tasks are supported.</p> <p>The action parameter works slightly different when passed via configuration. The parameter(s) needs to be converted into callable method(s). If action is a pipeline that has been defined in the current configuration, it will use that pipeline as the action.</p> <p>There are three special action names <code>index</code>, <code>upsert</code> and <code>search</code>. If <code>index</code> or <code>upsert</code> are used as the action, the task will collect workflow data elements and load them into defined the embeddings index. If <code>search</code> is used, the task will execute embeddings queries for each input data element.</p> <p>Otherwise, the action must be a path to a callable object or function. The configuration parser will resolve the function name and use that as the task action.</p>"},{"location":"api/configuration/#task","title":"task","text":"<pre><code>task: string\n</code></pre> <p>Optionally sets the type of task to create. For example, this could be a <code>file</code> task or a <code>retrieve</code> task. If this is not specified, a generic task is created. The list of workflow tasks can be found here.</p>"},{"location":"api/configuration/#args","title":"args","text":"<pre><code>args: list\n</code></pre> <p>Optional list of static arguments to pass to the workflow task. These are combined with workflow data to pass to each <code>__call__</code>.</p>"},{"location":"api/methods/","title":"Methods","text":""},{"location":"api/methods/#txtai.api.base.API","title":"<code> API            (Application)         </code>","text":"<p>Base API template. The API is an extended txtai application, adding the ability to cluster API instances together.</p> <p>Downstream applications can extend this base template to add/modify functionality.</p> Source code in <code>txtai/api/base.py</code> <pre><code>class API(Application):\n\"\"\"\n    Base API template. The API is an extended txtai application, adding the ability to cluster API instances together.\n\n    Downstream applications can extend this base template to add/modify functionality.\n    \"\"\"\n\n    def __init__(self, config, loaddata=True):\n        super().__init__(config, loaddata)\n\n        # Embeddings cluster\n        self.cluster = None\n        if self.config.get(\"cluster\"):\n            self.cluster = Cluster(self.config[\"cluster\"])\n\n    # pylint: disable=W0221\n    def search(self, query, request=None, limit=None):\n        # When search is invoked via the API, limit is set from the request\n        # When search is invoked directly, limit is set using the method parameter\n        limit = self.limit(request.query_params.get(\"limit\") if request and hasattr(request, \"query_params\") else limit)\n\n        if self.cluster:\n            return self.cluster.search(query, limit)\n\n        return super().search(query, limit)\n\n    def batchsearch(self, queries, limit=None):\n        if self.cluster:\n            return self.cluster.batchsearch(queries, self.limit(limit))\n\n        return super().batchsearch(queries, limit)\n\n    def add(self, documents):\n\"\"\"\n        Adds a batch of documents for indexing.\n\n        Downstream applications can override this method to also store full documents in an external system.\n\n        Args:\n            documents: list of {id: value, text: value}\n\n        Returns:\n            unmodified input documents\n        \"\"\"\n\n        if self.cluster:\n            self.cluster.add(documents)\n        else:\n            super().add(documents)\n\n        return documents\n\n    def index(self):\n\"\"\"\n        Builds an embeddings index for previously batched documents.\n        \"\"\"\n\n        if self.cluster:\n            self.cluster.index()\n        else:\n            super().index()\n\n    def upsert(self):\n\"\"\"\n        Runs an embeddings upsert operation for previously batched documents.\n        \"\"\"\n\n        if self.cluster:\n            self.cluster.upsert()\n        else:\n            super().upsert()\n\n    def delete(self, ids):\n\"\"\"\n        Deletes from an embeddings index. Returns list of ids deleted.\n\n        Args:\n            ids: list of ids to delete\n\n        Returns:\n            ids deleted\n        \"\"\"\n\n        if self.cluster:\n            return self.cluster.delete(ids)\n\n        return super().delete(ids)\n\n    def count(self):\n\"\"\"\n        Total number of elements in this embeddings index.\n\n        Returns:\n            number of elements in embeddings index\n        \"\"\"\n\n        if self.cluster:\n            return self.cluster.count()\n\n        return super().count()\n\n    def limit(self, limit):\n\"\"\"\n        Parses the number of results to return from the request. Allows range of 1-250, with a default of 10.\n\n        Args:\n            limit: limit parameter\n\n        Returns:\n            bounded limit\n        \"\"\"\n\n        # Return between 1 and 250 results, defaults to 10\n        return max(1, min(250, int(limit) if limit else 10))\n</code></pre>"},{"location":"api/methods/#txtai.api.base.API.add","title":"<code>add(self, documents)</code>","text":"<p>Adds a batch of documents for indexing.</p> <p>Downstream applications can override this method to also store full documents in an external system.</p> <p>Parameters:</p> Name Type Description Default <code>documents</code> <p>list of {id: value, text: value}</p> required <p>Returns:</p> Type Description <p>unmodified input documents</p> Source code in <code>txtai/api/base.py</code> <pre><code>def add(self, documents):\n\"\"\"\n    Adds a batch of documents for indexing.\n\n    Downstream applications can override this method to also store full documents in an external system.\n\n    Args:\n        documents: list of {id: value, text: value}\n\n    Returns:\n        unmodified input documents\n    \"\"\"\n\n    if self.cluster:\n        self.cluster.add(documents)\n    else:\n        super().add(documents)\n\n    return documents\n</code></pre>"},{"location":"api/methods/#txtai.api.base.API.batchsearch","title":"<code>batchsearch(self, queries, limit=None)</code>","text":"<p>Finds documents in the embeddings model most similar to the input queries. Returns a list of {id: value, score: value} sorted by highest score per query, where id is the document id in the embeddings model.</p> <p>Parameters:</p> Name Type Description Default <code>queries</code> <p>queries text</p> required <code>limit</code> <p>maximum results</p> <code>None</code> <p>Returns:</p> Type Description <code>list of {id</code> <p>value, score: value} per query</p> Source code in <code>txtai/api/base.py</code> <pre><code>def batchsearch(self, queries, limit=None):\n    if self.cluster:\n        return self.cluster.batchsearch(queries, self.limit(limit))\n\n    return super().batchsearch(queries, limit)\n</code></pre>"},{"location":"api/methods/#txtai.api.base.API.count","title":"<code>count(self)</code>","text":"<p>Total number of elements in this embeddings index.</p> <p>Returns:</p> Type Description <p>number of elements in embeddings index</p> Source code in <code>txtai/api/base.py</code> <pre><code>def count(self):\n\"\"\"\n    Total number of elements in this embeddings index.\n\n    Returns:\n        number of elements in embeddings index\n    \"\"\"\n\n    if self.cluster:\n        return self.cluster.count()\n\n    return super().count()\n</code></pre>"},{"location":"api/methods/#txtai.api.base.API.delete","title":"<code>delete(self, ids)</code>","text":"<p>Deletes from an embeddings index. Returns list of ids deleted.</p> <p>Parameters:</p> Name Type Description Default <code>ids</code> <p>list of ids to delete</p> required <p>Returns:</p> Type Description <p>ids deleted</p> Source code in <code>txtai/api/base.py</code> <pre><code>def delete(self, ids):\n\"\"\"\n    Deletes from an embeddings index. Returns list of ids deleted.\n\n    Args:\n        ids: list of ids to delete\n\n    Returns:\n        ids deleted\n    \"\"\"\n\n    if self.cluster:\n        return self.cluster.delete(ids)\n\n    return super().delete(ids)\n</code></pre>"},{"location":"api/methods/#txtai.api.base.API.index","title":"<code>index(self)</code>","text":"<p>Builds an embeddings index for previously batched documents.</p> Source code in <code>txtai/api/base.py</code> <pre><code>def index(self):\n\"\"\"\n    Builds an embeddings index for previously batched documents.\n    \"\"\"\n\n    if self.cluster:\n        self.cluster.index()\n    else:\n        super().index()\n</code></pre>"},{"location":"api/methods/#txtai.api.base.API.search","title":"<code>search(self, query, request=None, limit=None)</code>","text":"<p>Finds documents in the embeddings model most similar to the input query. Returns a list of {id: value, score: value} sorted by highest score, where id is the document id in the embeddings model.</p> <p>Downstream applications can override this method to provide enriched search results.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <p>query text</p> required <code>limit</code> <p>maximum results, used if request is None</p> <code>None</code> <p>Returns:</p> Type Description <code>list of {id</code> <p>value, score: value}</p> Source code in <code>txtai/api/base.py</code> <pre><code>def search(self, query, request=None, limit=None):\n    # When search is invoked via the API, limit is set from the request\n    # When search is invoked directly, limit is set using the method parameter\n    limit = self.limit(request.query_params.get(\"limit\") if request and hasattr(request, \"query_params\") else limit)\n\n    if self.cluster:\n        return self.cluster.search(query, limit)\n\n    return super().search(query, limit)\n</code></pre>"},{"location":"api/methods/#txtai.api.base.API.upsert","title":"<code>upsert(self)</code>","text":"<p>Runs an embeddings upsert operation for previously batched documents.</p> Source code in <code>txtai/api/base.py</code> <pre><code>def upsert(self):\n\"\"\"\n    Runs an embeddings upsert operation for previously batched documents.\n    \"\"\"\n\n    if self.cluster:\n        self.cluster.upsert()\n    else:\n        super().upsert()\n</code></pre>"},{"location":"embeddings/","title":"Embeddings","text":"<p>Embeddings is the engine that delivers semantic search. Data is transformed into embeddings vectors where similar concepts will produce similar vectors. Indexes both large and small are built with these vectors. The indexes are used to find results that have the same meaning, not necessarily the same keywords.</p> <p>The following code snippet shows how to build and search an embeddings index.</p> <pre><code>from txtai.embeddings import Embeddings\n\n# Create embeddings model, backed by sentence-transformers &amp; transformers\nembeddings = Embeddings({\"path\": \"sentence-transformers/nli-mpnet-base-v2\"})\n\ndata = [\n  \"US tops 5 million confirmed virus cases\",\n  \"Canada's last fully intact ice shelf has suddenly collapsed, \" +\n  \"forming a Manhattan-sized iceberg\",\n  \"Beijing mobilises invasion craft along coast as Taiwan tensions escalate\",\n  \"The National Park Service warns against sacrificing slower friends \" +\n  \"in a bear attack\",\n  \"Maine man wins $1M from $25 lottery ticket\",\n  \"Make huge profits without work, earn up to $100,000 a day\"\n]\n\n# Create an index for the list of text\nembeddings.index([(uid, text, None) for uid, text in enumerate(data)])\n\nprint(\"%-20s %s\" % (\"Query\", \"Best Match\"))\nprint(\"-\" * 50)\n\n# Run an embeddings search for each query\nfor query in (\"feel good story\", \"climate change\", \"public health story\", \"war\",\n              \"wildlife\", \"asia\", \"lucky\", \"dishonest junk\"):\n    # Extract uid of first result\n    # search result format: (uid, score)\n    uid = embeddings.search(query, 1)[0][0]\n\n    # Print text\n    print(\"%-20s %s\" % (query, data[uid]))\n</code></pre>"},{"location":"embeddings/#build","title":"Build","text":"<p>An embeddings instance can be created as follows:</p> <pre><code>embeddings = Embeddings({\"path\": \"sentence-transformers/nli-mpnet-base-v2\"})\n</code></pre> <p>The example above builds a transformers-based embeddings instance. In this case, when loading and searching for data, a transformers model is used to vectorize data.</p> <p>The embeddings instance is configuration-driven based on what is passed in the constructor. Embeddings indexes store vectors and can optionally store content. Content storage enables additional filtering and data retrieval options.</p>"},{"location":"embeddings/#index","title":"Index","text":"<p>After creating a new embeddings instance, the next step is adding data to it.</p> <pre><code>embeddings.index([(uid, text, None) for uid, text in enumerate(data)])\n</code></pre> <p>The index method takes an iterable collection of tuples with three values. </p> Element Description id unique record id data input data to index, can be text, a dictionary or object tags optional tags string, used to mark/label data as it's indexed <p>When the data element is a dictionary and it has a field named <code>text</code>, that will be used for indexing.</p> <p>The input iterable can be a list or generator. Generators help with indexing very large datasets as only portions of the data is in memory at any given time.</p> <p>More information on indexing can be found in the index guide.</p>"},{"location":"embeddings/#search","title":"Search","text":"<p>Once data is indexed, it is ready for search.</p> <pre><code>embeddings.search(query, limit)\n</code></pre> <p>The search method takes two parameters, the query and query limit. The results format is different based on whether content is stored or not.</p> <ul> <li>List of <code>(id, score)</code> when content is not stored</li> <li>List of <code>{**query columns}</code> when content is stored</li> </ul> <p>Both natural language and SQL queries are supported. More information can be found in the query guide.</p>"},{"location":"embeddings/#more-examples","title":"More examples","text":"<p>See this link for a full list of embeddings examples.</p>"},{"location":"embeddings/indexing/","title":"Index guide","text":"<p>This section gives an in-depth overview on how to index data with txtai. We'll cover vectorization, indexing, updating and deleting data.</p>"},{"location":"embeddings/indexing/#vectorization","title":"Vectorization","text":"<p>The most compute intensive step in building an index is vectorization. The path parameter sets the path to the vector model. There is logic to automatically detect the vector model method but it can also be set directly.</p> <p>The batch and encodebatch parameters control the vectorization process. Larger values for <code>batch</code> will pass larger batches to the vectorization method. Larger values for <code>encodebatch</code> will pass larger batches for each vector encode call. In the case of GPU vector models, larger values will consume more GPU memory.</p> <p>Data is buffered to temporary storage during indexing as embeddings vectors can be quite large (for example 768 dimensions of float32 is 768 * 4 = 3072 bytes per vector). Once vectorization is complete, a mmapped array is created with all vectors for Approximate Nearest Neighbor (ANN) indexing.</p>"},{"location":"embeddings/indexing/#setting-a-backend","title":"Setting a backend","text":"<p>As mentioned above, computed vectors are stored in an ANN. There are various index backends that can be configured. Faiss is the default backend.</p>"},{"location":"embeddings/indexing/#content-storage","title":"Content storage","text":"<p>Embeddings indexes can optionally store content. When this is enabled, the input content is saved in a database alongside the computed vectors. This enables filtering on additional fields and content retrieval.</p>"},{"location":"embeddings/indexing/#index-vs-upsert","title":"Index vs Upsert","text":"<p>Data is loaded into an index with either an index or upsert call.</p> <pre><code>embeddings.index([(uid, text, None) for uid, text in enumerate(data)])\nembeddings.upsert([(uid, text, None) for uid, text in enumerate(data)])\n</code></pre> <p>The <code>index</code> call will build a brand new index replacing an existing one. <code>upsert</code> will insert or update records. <code>upsert</code> ops do not require a full index rebuild.</p>"},{"location":"embeddings/indexing/#save","title":"Save","text":"<p>Indexes can be stored in a directory using the save method.</p> <pre><code>embeddings.save(\"/path/to/save\")\n</code></pre> <p>Compressed indexes are also supported.</p> <pre><code>embeddings.save(\"/path/to/save/index.tar.gz\")\n</code></pre> <p>In addition to saving indexes locally, they can also be persisted to cloud storage.</p> <pre><code>embeddings.save(\"/path/to/save/index.tar.gz\", cloud={...})\n</code></pre> <p>This is especially useful when running in a serverless context or otherwise running on temporary compute. Cloud storage is only supported with compressed indexes.</p> <p>Embeddings indexes can be restored using the load method.</p> <pre><code>embeddings.load(\"/path/to/load\")\n</code></pre>"},{"location":"embeddings/indexing/#delete","title":"Delete","text":"<p>Content can be removed from the index with the delete method. This method takes a list of ids to delete.</p> <pre><code>embeddings.delete(ids)\n</code></pre>"},{"location":"embeddings/indexing/#reindex","title":"Reindex","text":"<p>When content storage is enabled, reindex can be called to rebuild the index with new settings. For example, the backend can be switched from faiss to hnsw or the vector model can be updated. This prevents having to go back to the original raw data. </p> <pre><code>embeddings.reindex({\"path\": \"sentence-transformers/all-MiniLM-L6-v2\", \"backend\": \"hnsw\"})\n</code></pre>"},{"location":"embeddings/indexing/#graph","title":"Graph","text":"<p>Dimensionality reduction with UMAP combined with HDBSCAN is a popular topic modeling method found in a number of libraries. txtai takes a different approach with a semantic graph.</p> <p>Enabling a graph network adds a semantic graph at index time as data is being vectorized. Vector embeddings are used to create relationships in the graph. Finally, community detection algorithms build topic clusters. Semantic graphs can also be used to analyze data connectivity.</p> <p>This approach has the advantage of only having to vectorize data once. It also has the advantage of better topic precision given there isn't a dimensionality reduction operation (UMAP). Semantic graph examples are shown below.</p> <p>Get a mapping of discovered topics to associated ids.</p> <pre><code>embeddings.graph.topics\n</code></pre> <p>Show the most central nodes in the index.</p> <pre><code>embeddings.graph.centrality()\n</code></pre> <p>Show how node 1 and node 2 are connected in the graph.</p> <pre><code>embeddings.graph.showpath(id1, id2)\n</code></pre> <p>Graphs are persisted alongside an embeddings index. Each save and load will also save and load the graph.</p>"},{"location":"embeddings/indexing/#scoring","title":"Scoring","text":"<p>When using word vector backed models with scoring set, a separate call is required before calling <code>index</code> as follows:</p> <pre><code>embeddings.score([(uid, text, None) for uid, text in enumerate(data)])\nembeddings.index([(uid, text, None) for uid, text in enumerate(data)])\n</code></pre> <p>Two calls are required to support generator-backed iteration of data. The scoring index requires a separate full-pass of the data.</p> <p>Scoring instances can also create a standalone keyword-based index (BM25, TF-IDF). See this link to learn more.</p>"},{"location":"embeddings/methods/","title":"Methods","text":""},{"location":"embeddings/methods/#txtai.embeddings.base.Embeddings","title":"<code> Embeddings        </code>","text":"<p>Embeddings is the engine that delivers semantic search. Data is transformed into embeddings vectors where similar concepts will produce similar vectors. Indexes both large and small are built with these vectors. The indexes are used to find results that have the same meaning, not necessarily the same keywords.</p> Source code in <code>txtai/embeddings/base.py</code> <pre><code>class Embeddings:\n\"\"\"\n    Embeddings is the engine that delivers semantic search. Data is transformed into embeddings vectors where similar concepts\n    will produce similar vectors. Indexes both large and small are built with these vectors. The indexes are used to find results\n    that have the same meaning, not necessarily the same keywords.\n    \"\"\"\n\n    # pylint: disable = W0231\n    def __init__(self, config=None):\n\"\"\"\n        Creates a new embeddings index. Embeddings indexes are thread-safe for read operations but writes must be\n        synchronized.\n\n        Args:\n            config: embeddings configuration\n        \"\"\"\n\n        # Index configuration\n        self.config = None\n\n        # Dimensionality reduction and scoring index - word vectors only\n        self.reducer, self.scoring = None, None\n\n        # Embeddings vector model - transforms data into similarity vectors\n        self.model = None\n\n        # Approximate nearest neighbor index\n        self.ann = None\n\n        # Document database\n        self.database = None\n\n        # Graph network\n        self.graph = None\n\n        # Query model\n        self.query = None\n\n        # Index archive\n        self.archive = None\n\n        # Set initial configuration\n        self.configure(config)\n\n    def score(self, documents):\n\"\"\"\n        Builds a scoring index. Only used by word vectors models.\n\n        Args:\n            documents: list of (id, data, tags)\n        \"\"\"\n\n        # Build scoring index over documents\n        if self.scoring:\n            self.scoring.index(documents)\n\n    def index(self, documents, reindex=False):\n\"\"\"\n        Builds an embeddings index. This method overwrites an existing index.\n\n        Args:\n            documents: list of (id, data, tags)\n            reindex: if this is a reindex operation in which case database creation is skipped, defaults to False\n        \"\"\"\n\n        # Set configuration to default configuration, if empty\n        if not self.config:\n            self.configure(self.defaults())\n\n        # Create document database, if necessary\n        if not reindex:\n            self.database = self.createdatabase()\n\n            # Reset archive since this is a new index\n            self.archive = None\n\n        # Create graph, if necessary\n        self.graph = self.creategraph()\n\n        # Create transform action\n        transform = Transform(self, Action.REINDEX if reindex else Action.INDEX)\n\n        with tempfile.NamedTemporaryFile(mode=\"wb\", suffix=\".npy\") as buffer:\n            # Load documents into database and transform to vectors\n            ids, dimensions, embeddings = transform(documents, buffer)\n            if ids:\n                # Build LSA model (if enabled). Remove principal components from embeddings.\n                if self.config.get(\"pca\"):\n                    self.reducer = Reducer(embeddings, self.config[\"pca\"])\n                    self.reducer(embeddings)\n\n                # Normalize embeddings\n                self.normalize(embeddings)\n\n                # Save index dimensions\n                self.config[\"dimensions\"] = dimensions\n\n                # Create approximate nearest neighbor index\n                self.ann = ANNFactory.create(self.config)\n\n                # Add embeddings to the index\n                self.ann.index(embeddings)\n\n                # Save indexids-ids mapping for indexes with no database, except when this is a reindex action\n                if not reindex and not self.database:\n                    self.config[\"ids\"] = ids\n\n        # Index graph, if necessary\n        if self.graph:\n            self.graph.index(Search(self, True), self.batchsimilarity)\n\n    def upsert(self, documents):\n\"\"\"\n        Runs an embeddings upsert operation. If the index exists, new data is\n        appended to the index, existing data is updated. If the index doesn't exist,\n        this method runs a standard index operation.\n\n        Args:\n            documents: list of (id, data, tags)\n        \"\"\"\n\n        # Run standard insert if index doesn't exist\n        if not self.ann:\n            self.index(documents)\n            return\n\n        # Create transform action\n        transform = Transform(self, Action.UPSERT)\n\n        with tempfile.NamedTemporaryFile(mode=\"wb\", suffix=\".npy\") as buffer:\n            # Load documents into database and transform to vectors\n            ids, _, embeddings = transform(documents, buffer)\n            if ids:\n                # Remove principal components from embeddings, if necessary\n                if self.reducer:\n                    self.reducer(embeddings)\n\n                # Normalize embeddings\n                self.normalize(embeddings)\n\n                # Append embeddings to the index\n                self.ann.append(embeddings)\n\n                # Save indexids-ids mapping for indexes with no database\n                if not self.database:\n                    self.config[\"ids\"] = self.config[\"ids\"] + ids\n\n        # Graph upsert, if necessary\n        if self.graph:\n            self.graph.upsert(Search(self, True))\n\n    def delete(self, ids):\n\"\"\"\n        Deletes from an embeddings index. Returns list of ids deleted.\n\n        Args:\n            ids: list of ids to delete\n\n        Returns:\n            list of ids deleted\n        \"\"\"\n\n        # List of internal indices for each candidate id to delete\n        indices = []\n\n        # List of deleted ids\n        deletes = []\n\n        if self.database:\n            # Retrieve indexid-id mappings from database\n            ids = self.database.ids(ids)\n\n            # Parse out indices and ids to delete\n            indices = [i for i, _ in ids]\n            deletes = sorted(set(uid for _, uid in ids))\n\n            # Delete ids from database\n            self.database.delete(deletes)\n        elif self.ann:\n            # Lookup indexids from config for indexes with no database\n            indexids = self.config[\"ids\"]\n\n            # Find existing ids\n            for uid in ids:\n                indices.extend([index for index, value in enumerate(indexids) if uid == value])\n\n            # Clear config ids\n            for index in indices:\n                deletes.append(indexids[index])\n                indexids[index] = None\n\n        # Delete indices from ann embeddings\n        if indices:\n            # Delete ids from index\n            self.ann.delete(indices)\n\n            # Delete ids from graph\n            if self.graph:\n                self.graph.delete(indices)\n\n        return deletes\n\n    def reindex(self, config, columns=None, function=None):\n\"\"\"\n        Recreates the approximate nearest neighbor (ann) index using config. This method only works if document\n        content storage is enabled.\n\n        Args:\n            config: new config\n            columns: optional list of document columns used to rebuild data\n            function: optional function to prepare content for indexing\n        \"\"\"\n\n        if self.database:\n            # Keep content and objects parameters to ensure database is preserved\n            config[\"content\"] = self.config[\"content\"]\n            if \"objects\" in self.config:\n                config[\"objects\"] = self.config[\"objects\"]\n\n            # Reset configuration\n            self.configure(config)\n\n            # Reindex\n            if function:\n                self.index(function(self.database.reindex(columns)), True)\n            else:\n                self.index(self.database.reindex(columns), True)\n\n    def transform(self, document):\n\"\"\"\n        Transforms document into an embeddings vector.\n\n        Args:\n            document: (id, data, tags)\n\n        Returns:\n            embeddings vector\n        \"\"\"\n\n        return self.batchtransform([document])[0]\n\n    def batchtransform(self, documents):\n\"\"\"\n        Transforms documents into embeddings vectors.\n\n        Args:\n            documents: list of (id, data, tags)\n\n        Returns:\n            embeddings vectors\n        \"\"\"\n\n        # Convert documents into sentence embeddings\n        embeddings = self.model.batchtransform(documents)\n\n        # Reduce the dimensionality of the embeddings. Scale the embeddings using this\n        # model to reduce the noise of common but less relevant terms.\n        if self.reducer:\n            self.reducer(embeddings)\n\n        # Normalize embeddings\n        self.normalize(embeddings)\n\n        return embeddings\n\n    def count(self):\n\"\"\"\n        Total number of elements in this embeddings index.\n\n        Returns:\n            number of elements in this embeddings index\n        \"\"\"\n\n        return self.ann.count() if self.ann else 0\n\n    def search(self, query, limit=None):\n\"\"\"\n        Finds documents most similar to the input queries. This method will run either an approximate\n        nearest neighbor (ann) search or an approximate nearest neighbor + database search depending\n        on if a database is available.\n\n        Args:\n            query: input query\n            limit: maximum results\n\n        Returns:\n            list of (id, score) for ann search, list of dict for an ann+database search\n        \"\"\"\n\n        results = self.batchsearch([query], limit)\n        return results[0] if results else results\n\n    def batchsearch(self, queries, limit=None):\n\"\"\"\n        Finds documents most similar to the input queries. This method will run either an approximate\n        nearest neighbor (ann) search or an approximate nearest neighbor + database search depending\n        on if a database is available.\n\n        Args:\n            queries: input queries\n            limit: maximum results\n\n        Returns:\n            list of (id, score) per query for ann search, list of dict per query for an ann+database search\n        \"\"\"\n\n        return Search(self)(queries, limit if limit else 3)\n\n    def similarity(self, query, data):\n\"\"\"\n        Computes the similarity between query and list of data. Returns a list of\n        (id, score) sorted by highest score, where id is the index in data.\n\n        Args:\n            query: input query\n            data: list of data\n\n        Returns:\n            list of (id, score)\n        \"\"\"\n\n        return self.batchsimilarity([query], data)[0]\n\n    def batchsimilarity(self, queries, data):\n\"\"\"\n        Computes the similarity between list of queries and list of data. Returns a list\n        of (id, score) sorted by highest score per query, where id is the index in data.\n\n        Args:\n            queries: input queries\n            data: list of data\n\n        Returns:\n            list of (id, score) per query\n        \"\"\"\n\n        # Convert queries to embedding vectors\n        queries = self.batchtransform((None, query, None) for query in queries)\n        data = self.batchtransform((None, row, None) for row in data)\n\n        # Dot product on normalized vectors is equal to cosine similarity\n        scores = np.dot(queries, data.T).tolist()\n\n        # Add index and sort desc based on score\n        return [sorted(enumerate(score), key=lambda x: x[1], reverse=True) for score in scores]\n\n    def explain(self, query, texts=None, limit=None):\n\"\"\"\n        Explains the importance of each input token in text for a query.\n\n        Args:\n            query: input query\n            texts: optional list of (text|list of tokens), otherwise runs search query\n            limit: optional limit if texts is None\n\n        Returns:\n            list of dict per input text where a higher token scores represents higher importance relative to the query\n        \"\"\"\n\n        results = self.batchexplain([query], texts, limit)\n        return results[0] if results else results\n\n    def batchexplain(self, queries, texts=None, limit=None):\n\"\"\"\n        Explains the importance of each input token in text for a list of queries.\n\n        Args:\n            query: input queries\n            texts: optional list of (text|list of tokens), otherwise runs search queries\n            limit: optional limit if texts is None\n\n        Returns:\n            list of dict per input text per query where a higher token scores represents higher importance relative to the query\n        \"\"\"\n\n        return Explain(self)(queries, texts, limit)\n\n    def exists(self, path, cloud=None):\n\"\"\"\n        Checks if an index exists at path.\n\n        Args:\n            path: input path\n            cloud: cloud storage configuration\n\n        Returns:\n            True if index exists, False otherwise\n        \"\"\"\n\n        # Check if this is an archive file and exists\n        path, apath = self.checkarchive(path)\n        if apath:\n            return self.archive.exists(apath, cloud)\n\n        return os.path.exists(f\"{path}/config\") and os.path.exists(f\"{path}/embeddings\")\n\n    def load(self, path, cloud=None):\n\"\"\"\n        Loads an existing index from path.\n\n        Args:\n            path: input path\n            cloud: cloud storage configuration\n        \"\"\"\n\n        # Check if this is an archive file and extract\n        path, apath = self.checkarchive(path)\n        if apath:\n            self.archive.load(apath, cloud)\n\n        # Index configuration\n        with open(f\"{path}/config\", \"rb\") as handle:\n            self.config = pickle.load(handle)\n\n            # Build full path to embedding vectors file\n            if self.config.get(\"storevectors\"):\n                self.config[\"path\"] = os.path.join(path, self.config[\"path\"])\n\n        # Approximate nearest neighbor index - stores embeddings vectors\n        self.ann = ANNFactory.create(self.config)\n        self.ann.load(f\"{path}/embeddings\")\n\n        # Dimensionality reduction model - word vectors only\n        if self.config.get(\"pca\"):\n            self.reducer = Reducer()\n            self.reducer.load(f\"{path}/lsa\")\n\n        # Embedding scoring index - word vectors only\n        if self.config.get(\"scoring\"):\n            self.scoring = ScoringFactory.create(self.config[\"scoring\"])\n            self.scoring.load(f\"{path}/scoring\")\n\n        # Sentence vectors model - transforms data to embeddings vectors\n        self.model = self.loadvectors()\n\n        # Query model\n        self.query = self.loadquery()\n\n        # Document database - stores document content\n        self.database = self.createdatabase()\n        if self.database:\n            self.database.load(f\"{path}/documents\")\n\n        # Graph network - stores relationships\n        self.graph = self.creategraph()\n        if self.graph:\n            self.graph.load(f\"{path}/graph\")\n\n    def save(self, path, cloud=None):\n\"\"\"\n        Saves an index in a directory at path unless path ends with tar.gz, tar.bz2, tar.xz or zip.\n        In those cases, the index is stored as a compressed file.\n\n        Args:\n            path: output path\n            cloud: cloud storage configuration\n        \"\"\"\n\n        if self.config:\n            # Check if this is an archive file\n            path, apath = self.checkarchive(path)\n\n            # Create output directory, if necessary\n            os.makedirs(path, exist_ok=True)\n\n            # Copy sentence vectors model\n            if self.config.get(\"storevectors\"):\n                shutil.copyfile(self.config[\"path\"], os.path.join(path, os.path.basename(self.config[\"path\"])))\n\n                self.config[\"path\"] = os.path.basename(self.config[\"path\"])\n\n            # Write index configuration\n            with open(f\"{path}/config\", \"wb\") as handle:\n                pickle.dump(self.config, handle, protocol=__pickle__)\n\n            # Save approximate nearest neighbor index\n            self.ann.save(f\"{path}/embeddings\")\n\n            # Save dimensionality reduction model (word vectors only)\n            if self.reducer:\n                self.reducer.save(f\"{path}/lsa\")\n\n            # Save embedding scoring index (word vectors only)\n            if self.scoring:\n                self.scoring.save(f\"{path}/scoring\")\n\n            # Save document database\n            if self.database:\n                self.database.save(f\"{path}/documents\")\n\n            # Save graph\n            if self.graph:\n                self.graph.save(f\"{path}/graph\")\n\n            # If this is an archive, save it\n            if apath:\n                self.archive.save(apath, cloud)\n\n    def close(self):\n\"\"\"\n        Closes this embeddings index and frees all resources.\n        \"\"\"\n\n        self.config, self.reducer, self.scoring, self.model = None, None, None, None\n        self.ann, self.graph, self.query, self.archive = None, None, None, None\n\n        # Close database connection if open\n        if self.database:\n            self.database.close()\n            self.database = None\n\n    def info(self):\n\"\"\"\n        Prints the current embeddings index configuration.\n        \"\"\"\n\n        # Copy and edit config\n        config = self.config.copy()\n\n        # Remove ids array if present\n        config.pop(\"ids\", None)\n\n        # Print configuration\n        print(json.dumps(config, sort_keys=True, default=str, indent=2))\n\n    def configure(self, config):\n\"\"\"\n        Sets the configuration for this embeddings index and loads config-driven models.\n\n        Args:\n            config: embeddings configuration\n        \"\"\"\n\n        # Configuration\n        self.config = config\n\n        if self.config and self.config.get(\"method\") != \"transformers\":\n            # Dimensionality reduction model\n            self.reducer = None\n\n            # Embedding scoring method - weighs each word in a sentence\n            self.scoring = ScoringFactory.create(self.config[\"scoring\"]) if self.config and self.config.get(\"scoring\") else None\n        else:\n            self.reducer, self.scoring = None, None\n\n        # Sentence vectors model - transforms data to embeddings vectors\n        self.model = self.loadvectors() if self.config else None\n\n        # Query model\n        self.query = self.loadquery() if self.config else None\n\n    def defaults(self):\n\"\"\"\n        Builds a default configuration.\n\n        Returns:\n            default configuration\n        \"\"\"\n\n        return {\"path\": \"sentence-transformers/all-MiniLM-L6-v2\"}\n\n    def loadvectors(self):\n\"\"\"\n        Loads a vector model set in config.\n\n        Returns:\n            vector model\n        \"\"\"\n\n        return VectorsFactory.create(self.config, self.scoring)\n\n    def loadquery(self):\n\"\"\"\n        Loads a query model set in config.\n\n        Returns:\n            query model\n        \"\"\"\n\n        if \"query\" in self.config:\n            return Query(**self.config[\"query\"])\n\n        return None\n\n    def checkarchive(self, path):\n\"\"\"\n        Checks if path is an archive file.\n\n        Args:\n            path: path to check\n\n        Returns:\n            (working directory, current path) if this is an archive, original path otherwise\n        \"\"\"\n\n        # Create archive instance, if necessary\n        self.archive = self.archive if self.archive else Archive()\n\n        # Check if path is an archive file\n        if self.archive.isarchive(path):\n            # Return temporary archive working directory and original path\n            return self.archive.path(), path\n\n        return path, None\n\n    def createdatabase(self):\n\"\"\"\n        Creates a database from config. This method will also close any existing database connection.\n\n        Returns:\n            new database, if enabled in config\n        \"\"\"\n\n        # Free existing database resources\n        if self.database:\n            self.database.close()\n\n        config = self.config.copy()\n\n        # Resolve callable functions\n        if \"functions\" in config:\n            config[\"functions\"] = Functions(self)(config)\n\n        # Create database from config and return\n        return DatabaseFactory.create(config)\n\n    def creategraph(self):\n\"\"\"\n        Creates a graph from config.\n\n        Returns:\n            new graph, if enabled in config\n        \"\"\"\n\n        return GraphFactory.create(self.config[\"graph\"]) if \"graph\" in self.config else None\n\n    def normalize(self, embeddings):\n\"\"\"\n        Normalizes embeddings using L2 normalization. Operation applied directly on array.\n\n        Args:\n            embeddings: input embeddings matrix\n        \"\"\"\n\n        # Calculation is different for matrices vs vectors\n        if len(embeddings.shape) &gt; 1:\n            embeddings /= np.linalg.norm(embeddings, axis=1)[:, np.newaxis]\n        else:\n            embeddings /= np.linalg.norm(embeddings)\n</code></pre>"},{"location":"embeddings/methods/#txtai.embeddings.base.Embeddings.__init__","title":"<code>__init__(self, config=None)</code>  <code>special</code>","text":"<p>Creates a new embeddings index. Embeddings indexes are thread-safe for read operations but writes must be synchronized.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <p>embeddings configuration</p> <code>None</code> Source code in <code>txtai/embeddings/base.py</code> <pre><code>def __init__(self, config=None):\n\"\"\"\n    Creates a new embeddings index. Embeddings indexes are thread-safe for read operations but writes must be\n    synchronized.\n\n    Args:\n        config: embeddings configuration\n    \"\"\"\n\n    # Index configuration\n    self.config = None\n\n    # Dimensionality reduction and scoring index - word vectors only\n    self.reducer, self.scoring = None, None\n\n    # Embeddings vector model - transforms data into similarity vectors\n    self.model = None\n\n    # Approximate nearest neighbor index\n    self.ann = None\n\n    # Document database\n    self.database = None\n\n    # Graph network\n    self.graph = None\n\n    # Query model\n    self.query = None\n\n    # Index archive\n    self.archive = None\n\n    # Set initial configuration\n    self.configure(config)\n</code></pre>"},{"location":"embeddings/methods/#txtai.embeddings.base.Embeddings.batchexplain","title":"<code>batchexplain(self, queries, texts=None, limit=None)</code>","text":"<p>Explains the importance of each input token in text for a list of queries.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <p>input queries</p> required <code>texts</code> <p>optional list of (text|list of tokens), otherwise runs search queries</p> <code>None</code> <code>limit</code> <p>optional limit if texts is None</p> <code>None</code> <p>Returns:</p> Type Description <p>list of dict per input text per query where a higher token scores represents higher importance relative to the query</p> Source code in <code>txtai/embeddings/base.py</code> <pre><code>def batchexplain(self, queries, texts=None, limit=None):\n\"\"\"\n    Explains the importance of each input token in text for a list of queries.\n\n    Args:\n        query: input queries\n        texts: optional list of (text|list of tokens), otherwise runs search queries\n        limit: optional limit if texts is None\n\n    Returns:\n        list of dict per input text per query where a higher token scores represents higher importance relative to the query\n    \"\"\"\n\n    return Explain(self)(queries, texts, limit)\n</code></pre>"},{"location":"embeddings/methods/#txtai.embeddings.base.Embeddings.batchsearch","title":"<code>batchsearch(self, queries, limit=None)</code>","text":"<p>Finds documents most similar to the input queries. This method will run either an approximate nearest neighbor (ann) search or an approximate nearest neighbor + database search depending on if a database is available.</p> <p>Parameters:</p> Name Type Description Default <code>queries</code> <p>input queries</p> required <code>limit</code> <p>maximum results</p> <code>None</code> <p>Returns:</p> Type Description <p>list of (id, score) per query for ann search, list of dict per query for an ann+database search</p> Source code in <code>txtai/embeddings/base.py</code> <pre><code>def batchsearch(self, queries, limit=None):\n\"\"\"\n    Finds documents most similar to the input queries. This method will run either an approximate\n    nearest neighbor (ann) search or an approximate nearest neighbor + database search depending\n    on if a database is available.\n\n    Args:\n        queries: input queries\n        limit: maximum results\n\n    Returns:\n        list of (id, score) per query for ann search, list of dict per query for an ann+database search\n    \"\"\"\n\n    return Search(self)(queries, limit if limit else 3)\n</code></pre>"},{"location":"embeddings/methods/#txtai.embeddings.base.Embeddings.batchsimilarity","title":"<code>batchsimilarity(self, queries, data)</code>","text":"<p>Computes the similarity between list of queries and list of data. Returns a list of (id, score) sorted by highest score per query, where id is the index in data.</p> <p>Parameters:</p> Name Type Description Default <code>queries</code> <p>input queries</p> required <code>data</code> <p>list of data</p> required <p>Returns:</p> Type Description <p>list of (id, score) per query</p> Source code in <code>txtai/embeddings/base.py</code> <pre><code>def batchsimilarity(self, queries, data):\n\"\"\"\n    Computes the similarity between list of queries and list of data. Returns a list\n    of (id, score) sorted by highest score per query, where id is the index in data.\n\n    Args:\n        queries: input queries\n        data: list of data\n\n    Returns:\n        list of (id, score) per query\n    \"\"\"\n\n    # Convert queries to embedding vectors\n    queries = self.batchtransform((None, query, None) for query in queries)\n    data = self.batchtransform((None, row, None) for row in data)\n\n    # Dot product on normalized vectors is equal to cosine similarity\n    scores = np.dot(queries, data.T).tolist()\n\n    # Add index and sort desc based on score\n    return [sorted(enumerate(score), key=lambda x: x[1], reverse=True) for score in scores]\n</code></pre>"},{"location":"embeddings/methods/#txtai.embeddings.base.Embeddings.batchtransform","title":"<code>batchtransform(self, documents)</code>","text":"<p>Transforms documents into embeddings vectors.</p> <p>Parameters:</p> Name Type Description Default <code>documents</code> <p>list of (id, data, tags)</p> required <p>Returns:</p> Type Description <p>embeddings vectors</p> Source code in <code>txtai/embeddings/base.py</code> <pre><code>def batchtransform(self, documents):\n\"\"\"\n    Transforms documents into embeddings vectors.\n\n    Args:\n        documents: list of (id, data, tags)\n\n    Returns:\n        embeddings vectors\n    \"\"\"\n\n    # Convert documents into sentence embeddings\n    embeddings = self.model.batchtransform(documents)\n\n    # Reduce the dimensionality of the embeddings. Scale the embeddings using this\n    # model to reduce the noise of common but less relevant terms.\n    if self.reducer:\n        self.reducer(embeddings)\n\n    # Normalize embeddings\n    self.normalize(embeddings)\n\n    return embeddings\n</code></pre>"},{"location":"embeddings/methods/#txtai.embeddings.base.Embeddings.close","title":"<code>close(self)</code>","text":"<p>Closes this embeddings index and frees all resources.</p> Source code in <code>txtai/embeddings/base.py</code> <pre><code>def close(self):\n\"\"\"\n    Closes this embeddings index and frees all resources.\n    \"\"\"\n\n    self.config, self.reducer, self.scoring, self.model = None, None, None, None\n    self.ann, self.graph, self.query, self.archive = None, None, None, None\n\n    # Close database connection if open\n    if self.database:\n        self.database.close()\n        self.database = None\n</code></pre>"},{"location":"embeddings/methods/#txtai.embeddings.base.Embeddings.count","title":"<code>count(self)</code>","text":"<p>Total number of elements in this embeddings index.</p> <p>Returns:</p> Type Description <p>number of elements in this embeddings index</p> Source code in <code>txtai/embeddings/base.py</code> <pre><code>def count(self):\n\"\"\"\n    Total number of elements in this embeddings index.\n\n    Returns:\n        number of elements in this embeddings index\n    \"\"\"\n\n    return self.ann.count() if self.ann else 0\n</code></pre>"},{"location":"embeddings/methods/#txtai.embeddings.base.Embeddings.defaults","title":"<code>defaults(self)</code>","text":"<p>Builds a default configuration.</p> <p>Returns:</p> Type Description <p>default configuration</p> Source code in <code>txtai/embeddings/base.py</code> <pre><code>def defaults(self):\n\"\"\"\n    Builds a default configuration.\n\n    Returns:\n        default configuration\n    \"\"\"\n\n    return {\"path\": \"sentence-transformers/all-MiniLM-L6-v2\"}\n</code></pre>"},{"location":"embeddings/methods/#txtai.embeddings.base.Embeddings.delete","title":"<code>delete(self, ids)</code>","text":"<p>Deletes from an embeddings index. Returns list of ids deleted.</p> <p>Parameters:</p> Name Type Description Default <code>ids</code> <p>list of ids to delete</p> required <p>Returns:</p> Type Description <p>list of ids deleted</p> Source code in <code>txtai/embeddings/base.py</code> <pre><code>def delete(self, ids):\n\"\"\"\n    Deletes from an embeddings index. Returns list of ids deleted.\n\n    Args:\n        ids: list of ids to delete\n\n    Returns:\n        list of ids deleted\n    \"\"\"\n\n    # List of internal indices for each candidate id to delete\n    indices = []\n\n    # List of deleted ids\n    deletes = []\n\n    if self.database:\n        # Retrieve indexid-id mappings from database\n        ids = self.database.ids(ids)\n\n        # Parse out indices and ids to delete\n        indices = [i for i, _ in ids]\n        deletes = sorted(set(uid for _, uid in ids))\n\n        # Delete ids from database\n        self.database.delete(deletes)\n    elif self.ann:\n        # Lookup indexids from config for indexes with no database\n        indexids = self.config[\"ids\"]\n\n        # Find existing ids\n        for uid in ids:\n            indices.extend([index for index, value in enumerate(indexids) if uid == value])\n\n        # Clear config ids\n        for index in indices:\n            deletes.append(indexids[index])\n            indexids[index] = None\n\n    # Delete indices from ann embeddings\n    if indices:\n        # Delete ids from index\n        self.ann.delete(indices)\n\n        # Delete ids from graph\n        if self.graph:\n            self.graph.delete(indices)\n\n    return deletes\n</code></pre>"},{"location":"embeddings/methods/#txtai.embeddings.base.Embeddings.exists","title":"<code>exists(self, path, cloud=None)</code>","text":"<p>Checks if an index exists at path.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <p>input path</p> required <code>cloud</code> <p>cloud storage configuration</p> <code>None</code> <p>Returns:</p> Type Description <p>True if index exists, False otherwise</p> Source code in <code>txtai/embeddings/base.py</code> <pre><code>def exists(self, path, cloud=None):\n\"\"\"\n    Checks if an index exists at path.\n\n    Args:\n        path: input path\n        cloud: cloud storage configuration\n\n    Returns:\n        True if index exists, False otherwise\n    \"\"\"\n\n    # Check if this is an archive file and exists\n    path, apath = self.checkarchive(path)\n    if apath:\n        return self.archive.exists(apath, cloud)\n\n    return os.path.exists(f\"{path}/config\") and os.path.exists(f\"{path}/embeddings\")\n</code></pre>"},{"location":"embeddings/methods/#txtai.embeddings.base.Embeddings.explain","title":"<code>explain(self, query, texts=None, limit=None)</code>","text":"<p>Explains the importance of each input token in text for a query.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <p>input query</p> required <code>texts</code> <p>optional list of (text|list of tokens), otherwise runs search query</p> <code>None</code> <code>limit</code> <p>optional limit if texts is None</p> <code>None</code> <p>Returns:</p> Type Description <p>list of dict per input text where a higher token scores represents higher importance relative to the query</p> Source code in <code>txtai/embeddings/base.py</code> <pre><code>def explain(self, query, texts=None, limit=None):\n\"\"\"\n    Explains the importance of each input token in text for a query.\n\n    Args:\n        query: input query\n        texts: optional list of (text|list of tokens), otherwise runs search query\n        limit: optional limit if texts is None\n\n    Returns:\n        list of dict per input text where a higher token scores represents higher importance relative to the query\n    \"\"\"\n\n    results = self.batchexplain([query], texts, limit)\n    return results[0] if results else results\n</code></pre>"},{"location":"embeddings/methods/#txtai.embeddings.base.Embeddings.index","title":"<code>index(self, documents, reindex=False)</code>","text":"<p>Builds an embeddings index. This method overwrites an existing index.</p> <p>Parameters:</p> Name Type Description Default <code>documents</code> <p>list of (id, data, tags)</p> required <code>reindex</code> <p>if this is a reindex operation in which case database creation is skipped, defaults to False</p> <code>False</code> Source code in <code>txtai/embeddings/base.py</code> <pre><code>def index(self, documents, reindex=False):\n\"\"\"\n    Builds an embeddings index. This method overwrites an existing index.\n\n    Args:\n        documents: list of (id, data, tags)\n        reindex: if this is a reindex operation in which case database creation is skipped, defaults to False\n    \"\"\"\n\n    # Set configuration to default configuration, if empty\n    if not self.config:\n        self.configure(self.defaults())\n\n    # Create document database, if necessary\n    if not reindex:\n        self.database = self.createdatabase()\n\n        # Reset archive since this is a new index\n        self.archive = None\n\n    # Create graph, if necessary\n    self.graph = self.creategraph()\n\n    # Create transform action\n    transform = Transform(self, Action.REINDEX if reindex else Action.INDEX)\n\n    with tempfile.NamedTemporaryFile(mode=\"wb\", suffix=\".npy\") as buffer:\n        # Load documents into database and transform to vectors\n        ids, dimensions, embeddings = transform(documents, buffer)\n        if ids:\n            # Build LSA model (if enabled). Remove principal components from embeddings.\n            if self.config.get(\"pca\"):\n                self.reducer = Reducer(embeddings, self.config[\"pca\"])\n                self.reducer(embeddings)\n\n            # Normalize embeddings\n            self.normalize(embeddings)\n\n            # Save index dimensions\n            self.config[\"dimensions\"] = dimensions\n\n            # Create approximate nearest neighbor index\n            self.ann = ANNFactory.create(self.config)\n\n            # Add embeddings to the index\n            self.ann.index(embeddings)\n\n            # Save indexids-ids mapping for indexes with no database, except when this is a reindex action\n            if not reindex and not self.database:\n                self.config[\"ids\"] = ids\n\n    # Index graph, if necessary\n    if self.graph:\n        self.graph.index(Search(self, True), self.batchsimilarity)\n</code></pre>"},{"location":"embeddings/methods/#txtai.embeddings.base.Embeddings.info","title":"<code>info(self)</code>","text":"<p>Prints the current embeddings index configuration.</p> Source code in <code>txtai/embeddings/base.py</code> <pre><code>def info(self):\n\"\"\"\n    Prints the current embeddings index configuration.\n    \"\"\"\n\n    # Copy and edit config\n    config = self.config.copy()\n\n    # Remove ids array if present\n    config.pop(\"ids\", None)\n\n    # Print configuration\n    print(json.dumps(config, sort_keys=True, default=str, indent=2))\n</code></pre>"},{"location":"embeddings/methods/#txtai.embeddings.base.Embeddings.load","title":"<code>load(self, path, cloud=None)</code>","text":"<p>Loads an existing index from path.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <p>input path</p> required <code>cloud</code> <p>cloud storage configuration</p> <code>None</code> Source code in <code>txtai/embeddings/base.py</code> <pre><code>def load(self, path, cloud=None):\n\"\"\"\n    Loads an existing index from path.\n\n    Args:\n        path: input path\n        cloud: cloud storage configuration\n    \"\"\"\n\n    # Check if this is an archive file and extract\n    path, apath = self.checkarchive(path)\n    if apath:\n        self.archive.load(apath, cloud)\n\n    # Index configuration\n    with open(f\"{path}/config\", \"rb\") as handle:\n        self.config = pickle.load(handle)\n\n        # Build full path to embedding vectors file\n        if self.config.get(\"storevectors\"):\n            self.config[\"path\"] = os.path.join(path, self.config[\"path\"])\n\n    # Approximate nearest neighbor index - stores embeddings vectors\n    self.ann = ANNFactory.create(self.config)\n    self.ann.load(f\"{path}/embeddings\")\n\n    # Dimensionality reduction model - word vectors only\n    if self.config.get(\"pca\"):\n        self.reducer = Reducer()\n        self.reducer.load(f\"{path}/lsa\")\n\n    # Embedding scoring index - word vectors only\n    if self.config.get(\"scoring\"):\n        self.scoring = ScoringFactory.create(self.config[\"scoring\"])\n        self.scoring.load(f\"{path}/scoring\")\n\n    # Sentence vectors model - transforms data to embeddings vectors\n    self.model = self.loadvectors()\n\n    # Query model\n    self.query = self.loadquery()\n\n    # Document database - stores document content\n    self.database = self.createdatabase()\n    if self.database:\n        self.database.load(f\"{path}/documents\")\n\n    # Graph network - stores relationships\n    self.graph = self.creategraph()\n    if self.graph:\n        self.graph.load(f\"{path}/graph\")\n</code></pre>"},{"location":"embeddings/methods/#txtai.embeddings.base.Embeddings.reindex","title":"<code>reindex(self, config, columns=None, function=None)</code>","text":"<p>Recreates the approximate nearest neighbor (ann) index using config. This method only works if document content storage is enabled.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <p>new config</p> required <code>columns</code> <p>optional list of document columns used to rebuild data</p> <code>None</code> <code>function</code> <p>optional function to prepare content for indexing</p> <code>None</code> Source code in <code>txtai/embeddings/base.py</code> <pre><code>def reindex(self, config, columns=None, function=None):\n\"\"\"\n    Recreates the approximate nearest neighbor (ann) index using config. This method only works if document\n    content storage is enabled.\n\n    Args:\n        config: new config\n        columns: optional list of document columns used to rebuild data\n        function: optional function to prepare content for indexing\n    \"\"\"\n\n    if self.database:\n        # Keep content and objects parameters to ensure database is preserved\n        config[\"content\"] = self.config[\"content\"]\n        if \"objects\" in self.config:\n            config[\"objects\"] = self.config[\"objects\"]\n\n        # Reset configuration\n        self.configure(config)\n\n        # Reindex\n        if function:\n            self.index(function(self.database.reindex(columns)), True)\n        else:\n            self.index(self.database.reindex(columns), True)\n</code></pre>"},{"location":"embeddings/methods/#txtai.embeddings.base.Embeddings.save","title":"<code>save(self, path, cloud=None)</code>","text":"<p>Saves an index in a directory at path unless path ends with tar.gz, tar.bz2, tar.xz or zip. In those cases, the index is stored as a compressed file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <p>output path</p> required <code>cloud</code> <p>cloud storage configuration</p> <code>None</code> Source code in <code>txtai/embeddings/base.py</code> <pre><code>def save(self, path, cloud=None):\n\"\"\"\n    Saves an index in a directory at path unless path ends with tar.gz, tar.bz2, tar.xz or zip.\n    In those cases, the index is stored as a compressed file.\n\n    Args:\n        path: output path\n        cloud: cloud storage configuration\n    \"\"\"\n\n    if self.config:\n        # Check if this is an archive file\n        path, apath = self.checkarchive(path)\n\n        # Create output directory, if necessary\n        os.makedirs(path, exist_ok=True)\n\n        # Copy sentence vectors model\n        if self.config.get(\"storevectors\"):\n            shutil.copyfile(self.config[\"path\"], os.path.join(path, os.path.basename(self.config[\"path\"])))\n\n            self.config[\"path\"] = os.path.basename(self.config[\"path\"])\n\n        # Write index configuration\n        with open(f\"{path}/config\", \"wb\") as handle:\n            pickle.dump(self.config, handle, protocol=__pickle__)\n\n        # Save approximate nearest neighbor index\n        self.ann.save(f\"{path}/embeddings\")\n\n        # Save dimensionality reduction model (word vectors only)\n        if self.reducer:\n            self.reducer.save(f\"{path}/lsa\")\n\n        # Save embedding scoring index (word vectors only)\n        if self.scoring:\n            self.scoring.save(f\"{path}/scoring\")\n\n        # Save document database\n        if self.database:\n            self.database.save(f\"{path}/documents\")\n\n        # Save graph\n        if self.graph:\n            self.graph.save(f\"{path}/graph\")\n\n        # If this is an archive, save it\n        if apath:\n            self.archive.save(apath, cloud)\n</code></pre>"},{"location":"embeddings/methods/#txtai.embeddings.base.Embeddings.score","title":"<code>score(self, documents)</code>","text":"<p>Builds a scoring index. Only used by word vectors models.</p> <p>Parameters:</p> Name Type Description Default <code>documents</code> <p>list of (id, data, tags)</p> required Source code in <code>txtai/embeddings/base.py</code> <pre><code>def score(self, documents):\n\"\"\"\n    Builds a scoring index. Only used by word vectors models.\n\n    Args:\n        documents: list of (id, data, tags)\n    \"\"\"\n\n    # Build scoring index over documents\n    if self.scoring:\n        self.scoring.index(documents)\n</code></pre>"},{"location":"embeddings/methods/#txtai.embeddings.base.Embeddings.search","title":"<code>search(self, query, limit=None)</code>","text":"<p>Finds documents most similar to the input queries. This method will run either an approximate nearest neighbor (ann) search or an approximate nearest neighbor + database search depending on if a database is available.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <p>input query</p> required <code>limit</code> <p>maximum results</p> <code>None</code> <p>Returns:</p> Type Description <p>list of (id, score) for ann search, list of dict for an ann+database search</p> Source code in <code>txtai/embeddings/base.py</code> <pre><code>def search(self, query, limit=None):\n\"\"\"\n    Finds documents most similar to the input queries. This method will run either an approximate\n    nearest neighbor (ann) search or an approximate nearest neighbor + database search depending\n    on if a database is available.\n\n    Args:\n        query: input query\n        limit: maximum results\n\n    Returns:\n        list of (id, score) for ann search, list of dict for an ann+database search\n    \"\"\"\n\n    results = self.batchsearch([query], limit)\n    return results[0] if results else results\n</code></pre>"},{"location":"embeddings/methods/#txtai.embeddings.base.Embeddings.similarity","title":"<code>similarity(self, query, data)</code>","text":"<p>Computes the similarity between query and list of data. Returns a list of (id, score) sorted by highest score, where id is the index in data.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <p>input query</p> required <code>data</code> <p>list of data</p> required <p>Returns:</p> Type Description <p>list of (id, score)</p> Source code in <code>txtai/embeddings/base.py</code> <pre><code>def similarity(self, query, data):\n\"\"\"\n    Computes the similarity between query and list of data. Returns a list of\n    (id, score) sorted by highest score, where id is the index in data.\n\n    Args:\n        query: input query\n        data: list of data\n\n    Returns:\n        list of (id, score)\n    \"\"\"\n\n    return self.batchsimilarity([query], data)[0]\n</code></pre>"},{"location":"embeddings/methods/#txtai.embeddings.base.Embeddings.transform","title":"<code>transform(self, document)</code>","text":"<p>Transforms document into an embeddings vector.</p> <p>Parameters:</p> Name Type Description Default <code>document</code> <p>(id, data, tags)</p> required <p>Returns:</p> Type Description <p>embeddings vector</p> Source code in <code>txtai/embeddings/base.py</code> <pre><code>def transform(self, document):\n\"\"\"\n    Transforms document into an embeddings vector.\n\n    Args:\n        document: (id, data, tags)\n\n    Returns:\n        embeddings vector\n    \"\"\"\n\n    return self.batchtransform([document])[0]\n</code></pre>"},{"location":"embeddings/methods/#txtai.embeddings.base.Embeddings.upsert","title":"<code>upsert(self, documents)</code>","text":"<p>Runs an embeddings upsert operation. If the index exists, new data is appended to the index, existing data is updated. If the index doesn't exist, this method runs a standard index operation.</p> <p>Parameters:</p> Name Type Description Default <code>documents</code> <p>list of (id, data, tags)</p> required Source code in <code>txtai/embeddings/base.py</code> <pre><code>def upsert(self, documents):\n\"\"\"\n    Runs an embeddings upsert operation. If the index exists, new data is\n    appended to the index, existing data is updated. If the index doesn't exist,\n    this method runs a standard index operation.\n\n    Args:\n        documents: list of (id, data, tags)\n    \"\"\"\n\n    # Run standard insert if index doesn't exist\n    if not self.ann:\n        self.index(documents)\n        return\n\n    # Create transform action\n    transform = Transform(self, Action.UPSERT)\n\n    with tempfile.NamedTemporaryFile(mode=\"wb\", suffix=\".npy\") as buffer:\n        # Load documents into database and transform to vectors\n        ids, _, embeddings = transform(documents, buffer)\n        if ids:\n            # Remove principal components from embeddings, if necessary\n            if self.reducer:\n                self.reducer(embeddings)\n\n            # Normalize embeddings\n            self.normalize(embeddings)\n\n            # Append embeddings to the index\n            self.ann.append(embeddings)\n\n            # Save indexids-ids mapping for indexes with no database\n            if not self.database:\n                self.config[\"ids\"] = self.config[\"ids\"] + ids\n\n    # Graph upsert, if necessary\n    if self.graph:\n        self.graph.upsert(Search(self, True))\n</code></pre>"},{"location":"embeddings/query/","title":"Query guide","text":"<p>This section covers how to query data with txtai. The simplest way to search for data is building a natural language string with the desired content to find. txtai also supports querying with SQL. We'll cover both methods here.</p>"},{"location":"embeddings/query/#natural-language-queries","title":"Natural language queries","text":"<p>In the simplest case, the query is text and the results are index text that is most similar to the query text.</p> <pre><code>embeddings.search(\"feel good story\")\nembeddings.search(\"wildlife\")\n</code></pre> <p>The queries above search the index for similarity matches on <code>feel good story</code> and <code>wildlife</code>. If content storage is enabled, a list of <code>{**query columns}</code> is returned. Otherwise, a list of <code>(id, score)</code> tuples are returned.</p>"},{"location":"embeddings/query/#sql","title":"SQL","text":"<p>txtai supports more complex queries with SQL. This is only supported if content storage is enabled. txtai has a translation layer that analyzes input SQL statements and combines similarity results with content stored in a relational database.</p> <p>SQL queries are run through <code>embeddings.search</code> like natural language queries but the examples below only show the SQL query for conciseness.</p> <pre><code>embeddings.search(\"SQL query\")\n</code></pre>"},{"location":"embeddings/query/#similar-clause","title":"Similar clause","text":"<p>The similar clause is a txtai function that enables similarity searches with SQL.</p> <pre><code>SELECT id, text, score FROM txtai WHERE similar('feel good story')\nSELECT id, text, score FROM txtai WHERE similar('feel good story')\n</code></pre> <p>The similar clause takes two arguments:</p> <pre><code>similar(\"query\", \"number of candidates\")\n</code></pre> Argument Description query natural language query to run number of candidates number of candidate results to return <p>The txtai query layer has to join results from two separate components, a relational store and a similarity index. With a similar clause, a similarity search is run and those ids are fed to the underlying database query.</p> <p>The number of candidates should be larger than the desired number of results when applying additional filter clauses. This ensures that <code>limit</code> results are still returned after applying additional filters. If the number of candidates is not specified, it is defaulted as follows:</p> <ul> <li>For a single query filter clause, the default is the query limit</li> <li>With multiple filtering clauses, the default is 10x the query limit</li> </ul>"},{"location":"embeddings/query/#dynamic-columns","title":"Dynamic columns","text":"<p>Content can be indexed in multiple ways when content storage is enabled. Remember that input documents take the form of <code>(id, data, tags)</code> tuples. If data is a string, then content is primarily filtered with similar clauses. If data is a dictionary, then all fields in the dictionary are indexed and searchable.</p> <p>For example:</p> <pre><code>embeddings.index([(0, {\"text\": \"text to index\", \"flag\": True,\n                       \"entry\": \"2022-01-01\"}, None)])\n</code></pre> <p>With the above input data, queries can now have more complex filters.</p> <pre><code>SELECT text, flag, entry FROM txtai WHERE similar('query') AND flag = 1\nAND entry &gt;= '2022-01-01'\n</code></pre> <p>txtai's query layer automatically detects columns and translates queries into a format that can be understood by the underlying database.</p> <p>Nested dictionaries/JSON is supported and can be escaped with bracket statements.</p> <pre><code>embeddings.index([(0, {\"text\": \"text to index\",\n                       \"parent\": {\"child element\": \"abc\"}}, None)])\n</code></pre> <pre><code>SELECT text FROM txtai WHERE [parent.child element] ='abc'\n</code></pre> <p>Note the bracket statement escaping the nested column with spaces in the name.</p>"},{"location":"embeddings/query/#aggregation-queries","title":"Aggregation queries","text":"<p>The goal of txtai's query language is to closely support all functions in the underlying database engine. The main challenge is ensuring dynamic columns are properly escaped into the engines native query function. </p> <p>Aggregation query examples.</p> <pre><code>SELECT count(*) FROM txtai WHERE similar('feel good story') AND score &gt;= 0.15\nSELECT max(length(text)) FROM txtai WHERE similar('feel good story')\nAND score &gt;= 0.15\nSELECT count(*), flag FROM txtai GROUP BY flag ORDER BY count(*) DESC\n</code></pre>"},{"location":"embeddings/query/#binary-objects","title":"Binary objects","text":"<p>txtai has support for storing and retrieving binary objects. Binary objects can be retrieved as shown in the example below.</p> <pre><code># Get an image\nrequest = open(\"demo.gif\")\n\n# Insert record\nembeddings.index([(\"txtai\", {\"text\": \"txtai executes machine-learning workflows.\",\n                             \"object\": request.read()}, None)])\n\n# Query txtai and get associated object\nquery = \"select object from txtai where similar('machine learning') limit 1\"\nresult = embeddings.search(query)[0][\"object\"]\n</code></pre>"},{"location":"embeddings/query/#custom-sql-functions","title":"Custom SQL functions","text":"<p>Custom, user-defined SQL functions extend selection, filtering and ordering clauses with additional logic. For example, the following snippet defines a function that translates text using a translation pipeline.</p> <pre><code># Translation pipeline\ntranslate = Translation()\n\n# Create embeddings index\nembeddings = Embeddings({\"path\": \"sentence-transformers/nli-mpnet-base-v2\",\n                         \"content\": True,\n                         \"functions\": [translate]})\n\n# Run a search using a custom SQL function\nembeddings.search(\"\"\"\nselect\n  text,\n  translation(text, 'de', null) 'text (DE)',\n  translation(text, 'es', null) 'text (ES)',\n  translation(text, 'fr', null) 'text (FR)'\nfrom txtai where similar('feel good story')\nlimit 1\n\"\"\")\n</code></pre>"},{"location":"embeddings/query/#query-translation","title":"Query translation","text":"<p>Natural language queries with filters can be converted to txtai-compatible SQL statements with query translation. For example:</p> <pre><code>embeddings.search(\"feel good story since yesterday\")\n</code></pre> <p>can be converted to a SQL statement with a similar clause and date filter.</p> <pre><code>select id, text, score from txtai where similar('feel good story') and\nentry &gt;= date('now', '-1 day')\n</code></pre> <p>This requires setting a query translation model. The default query translation model is t5-small-txtsql but this can easily be finetuned to handle different use cases.</p>"},{"location":"embeddings/query/#combined-index-architecture","title":"Combined index architecture","text":"<p>When content storage is enabled, txtai becomes a dual storage engine. Content is stored in an underlying database (currently supports SQLite) along with an Approximate Nearest Neighbor (ANN) index. These components combine to deliver similarity search alongside traditional structured search.</p> <p>The ANN index stores ids and vectors for each input element. When a natural language query is run, the query is translated into a vector and a similarity query finds the best matching ids. When a database is added into the mix, an additional step is applied. This step takes those ids and effectively inserts them as part of the underlying database query.</p> <p>Dynamic columns are supported via the underlying engine. For SQLite, data is stored as JSON and dynamic columns are converted into <code>json_extract</code> clauses. This same concept can be expanded to other storage engines like PostgreSQL and could even work with NoSQL stores. </p>"},{"location":"embeddings/configuration/","title":"Configuration","text":"<p>This following describes available embeddings configuration. These parameters are set via the Embeddings constructor.</p>"},{"location":"embeddings/configuration/#path","title":"path","text":"<pre><code>path: string\n</code></pre> <p>Sets the path for a vectors model. When using a transformers/sentence-transformers model, this can be any model on the Hugging Face Model Hub or a local file path. Otherwise, it must be a local file path to a word embeddings model.</p>"},{"location":"embeddings/configuration/#method","title":"method","text":"<pre><code>method: transformers|sentence-transformers|words|external\n</code></pre> <p>Sentence embeddings method to use. If the method is not provided, it is inferred using the <code>path</code>.</p> <p><code>sentence-transformers</code> and <code>words</code> require the similarity extras package to be installed.</p>"},{"location":"embeddings/configuration/#transformers","title":"transformers","text":"<p>Builds sentence embeddings using a transformers model. While this can be any transformers model, it works best with models trained to build sentence embeddings.</p>"},{"location":"embeddings/configuration/#sentence-transformers","title":"sentence-transformers","text":"<p>Same as transformers but loads models with the sentence-transformers library.</p>"},{"location":"embeddings/configuration/#words","title":"words","text":"<p>Builds sentence embeddings using a word embeddings model. Transformers models are the preferred vector backend in most cases. Word embeddings models may be deprecated in the future.</p>"},{"location":"embeddings/configuration/#storevectors","title":"storevectors","text":"<pre><code>storevectors: boolean\n</code></pre> <p>Enables copying of a vectors model set in path into the embeddings models output directory on save. This option enables a fully encapsulated index with no external file dependencies.</p>"},{"location":"embeddings/configuration/#scoring","title":"scoring","text":"<pre><code>scoring: bm25|tfidf|sif\n</code></pre> <p>A scoring model builds weighted averages of word vectors for a given sentence. Supports BM25, TF-IDF and SIF (smooth inverse frequency) methods. If a scoring method is not provided, mean sentence embeddings are built.</p>"},{"location":"embeddings/configuration/#pca","title":"pca","text":"<pre><code>pca: int\n</code></pre> <p>Removes n principal components from generated sentence embeddings. When enabled, a TruncatedSVD model is built to help with dimensionality reduction. After pooling of vectors creates a single sentence embedding, this method is applied.</p>"},{"location":"embeddings/configuration/#external","title":"external","text":"<p>Sentence embeddings are loaded via an external model or API. Requires setting the transform parameter to a function that translates data into vectors.</p>"},{"location":"embeddings/configuration/#transform","title":"transform","text":"<pre><code>transform: function\n</code></pre> <p>When method is <code>external</code>, this function transforms input content into embeddings. The input to this function is a list of data. This method must return either a numpy array or list of numpy arrays.</p>"},{"location":"embeddings/configuration/#batch","title":"batch","text":"<pre><code>batch: int\n</code></pre> <p>Sets the transform batch size. This parameter controls how input streams are chunked and vectorized.</p>"},{"location":"embeddings/configuration/#encodebatch","title":"encodebatch","text":"<pre><code>encodebatch: int\n</code></pre> <p>Sets the encode batch size. This parameter controls the underlying vector model batch size. This often corresponds to a GPU batch size, which controls GPU memory usage.</p>"},{"location":"embeddings/configuration/#tokenize","title":"tokenize","text":"<pre><code>tokenize: boolean\n</code></pre> <p>Enables string tokenization (defaults to false). This method applies tokenization rules that only work with English language text and may increase the quality of English language sentence embeddings in some situations.</p>"},{"location":"embeddings/configuration/#backend","title":"backend","text":"<pre><code>backend: faiss|hnsw|annoy\n</code></pre> <p>Approximate Nearest Neighbor (ANN) index backend for storing generated sentence embeddings. <code>Defaults to faiss</code>. Additional backends require the similarity extras package to be installed.</p> <p>Backend-specific settings are set with a corresponding configuration object having the same name as the backend (i.e. annoy, faiss, or hnsw). None of these are required and are set to defaults if omitted.</p>"},{"location":"embeddings/configuration/#faiss","title":"faiss","text":"<pre><code>faiss:\ncomponents: comma separated list of components - defaults to \"Flat\" for small\nindices and \"IVFx,Flat\" for larger indexes where\nx = 4 * sqrt(embeddings count)\nnprobe: search probe setting (int) - defaults to x/16 (as defined above)\nfor larger indexes\nquantize: store vectors with 8-bit precision vs 32-bit (boolean)\ndefaults to false\nmmap: load as on-disk index (boolean) - trade query response time for a\nsmaller RAM footprint, defaults to false\n</code></pre> <p>See the following Faiss documentation links for more information.</p> <ul> <li>Guidelines for choosing an index</li> <li>Index configuration summary</li> <li>Index Factory</li> <li>Search Tuning</li> </ul>"},{"location":"embeddings/configuration/#hnsw","title":"hnsw","text":"<pre><code>hnsw:\nefconstruction:  ef_construction param for init_index (int) - defaults to 200\nm: M param for init_index (int) - defaults to 16\nrandomseed: random-seed param for init_index (int) - defaults to 100\nefsearch: ef search param (int) - defaults to None and not set\n</code></pre> <p>See Hnswlib documentation for more information on these parameters.</p>"},{"location":"embeddings/configuration/#annoy","title":"annoy","text":"<pre><code>annoy:\nntrees: number of trees (int) - defaults to 10\nsearchk: search_k search setting (int) - defaults to -1\n</code></pre> <p>See Annoy documentation for more information on these parameters. Note that annoy indexes can not be modified after creation, upserts/deletes and other modifications are not supported.</p>"},{"location":"embeddings/configuration/#content","title":"content","text":"<pre><code>content: string|boolean\n</code></pre> <p>Enables content storage. When true, the default content storage engine will be used. Otherwise, the string must specify the supported content storage engine to use.</p>"},{"location":"embeddings/configuration/#functions","title":"functions","text":"<pre><code>functions: list\n</code></pre> <p>List of functions with user-defined SQL functions, only used when content is enabled. Each list element must be one of the following:</p> <ul> <li>function</li> <li>callable object</li> <li>dict with fields for name, argcount and function</li> </ul> <p>An example can be found here.</p>"},{"location":"embeddings/configuration/#query","title":"query","text":"<pre><code>query:\npath: sets the path for the query model - this can be any model on the\nHugging Face Model Hub or a local file path.\nprefix: text prefix to prepend to all inputs\nmaxlength: maximum generated sequence length\n</code></pre> <p>Query translation model. Translates natural language queries to txtai compatible SQL statements.</p>"},{"location":"embeddings/configuration/#graph","title":"graph","text":"<pre><code>graph:\nbackend: graph network backend (string), defaults to \"networkx\"\nbatchsize: batch query size, used to query embeddings index (int)\ndefaults to 256\nlimit: maximum number of results to return per embeddings query (int)\ndefaults to 15\nminscore: minimum score required to consider embeddings query matches (float)\ndefaults to 0.1\napproximate: when true, queries only run for nodes without edges (boolean)\ndefaults to true\ntopics: see below\n</code></pre> <p>Enables graph storage. When set, a graph network is built using the embeddings index. Graph nodes are synced with each embeddings index operation (index/upsert/delete). Graph edges are created using the embeddings index upon completion of each index/upsert/delete embeddings index call.</p> <p>Defaults are tuned so that in most cases these values don't need to be changed. </p>"},{"location":"embeddings/configuration/#topics","title":"topics","text":"<pre><code>topics:\nalgorithm: community detection algorithm (string), options are\nlouvain (default), greedy, lpa\nlevel: controls number of topics (string), options are best (default) or first\nresolution: controls number of topics (int), larger values create more\ntopics (int), defaults to 100\nlabels: scoring index method used to build topic labels (string)\noptions are bm25 (default), tfidf, sif\nterms: number of frequent terms to use for topic labels (int), defaults to 4\ncategories: optional list of categories used to group topics, allows\ngranular topics with broad categories grouping topics\n</code></pre> <p>Enables topic modeling. Defaults are tuned so that in most cases these values don't need to be changed (except for categories). These parameters are available for advanced use cases where one wants full control over the community detection process.</p>"},{"location":"embeddings/configuration/cloud/","title":"Cloud","text":"<p>This section describes parameters used to sync compressed indexes with cloud storage. These parameters are only enabled if an embeddings index is stored as compressed. They are set via the embeddings.load and embeddings.save methods.</p>"},{"location":"embeddings/configuration/cloud/#provider","title":"provider","text":"<pre><code>provider: string\n</code></pre> <p>The cloud storage provider, see full list of providers here.</p>"},{"location":"embeddings/configuration/cloud/#container","title":"container","text":"<pre><code>container: string\n</code></pre> <p>Container/bucket/directory name.</p>"},{"location":"embeddings/configuration/cloud/#key","title":"key","text":"<pre><code>key: string\n</code></pre> <p>Provider-specific access key. Can also be set via ACCESS_KEY environment variable. Ensure the configuration file is secured if added to the file.</p>"},{"location":"embeddings/configuration/cloud/#secret","title":"secret","text":"<pre><code>secret: string\n</code></pre> <p>Provider-specific access secret. Can also be set via ACCESS_SECRET environment variable. Ensure the configuration file is secured if added to the file.</p>"},{"location":"embeddings/configuration/cloud/#host","title":"host","text":"<pre><code>host: string\n</code></pre> <p>Optional server host name. Set when using a local cloud storage server.</p>"},{"location":"embeddings/configuration/cloud/#port","title":"port","text":"<pre><code>port: int\n</code></pre> <p>Optional server port. Set when using a local cloud storage server.</p>"},{"location":"embeddings/configuration/cloud/#token","title":"token","text":"<pre><code>token: string\n</code></pre> <p>Optional temporary session token</p>"},{"location":"embeddings/configuration/cloud/#region","title":"region","text":"<pre><code>region: string\n</code></pre> <p>Optional parameter to specify the storage region, provider-specific.</p>"},{"location":"pipeline/","title":"Pipeline","text":"<p>txtai provides a generic pipeline processing framework with the only interface requirement being a <code>__call__</code> method. Pipelines are flexible and process various types of data. Pipelines can wrap machine learning models as well as other processes.</p> <p>Pipelines are run with Python or configuration. Pipelines can be instantiated in configuration using the lower case name of the pipeline. Configuration-driven pipelines are run with workflows or the API.</p>"},{"location":"pipeline/#list-of-pipelines","title":"List of pipelines","text":"<p>The following is a list of the default pipelines available in txtai.</p> <ul> <li>Audio<ul> <li>TextToSpeech</li> <li>Transcription</li> </ul> </li> <li>Data Processing<ul> <li>Segmentation</li> <li>Tabular</li> <li>Text extraction</li> </ul> </li> <li>Image<ul> <li>Caption</li> <li>Image Hash</li> <li>Objects</li> </ul> </li> <li>Text<ul> <li>Entity</li> <li>Extractive QA</li> <li>Generator</li> <li>Labeling</li> <li>Sequences</li> <li>Similarity</li> <li>Summary</li> <li>Translation</li> </ul> </li> <li>Training<ul> <li>HF ONNX</li> <li>ML ONNX</li> <li>Trainer</li> </ul> </li> </ul>"},{"location":"pipeline/audio/texttospeech/","title":"Text To Speech","text":"<p>The Text To Speech pipeline generates speech from text.</p>"},{"location":"pipeline/audio/texttospeech/#example","title":"Example","text":"<p>The following shows a simple example using this pipeline.</p> <pre><code>from txtai.pipeline import TextToSpeech\n\n# Create and run pipeline\ntts = TextToSpeech()\ntts(\"Say something here\")\n</code></pre> <p>See the link below for a more detailed example.</p> Notebook Description Text to speech generation Generate speech from text <p>This pipeline is backed by ONNX models from the Hugging Face Hub. The following models are currently available.</p> <ul> <li>ljspeech-jets-onnx</li> <li>ljspeech-vits-onnx</li> </ul>"},{"location":"pipeline/audio/texttospeech/#configuration-driven-example","title":"Configuration-driven example","text":"<p>Pipelines are run with Python or configuration. Pipelines can be instantiated in configuration using the lower case name of the pipeline. Configuration-driven pipelines are run with workflows or the API.</p>"},{"location":"pipeline/audio/texttospeech/#configyml","title":"config.yml","text":"<pre><code># Create pipeline using lower case class name\ntexttospeech:\n\n# Run pipeline with workflow\nworkflow:\ntts:\ntasks:\n- action: texttospeech\n</code></pre>"},{"location":"pipeline/audio/texttospeech/#run-with-workflows","title":"Run with Workflows","text":"<pre><code>from txtai.app import Application\n\n# Create and run pipeline with workflow\napp = Application(\"config.yml\")\nlist(app.workflow(\"tts\", [\"Say something here\"]))\n</code></pre>"},{"location":"pipeline/audio/texttospeech/#run-with-api","title":"Run with API","text":"<pre><code>CONFIG=config.yml uvicorn \"txtai.api:app\" &amp;\n\ncurl \\\n-X POST \"http://localhost:8000/workflow\" \\\n-H \"Content-Type: application/json\" \\\n-d '{\"name\":\"tts\", \"elements\":[\"Say something here\"]}'\n</code></pre>"},{"location":"pipeline/audio/texttospeech/#methods","title":"Methods","text":"<p>Python documentation for the pipeline.</p>"},{"location":"pipeline/audio/texttospeech/#txtai.pipeline.audio.texttospeech.TextToSpeech.__init__","title":"<code>__init__(self, path='NeuML/ljspeech-jets-onnx', maxtokens=512)</code>  <code>special</code>","text":"<p>Creates a new TextToSpeech pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <p>optional Hugging Face model hub id</p> <code>'NeuML/ljspeech-jets-onnx'</code> <code>maxtokens</code> <p>maximum number of tokens model can process, defaults to 512</p> <code>512</code> Source code in <code>txtai/pipeline/audio/texttospeech.py</code> <pre><code>def __init__(self, path=\"NeuML/ljspeech-jets-onnx\", maxtokens=512):\n\"\"\"\n    Creates a new TextToSpeech pipeline.\n\n    Args:\n        path: optional Hugging Face model hub id\n        maxtokens: maximum number of tokens model can process, defaults to 512\n    \"\"\"\n\n    if not TTS:\n        raise ImportError('TextToSpeech pipeline is not available - install \"pipeline\" extra to enable')\n\n    # Get path to model and config\n    config = hf_hub_download(path, filename=\"config.yaml\")\n    model = hf_hub_download(path, filename=\"model.onnx\")\n\n    # Read yaml config\n    with open(config, \"r\", encoding=\"utf-8\") as f:\n        config = yaml.safe_load(f)\n\n    # Create tokenizer\n    tokens = config.get(\"token\", {}).get(\"list\")\n    self.tokenizer = TTSTokenizer(tokens)\n\n    # Create ONNX Session\n    self.model = ort.InferenceSession(model, ort.SessionOptions(), self.providers())\n\n    # Max number of input tokens model can handle\n    self.maxtokens = maxtokens\n\n    # Get model input name, typically \"text\"\n    self.input = self.model.get_inputs()[0].name\n</code></pre>"},{"location":"pipeline/audio/texttospeech/#txtai.pipeline.audio.texttospeech.TextToSpeech.__call__","title":"<code>__call__(self, text)</code>  <code>special</code>","text":"<p>Generates speech from text. Text longer than maxtokens will be batched and returned as a single waveform per text input.</p> <p>This method supports files as a string or a list. If the input is a string, the return type is string. If text is a list, the return type is a list.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <p>text|list</p> required <p>Returns:</p> Type Description <p>list of speech as NumPy array waveforms</p> Source code in <code>txtai/pipeline/audio/texttospeech.py</code> <pre><code>def __call__(self, text):\n\"\"\"\n    Generates speech from text. Text longer than maxtokens will be batched and returned\n    as a single waveform per text input.\n\n    This method supports files as a string or a list. If the input is a string,\n    the return type is string. If text is a list, the return type is a list.\n\n    Args:\n        text: text|list\n\n    Returns:\n        list of speech as NumPy array waveforms\n    \"\"\"\n\n    # Convert results to a list if necessary\n    texts = [text] if isinstance(text, str) else text\n\n    outputs = []\n    for x in texts:\n        # Truncate to max size model can handle\n        x = self.tokenizer(x)\n\n        # Run input through model and store result\n        result = self.execute(x)\n        outputs.append(result)\n\n    # Return results\n    return outputs[0] if isinstance(text, str) else outputs\n</code></pre>"},{"location":"pipeline/audio/transcription/","title":"Transcription","text":"<p>The Transcription pipeline converts speech in audio files to text.</p>"},{"location":"pipeline/audio/transcription/#example","title":"Example","text":"<p>The following shows a simple example using this pipeline.</p> <pre><code>from txtai.pipeline import Transcription\n\n# Create and run pipeline\ntranscribe = Transcription()\ntranscribe(\"path to wav file\")\n</code></pre> <p>See the link below for a more detailed example.</p> Notebook Description Transcribe audio to text Convert audio files to text"},{"location":"pipeline/audio/transcription/#configuration-driven-example","title":"Configuration-driven example","text":"<p>Pipelines are run with Python or configuration. Pipelines can be instantiated in configuration using the lower case name of the pipeline. Configuration-driven pipelines are run with workflows or the API.</p>"},{"location":"pipeline/audio/transcription/#configyml","title":"config.yml","text":"<pre><code># Create pipeline using lower case class name\ntranscription:\n\n# Run pipeline with workflow\nworkflow:\ntranscribe:\ntasks:\n- action: transcription\n</code></pre>"},{"location":"pipeline/audio/transcription/#run-with-workflows","title":"Run with Workflows","text":"<pre><code>from txtai.app import Application\n\n# Create and run pipeline with workflow\napp = Application(\"config.yml\")\nlist(app.workflow(\"transcribe\", [\"path to wav file\"]))\n</code></pre>"},{"location":"pipeline/audio/transcription/#run-with-api","title":"Run with API","text":"<pre><code>CONFIG=config.yml uvicorn \"txtai.api:app\" &amp;\n\ncurl \\\n-X POST \"http://localhost:8000/workflow\" \\\n-H \"Content-Type: application/json\" \\\n-d '{\"name\":\"transcribe\", \"elements\":[\"path to wav file\"]}'\n</code></pre>"},{"location":"pipeline/audio/transcription/#methods","title":"Methods","text":"<p>Python documentation for the pipeline.</p>"},{"location":"pipeline/audio/transcription/#txtai.pipeline.audio.transcription.Transcription.__init__","title":"<code>__init__(self, path=None, quantize=False, gpu=True, model=None)</code>  <code>special</code>","text":"Source code in <code>txtai/pipeline/audio/transcription.py</code> <pre><code>def __init__(self, path=None, quantize=False, gpu=True, model=None):\n    if not SOUNDFILE:\n        raise ImportError(\"SoundFile library not installed or libsndfile not found\")\n\n    # Call parent constructor\n    super().__init__(\"automatic-speech-recognition\", path, quantize, gpu, model)\n</code></pre>"},{"location":"pipeline/audio/transcription/#txtai.pipeline.audio.transcription.Transcription.__call__","title":"<code>__call__(self, audio, rate=None, chunk=10, join=True)</code>  <code>special</code>","text":"<p>Transcribes audio files or data to text.</p> <p>This method supports a single audio element or a list of audio. If the input is audio, the return type is a string. If text is a list, a list of strings is returned</p> <p>Parameters:</p> Name Type Description Default <code>audio</code> <p>audio|list</p> required <code>rate</code> <p>sample rate, only required with raw audio data</p> <code>None</code> <code>chunk</code> <p>process audio in chunk second sized segments</p> <code>10</code> <code>join</code> <p>if True (default), combine each chunk back together into a single text output.   When False, chunks are returned as a list of dicts, each having raw associated audio and   sample rate in addition to text</p> <code>True</code> <p>Returns:</p> Type Description <p>list of transcribed text</p> Source code in <code>txtai/pipeline/audio/transcription.py</code> <pre><code>def __call__(self, audio, rate=None, chunk=10, join=True):\n\"\"\"\n    Transcribes audio files or data to text.\n\n    This method supports a single audio element or a list of audio. If the input is audio, the return\n    type is a string. If text is a list, a list of strings is returned\n\n    Args:\n        audio: audio|list\n        rate: sample rate, only required with raw audio data\n        chunk: process audio in chunk second sized segments\n        join: if True (default), combine each chunk back together into a single text output.\n              When False, chunks are returned as a list of dicts, each having raw associated audio and\n              sample rate in addition to text\n\n    Returns:\n        list of transcribed text\n    \"\"\"\n\n    # Convert single element to list\n    values = [audio] if not isinstance(audio, list) else audio\n\n    # Read input audio\n    speech = self.read(values, rate)\n\n    # Apply transformation rules and store results\n    results = self.batchprocess(speech, chunk) if chunk and not join else self.process(speech, chunk)\n\n    # Return single element if single element passed in\n    return results[0] if not isinstance(audio, list) else results\n</code></pre>"},{"location":"pipeline/data/segmentation/","title":"Segmentation","text":"<p>The Segmentation pipeline segments text into semantic units.</p>"},{"location":"pipeline/data/segmentation/#example","title":"Example","text":"<p>The following shows a simple example using this pipeline.</p> <pre><code>from txtai.pipeline import Segmentation\n\n# Create and run pipeline\nsegment = Segmentation(sentences=True)\nsegment(\"This is a test. And another test.\")\n</code></pre>"},{"location":"pipeline/data/segmentation/#configuration-driven-example","title":"Configuration-driven example","text":"<p>Pipelines are run with Python or configuration. Pipelines can be instantiated in configuration using the lower case name of the pipeline. Configuration-driven pipelines are run with workflows or the API.</p>"},{"location":"pipeline/data/segmentation/#configyml","title":"config.yml","text":"<pre><code># Create pipeline using lower case class name\nsegmentation:\nsentences: true\n\n# Run pipeline with workflow\nworkflow:\nsegment:\ntasks:\n- action: segmentation\n</code></pre>"},{"location":"pipeline/data/segmentation/#run-with-workflows","title":"Run with Workflows","text":"<pre><code>from txtai.app import Application\n\n# Create and run pipeline with workflow\napp = Application(\"config.yml\")\nlist(app.workflow(\"segment\", [\"This is a test. And another test.\"]))\n</code></pre>"},{"location":"pipeline/data/segmentation/#run-with-api","title":"Run with API","text":"<pre><code>CONFIG=config.yml uvicorn \"txtai.api:app\" &amp;\n\ncurl \\\n-X POST \"http://localhost:8000/workflow\" \\\n-H \"Content-Type: application/json\" \\\n-d '{\"name\":\"segment\", \"elements\":[\"This is a test. And another test.\"]}'\n</code></pre>"},{"location":"pipeline/data/segmentation/#methods","title":"Methods","text":"<p>Python documentation for the pipeline.</p>"},{"location":"pipeline/data/segmentation/#txtai.pipeline.data.segmentation.Segmentation.__init__","title":"<code>__init__(self, sentences=False, lines=False, paragraphs=False, minlength=None, join=False)</code>  <code>special</code>","text":"<p>Creates a new Segmentation pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>sentences</code> <p>tokenize text into sentences if True, defaults to False</p> <code>False</code> <code>lines</code> <p>tokenizes text into lines if True, defaults to False</p> <code>False</code> <code>paragraphs</code> <p>tokenizes text into paragraphs if True, defaults to False</p> <code>False</code> <code>minlength</code> <p>require at least minlength characters per text element, defaults to None</p> <code>None</code> <code>join</code> <p>joins tokenized sections back together if True, defaults to False</p> <code>False</code> Source code in <code>txtai/pipeline/data/segmentation.py</code> <pre><code>def __init__(self, sentences=False, lines=False, paragraphs=False, minlength=None, join=False):\n\"\"\"\n    Creates a new Segmentation pipeline.\n\n    Args:\n        sentences: tokenize text into sentences if True, defaults to False\n        lines: tokenizes text into lines if True, defaults to False\n        paragraphs: tokenizes text into paragraphs if True, defaults to False\n        minlength: require at least minlength characters per text element, defaults to None\n        join: joins tokenized sections back together if True, defaults to False\n    \"\"\"\n\n    if not NLTK:\n        raise ImportError('Segmentation pipeline is not available - install \"pipeline\" extra to enable')\n\n    self.sentences = sentences\n    self.lines = lines\n    self.paragraphs = paragraphs\n    self.minlength = minlength\n    self.join = join\n</code></pre>"},{"location":"pipeline/data/segmentation/#txtai.pipeline.data.segmentation.Segmentation.__call__","title":"<code>__call__(self, text)</code>  <code>special</code>","text":"<p>Segments text into semantic units.</p> <p>This method supports text as a string or a list. If the input is a string, the return type is text|list. If text is a list, a list of returned, this could be a list of text or a list of lists depending on the tokenization strategy.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <p>text|list</p> required <p>Returns:</p> Type Description <p>segmented text</p> Source code in <code>txtai/pipeline/data/segmentation.py</code> <pre><code>def __call__(self, text):\n\"\"\"\n    Segments text into semantic units.\n\n    This method supports text as a string or a list. If the input is a string, the return\n    type is text|list. If text is a list, a list of returned, this could be a\n    list of text or a list of lists depending on the tokenization strategy.\n\n    Args:\n        text: text|list\n\n    Returns:\n        segmented text\n    \"\"\"\n\n    # Get inputs\n    texts = [text] if not isinstance(text, list) else text\n\n    # Extract text for each input file\n    results = []\n    for value in texts:\n        # Get text\n        value = self.text(value)\n\n        # Parse and add extracted results\n        results.append(self.parse(value))\n\n    return results[0] if isinstance(text, str) else results\n</code></pre>"},{"location":"pipeline/data/tabular/","title":"Tabular","text":"<p>The Tabular pipeline splits tabular data into rows and columns. The tabular pipeline is most useful in creating (id, text, tag) tuples to load into Embedding indexes. </p>"},{"location":"pipeline/data/tabular/#example","title":"Example","text":"<p>The following shows a simple example using this pipeline.</p> <pre><code>from txtai.pipeline import Tabular\n\n# Create and run pipeline\ntabular = Tabular(\"id\", [\"text\"])\ntabular(\"path to csv file\")\n</code></pre> <p>See the link below for a more detailed example.</p> Notebook Description Transform tabular data with composable workflows Transform, index and search tabular data"},{"location":"pipeline/data/tabular/#configuration-driven-example","title":"Configuration-driven example","text":"<p>Pipelines are run with Python or configuration. Pipelines can be instantiated in configuration using the lower case name of the pipeline. Configuration-driven pipelines are run with workflows or the API.</p>"},{"location":"pipeline/data/tabular/#configyml","title":"config.yml","text":"<pre><code># Create pipeline using lower case class name\ntabular:\nidcolumn: id\ntextcolumns:\n- text\n\n# Run pipeline with workflow\nworkflow:\ntabular:\ntasks:\n- action: tabular\n</code></pre>"},{"location":"pipeline/data/tabular/#run-with-workflows","title":"Run with Workflows","text":"<pre><code>from txtai.app import Application\n\n# Create and run pipeline with workflow\napp = Application(\"config.yml\")\nlist(app.workflow(\"tabular\", [\"path to csv file\"]))\n</code></pre>"},{"location":"pipeline/data/tabular/#run-with-api","title":"Run with API","text":"<pre><code>CONFIG=config.yml uvicorn \"txtai.api:app\" &amp;\n\ncurl \\\n-X POST \"http://localhost:8000/workflow\" \\\n-H \"Content-Type: application/json\" \\\n-d '{\"name\":\"tabular\", \"elements\":[\"path to csv file\"]}'\n</code></pre>"},{"location":"pipeline/data/tabular/#methods","title":"Methods","text":"<p>Python documentation for the pipeline.</p>"},{"location":"pipeline/data/tabular/#txtai.pipeline.data.tabular.Tabular.__init__","title":"<code>__init__(self, idcolumn=None, textcolumns=None, content=False)</code>  <code>special</code>","text":"<p>Creates a new Tabular pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>idcolumn</code> <p>column name to use for row id</p> <code>None</code> <code>textcolumns</code> <p>list of columns to combine as a text field</p> <code>None</code> <code>content</code> <p>if True, a dict per row is generated with all fields. If content is a list, a subset of fields      is included in the generated rows.</p> <code>False</code> Source code in <code>txtai/pipeline/data/tabular.py</code> <pre><code>def __init__(self, idcolumn=None, textcolumns=None, content=False):\n\"\"\"\n    Creates a new Tabular pipeline.\n\n    Args:\n        idcolumn: column name to use for row id\n        textcolumns: list of columns to combine as a text field\n        content: if True, a dict per row is generated with all fields. If content is a list, a subset of fields\n                 is included in the generated rows.\n    \"\"\"\n\n    if not PANDAS:\n        raise ImportError('Tabular pipeline is not available - install \"pipeline\" extra to enable')\n\n    self.idcolumn = idcolumn\n    self.textcolumns = textcolumns\n    self.content = content\n</code></pre>"},{"location":"pipeline/data/tabular/#txtai.pipeline.data.tabular.Tabular.__call__","title":"<code>__call__(self, data)</code>  <code>special</code>","text":"<p>Splits data into rows and columns.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <p>input data</p> required <p>Returns:</p> Type Description <p>list of (id, text, tag)</p> Source code in <code>txtai/pipeline/data/tabular.py</code> <pre><code>def __call__(self, data):\n\"\"\"\n    Splits data into rows and columns.\n\n    Args:\n        data: input data\n\n    Returns:\n        list of (id, text, tag)\n    \"\"\"\n\n    items = [data] if not isinstance(data, list) else data\n\n    # Combine all rows into single return element\n    results = []\n    dicts = []\n\n    for item in items:\n        # File path\n        if isinstance(item, str):\n            _, extension = os.path.splitext(item)\n            extension = extension.replace(\".\", \"\").lower()\n\n            if extension == \"csv\":\n                df = pd.read_csv(item)\n\n            results.append(self.process(df))\n\n        # Dict\n        if isinstance(item, dict):\n            dicts.append(item)\n\n        # List of dicts\n        elif isinstance(item, list):\n            df = pd.DataFrame(item)\n            results.append(self.process(df))\n\n    if dicts:\n        df = pd.DataFrame(dicts)\n        results.extend(self.process(df))\n\n    return results[0] if not isinstance(data, list) else results\n</code></pre>"},{"location":"pipeline/data/textractor/","title":"Textractor","text":"<p>The Textractor pipeline extracts and splits text from documents. This pipeline uses either an Apache Tika backend (if Java is available) or BeautifulSoup4.</p>"},{"location":"pipeline/data/textractor/#example","title":"Example","text":"<p>The following shows a simple example using this pipeline.</p> <pre><code>from txtai.pipeline import Textractor\n\n# Create and run pipeline\ntextract = Textractor()\ntextract(\"https://github.com/neuml/txtai\")\n</code></pre> <p>See the link below for a more detailed example.</p> Notebook Description Extract text from documents Extract text from PDF, Office, HTML and more"},{"location":"pipeline/data/textractor/#configuration-driven-example","title":"Configuration-driven example","text":"<p>Pipelines are run with Python or configuration. Pipelines can be instantiated in configuration using the lower case name of the pipeline. Configuration-driven pipelines are run with workflows or the API.</p>"},{"location":"pipeline/data/textractor/#configyml","title":"config.yml","text":"<pre><code># Create pipeline using lower case class name\ntextractor:\n\n# Run pipeline with workflow\nworkflow:\ntextract:\ntasks:\n- action: textractor\n</code></pre>"},{"location":"pipeline/data/textractor/#run-with-workflows","title":"Run with Workflows","text":"<pre><code>from txtai.app import Application\n\n# Create and run pipeline with workflow\napp = Application(\"config.yml\")\nlist(app.workflow(\"textract\", [\"https://github.com/neuml/txtai\"]))\n</code></pre>"},{"location":"pipeline/data/textractor/#run-with-api","title":"Run with API","text":"<pre><code>CONFIG=config.yml uvicorn \"txtai.api:app\" &amp;\n\ncurl \\\n-X POST \"http://localhost:8000/workflow\" \\\n-H \"Content-Type: application/json\" \\\n-d '{\"name\":\"textract\", \"elements\":[\"https://github.com/neuml/txtai\"]}'\n</code></pre>"},{"location":"pipeline/data/textractor/#methods","title":"Methods","text":"<p>Python documentation for the pipeline.</p>"},{"location":"pipeline/data/textractor/#txtai.pipeline.data.textractor.Textractor.__init__","title":"<code>__init__(self, sentences=False, lines=False, paragraphs=False, minlength=None, join=False, tika=True)</code>  <code>special</code>","text":"Source code in <code>txtai/pipeline/data/textractor.py</code> <pre><code>def __init__(self, sentences=False, lines=False, paragraphs=False, minlength=None, join=False, tika=True):\n    if not TIKA:\n        raise ImportError('Textractor pipeline is not available - install \"pipeline\" extra to enable')\n\n    super().__init__(sentences, lines, paragraphs, minlength, join)\n\n    # Determine if Tika (default if Java is available) or Beautiful Soup should be used\n    # Beautiful Soup only supports HTML, Tika supports a wide variety of file formats, including HTML.\n    self.tika = self.checkjava() if tika else False\n</code></pre>"},{"location":"pipeline/data/textractor/#txtai.pipeline.data.segmentation.Textractor.__call__","title":"<code>__call__(self, text)</code>  <code>special</code>","text":"Source code in <code>txtai/pipeline/data/segmentation.py</code> <pre><code>def __call__(self, text):\n\"\"\"\n    Segments text into semantic units.\n\n    This method supports text as a string or a list. If the input is a string, the return\n    type is text|list. If text is a list, a list of returned, this could be a\n    list of text or a list of lists depending on the tokenization strategy.\n\n    Args:\n        text: text|list\n\n    Returns:\n        segmented text\n    \"\"\"\n\n    # Get inputs\n    texts = [text] if not isinstance(text, list) else text\n\n    # Extract text for each input file\n    results = []\n    for value in texts:\n        # Get text\n        value = self.text(value)\n\n        # Parse and add extracted results\n        results.append(self.parse(value))\n\n    return results[0] if isinstance(text, str) else results\n</code></pre>"},{"location":"pipeline/image/caption/","title":"Caption","text":"<p>The caption pipeline reads a list of images and returns a list of captions for those images.</p>"},{"location":"pipeline/image/caption/#example","title":"Example","text":"<p>The following shows a simple example using this pipeline.</p> <pre><code>from txtai.pipeline import Caption\n\n# Create and run pipeline\ncaption = Caption()\ncaption(\"path to image file\")\n</code></pre> <p>See the link below for a more detailed example.</p> Notebook Description Generate image captions and detect objects Captions and object detection for images"},{"location":"pipeline/image/caption/#configuration-driven-example","title":"Configuration-driven example","text":"<p>Pipelines are run with Python or configuration. Pipelines can be instantiated in configuration using the lower case name of the pipeline. Configuration-driven pipelines are run with workflows or the API.</p>"},{"location":"pipeline/image/caption/#configyml","title":"config.yml","text":"<pre><code># Create pipeline using lower case class name\ncaption:\n\n# Run pipeline with workflow\nworkflow:\ncaption:\ntasks:\n- action: caption\n</code></pre>"},{"location":"pipeline/image/caption/#run-with-workflows","title":"Run with Workflows","text":"<pre><code>from txtai.app import Application\n\n# Create and run pipeline with workflow\napp = Application(\"config.yml\")\nlist(app.workflow(\"caption\", [\"path to image file\"]))\n</code></pre>"},{"location":"pipeline/image/caption/#run-with-api","title":"Run with API","text":"<pre><code>CONFIG=config.yml uvicorn \"txtai.api:app\" &amp;\n\ncurl \\\n-X POST \"http://localhost:8000/workflow\" \\\n-H \"Content-Type: application/json\" \\\n-d '{\"name\":\"caption\", \"elements\":[\"path to image file\"]}'\n</code></pre>"},{"location":"pipeline/image/caption/#methods","title":"Methods","text":"<p>Python documentation for the pipeline.</p>"},{"location":"pipeline/image/caption/#txtai.pipeline.image.caption.Caption.__init__","title":"<code>__init__(self, path=None, quantize=False, gpu=True, model=None)</code>  <code>special</code>","text":"Source code in <code>txtai/pipeline/image/caption.py</code> <pre><code>def __init__(self, path=None, quantize=False, gpu=True, model=None):\n    if not PIL:\n        raise ImportError('Captions pipeline is not available - install \"pipeline\" extra to enable')\n\n    # Call parent constructor\n    super().__init__(\"image-to-text\", path, quantize, gpu, model)\n</code></pre>"},{"location":"pipeline/image/caption/#txtai.pipeline.image.caption.Caption.__call__","title":"<code>__call__(self, images)</code>  <code>special</code>","text":"<p>Builds captions for images.</p> <p>This method supports a single image or a list of images. If the input is an image, the return type is a string. If text is a list, a list of strings is returned</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <p>image|list</p> required <p>Returns:</p> Type Description <p>list of captions</p> Source code in <code>txtai/pipeline/image/caption.py</code> <pre><code>def __call__(self, images):\n\"\"\"\n    Builds captions for images.\n\n    This method supports a single image or a list of images. If the input is an image, the return\n    type is a string. If text is a list, a list of strings is returned\n\n    Args:\n        images: image|list\n\n    Returns:\n        list of captions\n    \"\"\"\n\n    # Convert single element to list\n    values = [images] if not isinstance(images, list) else images\n\n    # Open images if file strings\n    values = [Image.open(image) if isinstance(image, str) else image for image in values]\n\n    # Get and clean captions\n    captions = []\n    for result in self.pipeline(values):\n        text = \" \".join([r[\"generated_text\"] for r in result]).strip()\n        captions.append(text)\n\n    # Return single element if single element passed in\n    return captions[0] if not isinstance(images, list) else captions\n</code></pre>"},{"location":"pipeline/image/imagehash/","title":"ImageHash","text":"<p>The image hash pipeline generates perceptual image hashes. These hashes can be used to detect near-duplicate images. This method is not backed by machine learning models and not intended to find conceptually similar images.</p>"},{"location":"pipeline/image/imagehash/#example","title":"Example","text":"<p>The following shows a simple example using this pipeline.</p> <pre><code>from txtai.pipeline import ImageHash\n\n# Create and run pipeline\nihash = ImageHash()\nihash(\"path to image file\")\n</code></pre> <p>See the link below for a more detailed example.</p> Notebook Description Near duplicate image detection Identify duplicate and near-duplicate images"},{"location":"pipeline/image/imagehash/#configuration-driven-example","title":"Configuration-driven example","text":"<p>Pipelines are run with Python or configuration. Pipelines can be instantiated in configuration using the lower case name of the pipeline. Configuration-driven pipelines are run with workflows or the API.</p>"},{"location":"pipeline/image/imagehash/#configyml","title":"config.yml","text":"<pre><code># Create pipeline using lower case class name\nimagehash:\n\n# Run pipeline with workflow\nworkflow:\nimagehash:\ntasks:\n- action: imagehash\n</code></pre>"},{"location":"pipeline/image/imagehash/#run-with-workflows","title":"Run with Workflows","text":"<pre><code>from txtai.app import Application\n\n# Create and run pipeline with workflow\napp = Application(\"config.yml\")\nlist(app.workflow(\"imagehash\", [\"path to image file\"]))\n</code></pre>"},{"location":"pipeline/image/imagehash/#run-with-api","title":"Run with API","text":"<pre><code>CONFIG=config.yml uvicorn \"txtai.api:app\" &amp;\n\ncurl \\\n-X POST \"http://localhost:8000/workflow\" \\\n-H \"Content-Type: application/json\" \\\n-d '{\"name\":\"imagehash\", \"elements\":[\"path to image file\"]}'\n</code></pre>"},{"location":"pipeline/image/imagehash/#methods","title":"Methods","text":"<p>Python documentation for the pipeline.</p>"},{"location":"pipeline/image/imagehash/#txtai.pipeline.image.imagehash.ImageHash.__init__","title":"<code>__init__(self, algorithm='average', size=8, strings=True)</code>  <code>special</code>","text":"<p>Creates an ImageHash pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>algorithm</code> <p>image hashing algorithm (average, perceptual, difference, wavelet, color)</p> <code>'average'</code> <code>size</code> <p>hash size</p> <code>8</code> <code>strings</code> <p>outputs hex strings if True (default), otherwise the pipeline returns numpy arrays</p> <code>True</code> Source code in <code>txtai/pipeline/image/imagehash.py</code> <pre><code>def __init__(self, algorithm=\"average\", size=8, strings=True):\n\"\"\"\n    Creates an ImageHash pipeline.\n\n    Args:\n        algorithm: image hashing algorithm (average, perceptual, difference, wavelet, color)\n        size: hash size\n        strings: outputs hex strings if True (default), otherwise the pipeline returns numpy arrays\n    \"\"\"\n\n    if not PIL:\n        raise ImportError('ImageHash pipeline is not available - install \"pipeline\" extra to enable')\n\n    self.algorithm = algorithm\n    self.size = size\n    self.strings = strings\n</code></pre>"},{"location":"pipeline/image/imagehash/#txtai.pipeline.image.imagehash.ImageHash.__call__","title":"<code>__call__(self, images)</code>  <code>special</code>","text":"<p>Generates perceptual image hashes.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <p>image|list</p> required <p>Returns:</p> Type Description <p>list of hashes</p> Source code in <code>txtai/pipeline/image/imagehash.py</code> <pre><code>def __call__(self, images):\n\"\"\"\n    Generates perceptual image hashes.\n\n    Args:\n        images: image|list\n\n    Returns:\n        list of hashes\n    \"\"\"\n\n    # Convert single element to list\n    values = [images] if not isinstance(images, list) else images\n\n    # Open images if file strings\n    values = [Image.open(image) if isinstance(image, str) else image for image in values]\n\n    # Convert images to hashes\n    hashes = [self.ihash(image) for image in values]\n\n    # Return single element if single element passed in\n    return hashes[0] if not isinstance(images, list) else hashes\n</code></pre>"},{"location":"pipeline/image/objects/","title":"Objects","text":"<p>The Objects pipeline reads a list of images and returns a list of detected objects.</p>"},{"location":"pipeline/image/objects/#example","title":"Example","text":"<p>The following shows a simple example using this pipeline.</p> <pre><code>from txtai.pipeline import Objects\n\n# Create and run pipeline\nobjects = Objects()\nobjects(\"path to image file\")\n</code></pre> <p>See the link below for a more detailed example.</p> Notebook Description Generate image captions and detect objects Captions and object detection for images"},{"location":"pipeline/image/objects/#configuration-driven-example","title":"Configuration-driven example","text":"<p>Pipelines are run with Python or configuration. Pipelines can be instantiated in configuration using the lower case name of the pipeline. Configuration-driven pipelines are run with workflows or the API.</p>"},{"location":"pipeline/image/objects/#configyml","title":"config.yml","text":"<pre><code># Create pipeline using lower case class name\nobjects:\n\n# Run pipeline with workflow\nworkflow:\nobjects:\ntasks:\n- action: objects\n</code></pre>"},{"location":"pipeline/image/objects/#run-with-workflows","title":"Run with Workflows","text":"<pre><code>from txtai.app import Application\n\n# Create and run pipeline with workflow\napp = Application(\"config.yml\")\nlist(app.workflow(\"objects\", [\"path to image file\"]))\n</code></pre>"},{"location":"pipeline/image/objects/#run-with-api","title":"Run with API","text":"<pre><code>CONFIG=config.yml uvicorn \"txtai.api:app\" &amp;\n\ncurl \\\n-X POST \"http://localhost:8000/workflow\" \\\n-H \"Content-Type: application/json\" \\\n-d '{\"name\":\"objects\", \"elements\":[\"path to image file\"]}'\n</code></pre>"},{"location":"pipeline/image/objects/#methods","title":"Methods","text":"<p>Python documentation for the pipeline.</p>"},{"location":"pipeline/image/objects/#txtai.pipeline.image.objects.Objects.__init__","title":"<code>__init__(self, path=None, quantize=False, gpu=True, model=None, classification=False, threshold=0.9)</code>  <code>special</code>","text":"Source code in <code>txtai/pipeline/image/objects.py</code> <pre><code>def __init__(self, path=None, quantize=False, gpu=True, model=None, classification=False, threshold=0.9):\n    if not PIL:\n        raise ImportError('Objects pipeline is not available - install \"pipeline\" extra to enable')\n\n    super().__init__(\"image-classification\" if classification else \"object-detection\", path, quantize, gpu, model)\n\n    self.classification = classification\n    self.threshold = threshold\n</code></pre>"},{"location":"pipeline/image/objects/#txtai.pipeline.image.objects.Objects.__call__","title":"<code>__call__(self, images, flatten=False, workers=0)</code>  <code>special</code>","text":"<p>Applies object detection/image classification models to images. Returns a list of (label, score).</p> <p>This method supports a single image or a list of images. If the input is an image, the return type is a 1D list of (label, score). If text is a list, a 2D list of (label, score) is returned with a row per image.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <p>image|list</p> required <code>flatten</code> <p>flatten output to a list of objects</p> <code>False</code> <code>workers</code> <p>number of concurrent workers to use for processing data, defaults to None</p> <code>0</code> <p>Returns:</p> Type Description <p>list of (label, score)</p> Source code in <code>txtai/pipeline/image/objects.py</code> <pre><code>def __call__(self, images, flatten=False, workers=0):\n\"\"\"\n    Applies object detection/image classification models to images. Returns a list of (label, score).\n\n    This method supports a single image or a list of images. If the input is an image, the return\n    type is a 1D list of (label, score). If text is a list, a 2D list of (label, score) is\n    returned with a row per image.\n\n    Args:\n        images: image|list\n        flatten: flatten output to a list of objects\n        workers: number of concurrent workers to use for processing data, defaults to None\n\n    Returns:\n        list of (label, score)\n    \"\"\"\n\n    # Convert single element to list\n    values = [images] if not isinstance(images, list) else images\n\n    # Open images if file strings\n    values = [Image.open(image) if isinstance(image, str) else image for image in values]\n\n    # Run pipeline\n    results = (\n        self.pipeline(values, num_workers=workers)\n        if self.classification\n        else self.pipeline(values, threshold=self.threshold, num_workers=workers)\n    )\n\n    # Build list of (id, score)\n    outputs = []\n    for result in results:\n        # Convert to (label, score) tuples\n        result = [(x[\"label\"], x[\"score\"]) for x in result if x[\"score\"] &gt; self.threshold]\n\n        # Sort by score descending\n        result = sorted(result, key=lambda x: x[1], reverse=True)\n\n        # Deduplicate labels\n        unique = set()\n        elements = []\n        for label, score in result:\n            if label not in unique:\n                elements.append(label if flatten else (label, score))\n                unique.add(label)\n\n        outputs.append(elements)\n\n    # Return single element if single element passed in\n    return outputs[0] if not isinstance(images, list) else outputs\n</code></pre>"},{"location":"pipeline/text/entity/","title":"Entity","text":"<p>The Entity pipeline applies a token classifier to text and extracts entity/label combinations.</p>"},{"location":"pipeline/text/entity/#example","title":"Example","text":"<p>The following shows a simple example using this pipeline.</p> <pre><code>from txtai.pipeline import Entity\n\n# Create and run pipeline\nentity = Entity()\nentity(\"Canada's last fully intact ice shelf has suddenly collapsed, \" \\\n       \"forming a Manhattan-sized iceberg\")\n</code></pre> <p>See the link below for a more detailed example.</p> Notebook Description Entity extraction workflows Identify entity/label combinations"},{"location":"pipeline/text/entity/#configuration-driven-example","title":"Configuration-driven example","text":"<p>Pipelines are run with Python or configuration. Pipelines can be instantiated in configuration using the lower case name of the pipeline. Configuration-driven pipelines are run with workflows or the API.</p>"},{"location":"pipeline/text/entity/#configyml","title":"config.yml","text":"<pre><code># Create pipeline using lower case class name\nentity:\n\n# Run pipeline with workflow\nworkflow:\nentity:\ntasks:\n- action: entity\n</code></pre>"},{"location":"pipeline/text/entity/#run-with-workflows","title":"Run with Workflows","text":"<pre><code>from txtai.app import Application\n\n# Create and run pipeline with workflow\napp = Application(\"config.yml\")\nlist(app.workflow(\"entity\", [\"Canada's last fully intact ice shelf has suddenly collapsed, forming a Manhattan-sized iceberg\"]))\n</code></pre>"},{"location":"pipeline/text/entity/#run-with-api","title":"Run with API","text":"<pre><code>CONFIG=config.yml uvicorn \"txtai.api:app\" &amp;\n\ncurl \\\n-X POST \"http://localhost:8000/workflow\" \\\n-H \"Content-Type: application/json\" \\\n-d '{\"name\":\"entity\", \"elements\": [\"Canadas last fully intact ice shelf has suddenly collapsed, forming a Manhattan-sized iceberg\"]}'\n</code></pre>"},{"location":"pipeline/text/entity/#methods","title":"Methods","text":"<p>Python documentation for the pipeline.</p>"},{"location":"pipeline/text/entity/#txtai.pipeline.text.entity.Entity.__init__","title":"<code>__init__(self, path=None, quantize=False, gpu=True, model=None)</code>  <code>special</code>","text":"Source code in <code>txtai/pipeline/text/entity.py</code> <pre><code>def __init__(self, path=None, quantize=False, gpu=True, model=None):\n    super().__init__(\"token-classification\", path, quantize, gpu, model)\n</code></pre>"},{"location":"pipeline/text/entity/#txtai.pipeline.text.entity.Entity.__call__","title":"<code>__call__(self, text, labels=None, aggregate='simple', flatten=None, join=False, workers=0)</code>  <code>special</code>","text":"<p>Applies a token classifier to text and extracts entity/label combinations.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <p>text|list</p> required <code>labels</code> <p>list of entity type labels to accept, defaults to None which accepts all</p> <code>None</code> <code>aggregate</code> <p>method to combine multi token entities - options are \"simple\" (default), \"first\", \"average\" or \"max\"</p> <code>'simple'</code> <code>flatten</code> <p>flatten output to a list of labels if present. Accepts a boolean or float value to only keep scores greater than that number.</p> <code>None</code> <code>join</code> <p>joins flattened output into a string if True, ignored if flatten not set</p> <code>False</code> <code>workers</code> <p>number of concurrent workers to use for processing data, defaults to None</p> <code>0</code> <p>Returns:</p> Type Description <p>list of (entity, entity type, score) or list of entities depending on flatten parameter</p> Source code in <code>txtai/pipeline/text/entity.py</code> <pre><code>def __call__(self, text, labels=None, aggregate=\"simple\", flatten=None, join=False, workers=0):\n\"\"\"\n    Applies a token classifier to text and extracts entity/label combinations.\n\n    Args:\n        text: text|list\n        labels: list of entity type labels to accept, defaults to None which accepts all\n        aggregate: method to combine multi token entities - options are \"simple\" (default), \"first\", \"average\" or \"max\"\n        flatten: flatten output to a list of labels if present. Accepts a boolean or float value to only keep scores greater than that number.\n        join: joins flattened output into a string if True, ignored if flatten not set\n        workers: number of concurrent workers to use for processing data, defaults to None\n\n    Returns:\n        list of (entity, entity type, score) or list of entities depending on flatten parameter\n    \"\"\"\n\n    # Run token classification pipeline\n    results = self.pipeline(text, aggregation_strategy=aggregate, num_workers=workers)\n\n    # Convert results to a list if necessary\n    if isinstance(text, str):\n        results = [results]\n\n    # Score threshold when flatten is set\n    threshold = 0.0 if isinstance(flatten, bool) else flatten\n\n    # Extract entities if flatten set, otherwise extract (entity, entity type, score) tuples\n    outputs = []\n    for result in results:\n        if flatten:\n            output = [r[\"word\"] for r in result if self.accept(r[\"entity_group\"], labels) and r[\"score\"] &gt;= threshold]\n            outputs.append(\" \".join(output) if join else output)\n        else:\n            outputs.append([(r[\"word\"], r[\"entity_group\"], float(r[\"score\"])) for r in result if self.accept(r[\"entity_group\"], labels)])\n\n    return outputs[0] if isinstance(text, str) else outputs\n</code></pre>"},{"location":"pipeline/text/extractor/","title":"Extractor","text":"<p>The Extractor pipeline is a combination of an embeddings query and an Extractive QA model. Filtering the context for a QA model helps maximize performance of the model.</p>"},{"location":"pipeline/text/extractor/#example","title":"Example","text":"<p>The following shows a simple example using this pipeline.</p> <pre><code>from txtai.embeddings import Embeddings\nfrom txtai.pipeline import Extractor\n\n# Embeddings model ranks candidates before passing to QA pipeline\nembeddings = Embeddings({\"path\": \"sentence-transformers/nli-mpnet-base-v2\"})\n\n# Create and run pipeline\nextractor = Extractor(embeddings, \"distilbert-base-cased-distilled-squad\")\nextractor([[\"What was won\"] * 3 + [False]],\n          [\"Maine man wins $1M from $25 lottery ticket\"])\n</code></pre> <p>See the links below for more detailed examples.</p> Notebook Description Extractive QA with txtai Introduction to extractive question-answering with txtai Extractive QA with Elasticsearch Run extractive question-answering queries with Elasticsearch Extractive QA to build structured data Build structured datasets using extractive question-answering"},{"location":"pipeline/text/extractor/#configuration-driven-example","title":"Configuration-driven example","text":"<p>Pipelines are run with Python or configuration. Pipelines can be instantiated in configuration using the lower case name of the pipeline. Configuration-driven pipelines are run with workflows or the API.</p>"},{"location":"pipeline/text/extractor/#configyml","title":"config.yml","text":"<pre><code># Create pipeline using lower case class name\nextractor:\n</code></pre>"},{"location":"pipeline/text/extractor/#run-with-workflows","title":"Run with Workflows","text":"<pre><code>from txtai.app import Application\n\n# Create and run pipeline with workflow\napp = Application(\"config.yml\")\nlist(app.extract([{\"name\": \"What was won\", \"query\": \"What was won\",\n                   \"question\", \"What was won\", \"snippet\": False}], \n                 [\"Maine man wins $1M from $25 lottery ticket\"]))\n</code></pre>"},{"location":"pipeline/text/extractor/#run-with-api","title":"Run with API","text":"<pre><code>CONFIG=config.yml uvicorn \"txtai.api:app\" &amp;\n\ncurl \\\n-X POST \"http://localhost:8000/extract\" \\\n-H \"Content-Type: application/json\" \\\n-d '{\"queue\": [{\"name\":\"What was won\", \"query\": \"What was won\", \"question\": \"What was won\", \"snippet\": false}], \"texts\": [\"Maine man wins $1M from $25 lottery ticket\"]}'\n</code></pre>"},{"location":"pipeline/text/extractor/#methods","title":"Methods","text":"<p>Python documentation for the pipeline.</p>"},{"location":"pipeline/text/extractor/#txtai.pipeline.text.extractor.Extractor.__init__","title":"<code>__init__(self, similarity, path, quantize=False, gpu=True, model=None, tokenizer=None, minscore=None, mintokens=None, context=None)</code>  <code>special</code>","text":"<p>Builds a new extractor.</p> <p>Parameters:</p> Name Type Description Default <code>similarity</code> <p>similarity instance (embeddings or similarity instance)</p> required <code>path</code> <p>path to qa model</p> required <code>quantize</code> <p>True if model should be quantized before inference, False otherwise.</p> <code>False</code> <code>gpu</code> <p>if gpu inference should be used (only works if GPUs are available)</p> <code>True</code> <code>model</code> <p>optional existing pipeline model to wrap</p> <code>None</code> <code>tokenizer</code> <p>Tokenizer class</p> <code>None</code> <code>minscore</code> <p>minimum score to include context match, defaults to None</p> <code>None</code> <code>mintokens</code> <p>minimum number of tokens to include context match, defaults to None</p> <code>None</code> <code>context</code> <p>topn context matches to include, defaults to 3</p> <code>None</code> Source code in <code>txtai/pipeline/text/extractor.py</code> <pre><code>def __init__(self, similarity, path, quantize=False, gpu=True, model=None, tokenizer=None, minscore=None, mintokens=None, context=None):\n\"\"\"\n    Builds a new extractor.\n\n    Args:\n        similarity: similarity instance (embeddings or similarity instance)\n        path: path to qa model\n        quantize: True if model should be quantized before inference, False otherwise.\n        gpu: if gpu inference should be used (only works if GPUs are available)\n        model: optional existing pipeline model to wrap\n        tokenizer: Tokenizer class\n        minscore: minimum score to include context match, defaults to None\n        mintokens: minimum number of tokens to include context match, defaults to None\n        context: topn context matches to include, defaults to 3\n    \"\"\"\n\n    # Similarity instance\n    self.similarity = similarity\n\n    # QA Pipeline\n    self.pipeline = Questions(path, quantize, gpu, model)\n\n    # Tokenizer class use default method if not set\n    self.tokenizer = tokenizer if tokenizer else Tokenizer\n\n    # Minimum score to include context match\n    self.minscore = minscore if minscore is not None else 0.0\n\n    # Minimum number of tokens to include context match\n    self.mintokens = mintokens if mintokens is not None else 0.0\n\n    # Top N context matches to include for question-answering\n    self.context = context if context else 3\n</code></pre>"},{"location":"pipeline/text/extractor/#txtai.pipeline.text.extractor.Extractor.__call__","title":"<code>__call__(self, queue, texts)</code>  <code>special</code>","text":"<p>Extracts answers to input questions. This method runs queries against a list of text, finds the top n best matches and uses that as the question context. A question-answering model is then run against the context for the input question, with the answer returned.</p> <p>Parameters:</p> Name Type Description Default <code>queue</code> <p>input queue (name, query, question, snippet)</p> required <code>texts</code> <p>list of text</p> required <p>Returns:</p> Type Description <p>list of (name, answer)</p> Source code in <code>txtai/pipeline/text/extractor.py</code> <pre><code>def __call__(self, queue, texts):\n\"\"\"\n    Extracts answers to input questions. This method runs queries against a list of text, finds the top n best matches\n    and uses that as the question context. A question-answering model is then run against the context for the input question,\n    with the answer returned.\n\n    Args:\n        queue: input queue (name, query, question, snippet)\n        texts: list of text\n\n    Returns:\n        list of (name, answer)\n    \"\"\"\n\n    # Execute embeddings query\n    results = self.query([query for _, query, _, _ in queue], texts)\n\n    # Build question-context pairs\n    names, questions, contexts, topns, snippets = [], [], [], [], []\n    for x, (name, _, question, snippet) in enumerate(queue):\n        # Build context using top n best matching segments\n        topn = sorted(results[x], key=lambda y: y[2], reverse=True)[: self.context]\n        context = \" \".join([text for _, text, _ in sorted(topn, key=lambda y: y[0])])\n\n        names.append(name)\n        questions.append(question)\n        contexts.append(context)\n        topns.append([text for _, text, _ in topn])\n        snippets.append(snippet)\n\n    # Run qa pipeline and return answers\n    return self.answers(names, questions, contexts, topns, snippets)\n</code></pre>"},{"location":"pipeline/text/generator/","title":"Generator","text":"<p>The Generator pipeline takes an input prompt and generates follow-on text.</p>"},{"location":"pipeline/text/generator/#example","title":"Example","text":"<p>The following shows a simple example using this pipeline.</p> <pre><code>from txtai.pipeline import Generator\n\n# Create and run pipeline\ngenerator = Generator()\ngenerator(\"Hello, how are you?\")\n</code></pre>"},{"location":"pipeline/text/generator/#configuration-driven-example","title":"Configuration-driven example","text":"<p>Pipelines are run with Python or configuration. Pipelines can be instantiated in configuration using the lower case name of the pipeline. Configuration-driven pipelines are run with workflows or the API.</p>"},{"location":"pipeline/text/generator/#configyml","title":"config.yml","text":"<pre><code># Create pipeline using lower case class name\ngenerator:\n\n# Run pipeline with workflow\nworkflow:\ngenerator:\ntasks:\n- action: generator\n</code></pre>"},{"location":"pipeline/text/generator/#run-with-workflows","title":"Run with Workflows","text":"<pre><code>from txtai.app import Application\n\n# Create and run pipeline with workflow\napp = Application(\"config.yml\")\nlist(app.workflow(\"generator\", [\"Hello, how are you?\"]))\n</code></pre>"},{"location":"pipeline/text/generator/#run-with-api","title":"Run with API","text":"<pre><code>CONFIG=config.yml uvicorn \"txtai.api:app\" &amp;\n\ncurl \\\n-X POST \"http://localhost:8000/workflow\" \\\n-H \"Content-Type: application/json\" \\\n-d '{\"name\":\"generator\", \"elements\": [\"Hello, how are you?\"]}'\n</code></pre>"},{"location":"pipeline/text/generator/#methods","title":"Methods","text":"<p>Python documentation for the pipeline.</p>"},{"location":"pipeline/text/generator/#txtai.pipeline.text.generator.Generator.__init__","title":"<code>__init__(self, path=None, quantize=False, gpu=True, model=None)</code>  <code>special</code>","text":"Source code in <code>txtai/pipeline/text/generator.py</code> <pre><code>def __init__(self, path=None, quantize=False, gpu=True, model=None):\n    super().__init__(self.task(), path, quantize, gpu, model)\n</code></pre>"},{"location":"pipeline/text/generator/#txtai.pipeline.text.generator.Generator.__call__","title":"<code>__call__(self, text, prefix=None, maxlength=512, workers=0)</code>  <code>special</code>","text":"<p>Generates text using input text</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <p>text|list</p> required <code>prefix</code> <p>optional prefix to prepend to text elements</p> <code>None</code> <code>maxlength</code> <p>maximum sequence length</p> <code>512</code> <code>workers</code> <p>number of concurrent workers to use for processing data, defaults to None</p> <code>0</code> <p>Returns:</p> Type Description <p>generated text</p> Source code in <code>txtai/pipeline/text/generator.py</code> <pre><code>def __call__(self, text, prefix=None, maxlength=512, workers=0):\n\"\"\"\n    Generates text using input text\n\n    Args:\n        text: text|list\n        prefix: optional prefix to prepend to text elements\n        maxlength: maximum sequence length\n        workers: number of concurrent workers to use for processing data, defaults to None\n\n    Returns:\n        generated text\n    \"\"\"\n\n    # List of texts\n    texts = text if isinstance(text, list) else [text]\n\n    # Add prefix, if necessary\n    if prefix:\n        texts = [f\"{prefix}{x}\" for x in texts]\n\n    # Run pipeline\n    results = self.pipeline(texts, max_length=maxlength, num_workers=workers)\n\n    # Get generated text\n    results = [self.clean(x) for x in results]\n\n    return results[0] if isinstance(text, str) else results\n</code></pre>"},{"location":"pipeline/text/labels/","title":"Labels","text":"<p>The Labels pipeline uses a text classification model to apply labels to input text. This pipeline can classify text using either a zero shot model (dynamic labeling) or a standard text classification model (fixed labeling).</p>"},{"location":"pipeline/text/labels/#example","title":"Example","text":"<p>The following shows a simple example using this pipeline.</p> <pre><code>from txtai.pipeline import Labels\n\n# Create and run pipeline\nlabels = Labels()\nlabels(\n    [\"Great news\", \"That's rough\"],\n    [\"positive\", \"negative\"]\n)\n</code></pre> <p>See the link below for a more detailed example.</p> Notebook Description Apply labels with zero shot classification Use zero shot learning for labeling, classification and topic modeling"},{"location":"pipeline/text/labels/#configuration-driven-example","title":"Configuration-driven example","text":"<p>Pipelines are run with Python or configuration. Pipelines can be instantiated in configuration using the lower case name of the pipeline. Configuration-driven pipelines are run with workflows or the API.</p>"},{"location":"pipeline/text/labels/#configyml","title":"config.yml","text":"<pre><code># Create pipeline using lower case class name\nlabels:\n\n# Run pipeline with workflow\nworkflow:\nlabels:\ntasks:\n- action: labels\nargs: [[\"positive\", \"negative\"]]\n</code></pre>"},{"location":"pipeline/text/labels/#run-with-workflows","title":"Run with Workflows","text":"<pre><code>from txtai.app import Application\n\n# Create and run pipeline with workflow\napp = Application(\"config.yml\")\nlist(app.workflow(\"labels\", [\"Great news\", \"That's rough\"]))\n</code></pre>"},{"location":"pipeline/text/labels/#run-with-api","title":"Run with API","text":"<pre><code>CONFIG=config.yml uvicorn \"txtai.api:app\" &amp;\n\ncurl \\\n-X POST \"http://localhost:8000/workflow\" \\\n-H \"Content-Type: application/json\" \\\n-d '{\"name\":\"labels\", \"elements\": [\"Great news\", \"Thats rough\"]}'\n</code></pre>"},{"location":"pipeline/text/labels/#methods","title":"Methods","text":"<p>Python documentation for the pipeline.</p>"},{"location":"pipeline/text/labels/#txtai.pipeline.text.labels.Labels.__init__","title":"<code>__init__(self, path=None, quantize=False, gpu=True, model=None, dynamic=True)</code>  <code>special</code>","text":"Source code in <code>txtai/pipeline/text/labels.py</code> <pre><code>def __init__(self, path=None, quantize=False, gpu=True, model=None, dynamic=True):\n    super().__init__(\"zero-shot-classification\" if dynamic else \"text-classification\", path, quantize, gpu, model)\n\n    # Set if labels are dynamic (zero shot) or fixed (standard text classification)\n    self.dynamic = dynamic\n</code></pre>"},{"location":"pipeline/text/labels/#txtai.pipeline.text.labels.Labels.__call__","title":"<code>__call__(self, text, labels=None, multilabel=False, flatten=None, workers=0)</code>  <code>special</code>","text":"<p>Applies a text classifier to text. Returns a list of (id, score) sorted by highest score, where id is the index in labels. For zero shot classification, a list of labels is required. For text classification models, a list of labels is optional, otherwise all trained labels are returned.</p> <p>This method supports text as a string or a list. If the input is a string, the return type is a 1D list of (id, score). If text is a list, a 2D list of (id, score) is returned with a row per string.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <p>text|list</p> required <code>labels</code> <p>list of labels</p> <code>None</code> <code>multilabel</code> <p>labels are independent if True, scores are normalized to sum to 1 per text item if False, raw scores returned if None</p> <code>False</code> <code>flatten</code> <p>flatten output to a list of labels if present. Accepts a boolean or float value to only keep scores greater than that number.</p> <code>None</code> <code>workers</code> <p>number of concurrent workers to use for processing data, defaults to None</p> <code>0</code> <p>Returns:</p> Type Description <p>list of (id, score) or list of labels depending on flatten parameter</p> Source code in <code>txtai/pipeline/text/labels.py</code> <pre><code>def __call__(self, text, labels=None, multilabel=False, flatten=None, workers=0):\n\"\"\"\n    Applies a text classifier to text. Returns a list of (id, score) sorted by highest score,\n    where id is the index in labels. For zero shot classification, a list of labels is required.\n    For text classification models, a list of labels is optional, otherwise all trained labels are returned.\n\n    This method supports text as a string or a list. If the input is a string, the return\n    type is a 1D list of (id, score). If text is a list, a 2D list of (id, score) is\n    returned with a row per string.\n\n    Args:\n        text: text|list\n        labels: list of labels\n        multilabel: labels are independent if True, scores are normalized to sum to 1 per text item if False, raw scores returned if None\n        flatten: flatten output to a list of labels if present. Accepts a boolean or float value to only keep scores greater than that number.\n        workers: number of concurrent workers to use for processing data, defaults to None\n\n    Returns:\n        list of (id, score) or list of labels depending on flatten parameter\n    \"\"\"\n\n    if self.dynamic:\n        # Run zero shot classification pipeline\n        results = self.pipeline(text, labels, multi_label=multilabel, truncation=True, num_workers=workers)\n    else:\n        # Set classification function based on inputs\n        function = \"none\" if multilabel is None else \"sigmoid\" if multilabel or len(self.labels()) == 1 else \"softmax\"\n\n        # Run text classification pipeline\n        results = self.pipeline(text, top_k=None, function_to_apply=function, num_workers=workers)\n\n    # Convert results to a list if necessary\n    if isinstance(text, str):\n        results = [results]\n\n    # Build list of outputs and return\n    outputs = self.outputs(results, labels, flatten)\n    return outputs[0] if isinstance(text, str) else outputs\n</code></pre>"},{"location":"pipeline/text/sequences/","title":"Sequences","text":"<p>The Sequences pipeline runs text through a sequence-sequence model and generates output text.</p>"},{"location":"pipeline/text/sequences/#example","title":"Example","text":"<p>The following shows a simple example using this pipeline.</p> <pre><code>from txtai.pipeline import Sequences\n\n# Create and run pipeline\nsequences = Sequences()\nsequences(\"Hello, how are you?\", \"translate English to French: \")\n</code></pre> <p>See the link below for a more detailed example.</p> Notebook Description Query translation Domain-specific natural language queries with query translation"},{"location":"pipeline/text/sequences/#configuration-driven-example","title":"Configuration-driven example","text":"<p>Pipelines are run with Python or configuration. Pipelines can be instantiated in configuration using the lower case name of the pipeline. Configuration-driven pipelines are run with workflows or the API.</p>"},{"location":"pipeline/text/sequences/#configyml","title":"config.yml","text":"<pre><code># Create pipeline using lower case class name\nsequences:\n\n# Run pipeline with workflow\nworkflow:\nsequences:\ntasks:\n- action: sequences\nargs: [\"translate English to French: \"]\n</code></pre>"},{"location":"pipeline/text/sequences/#run-with-workflows","title":"Run with Workflows","text":"<pre><code>from txtai.app import Application\n\n# Create and run pipeline with workflow\napp = Application(\"config.yml\")\nlist(app.workflow(\"sequences\", [\"Hello, how are you?\"]))\n</code></pre>"},{"location":"pipeline/text/sequences/#run-with-api","title":"Run with API","text":"<pre><code>CONFIG=config.yml uvicorn \"txtai.api:app\" &amp;\n\ncurl \\\n-X POST \"http://localhost:8000/workflow\" \\\n-H \"Content-Type: application/json\" \\\n-d '{\"name\":\"sequences\", \"elements\": [\"Hello, how are you?\"]}'\n</code></pre>"},{"location":"pipeline/text/sequences/#methods","title":"Methods","text":"<p>Python documentation for the pipeline.</p>"},{"location":"pipeline/text/sequences/#txtai.pipeline.text.generator.Sequences.__init__","title":"<code>__init__(self, path=None, quantize=False, gpu=True, model=None)</code>  <code>special</code>","text":"Source code in <code>txtai/pipeline/text/generator.py</code> <pre><code>def __init__(self, path=None, quantize=False, gpu=True, model=None):\n    super().__init__(self.task(), path, quantize, gpu, model)\n</code></pre>"},{"location":"pipeline/text/sequences/#txtai.pipeline.text.generator.Sequences.__call__","title":"<code>__call__(self, text, prefix=None, maxlength=512, workers=0)</code>  <code>special</code>","text":"Source code in <code>txtai/pipeline/text/generator.py</code> <pre><code>def __call__(self, text, prefix=None, maxlength=512, workers=0):\n\"\"\"\n    Generates text using input text\n\n    Args:\n        text: text|list\n        prefix: optional prefix to prepend to text elements\n        maxlength: maximum sequence length\n        workers: number of concurrent workers to use for processing data, defaults to None\n\n    Returns:\n        generated text\n    \"\"\"\n\n    # List of texts\n    texts = text if isinstance(text, list) else [text]\n\n    # Add prefix, if necessary\n    if prefix:\n        texts = [f\"{prefix}{x}\" for x in texts]\n\n    # Run pipeline\n    results = self.pipeline(texts, max_length=maxlength, num_workers=workers)\n\n    # Get generated text\n    results = [self.clean(x) for x in results]\n\n    return results[0] if isinstance(text, str) else results\n</code></pre>"},{"location":"pipeline/text/similarity/","title":"Similarity","text":"<p>The Similarity pipeline computes similarity between queries and list of text using a text classifier.</p> <p>This pipeline supports both standard text classification models and zero-shot classification models. The pipeline uses the queries as labels for the input text. The results are transposed to get scores per query/label vs scores per input text. </p> <p>Cross-encoder models are supported via the <code>crossencode=True</code> constructor parameter. These models are loaded with a CrossEncoder pipeline that can also be instantiated directly. The CrossEncoder pipeline has the same methods and functionality as described below.</p>"},{"location":"pipeline/text/similarity/#example","title":"Example","text":"<p>The following shows a simple example using this pipeline.</p> <pre><code>from txtai.pipeline import Similarity\n\n# Create and run pipeline\nsimilarity = Similarity()\nsimilarity(\"feel good story\", [\n    \"Maine man wins $1M from $25 lottery ticket\", \n    \"Don't sacrifice slower friends in a bear attack\"\n])\n</code></pre> <p>See the link below for a more detailed example.</p> Notebook Description Add semantic search to Elasticsearch Add semantic search to existing search systems"},{"location":"pipeline/text/similarity/#configuration-driven-example","title":"Configuration-driven example","text":"<p>Pipelines are run with Python or configuration. Pipelines can be instantiated in configuration using the lower case name of the pipeline. Configuration-driven pipelines are run with workflows or the API.</p>"},{"location":"pipeline/text/similarity/#configyml","title":"config.yml","text":"<pre><code># Create pipeline using lower case class name\nsimilarity:\n</code></pre>"},{"location":"pipeline/text/similarity/#run-with-workflows","title":"Run with Workflows","text":"<pre><code>from txtai.app import Application\n\n# Create and run pipeline with workflow\napp = Application(\"config.yml\")\napp.similarity(\"feel good story\", [\n    \"Maine man wins $1M from $25 lottery ticket\", \n    \"Don't sacrifice slower friends in a bear attack\"\n])\n</code></pre>"},{"location":"pipeline/text/similarity/#run-with-api","title":"Run with API","text":"<pre><code>CONFIG=config.yml uvicorn \"txtai.api:app\" &amp;\n\ncurl \\\n-X POST \"http://localhost:8000/similarity\" \\\n-H \"Content-Type: application/json\" \\\n-d '{\"query\": \"feel good story\", \"texts\": [\"Maine man wins $1M from $25 lottery ticket\", \"Dont sacrifice slower friends in a bear attack\"]}'\n</code></pre>"},{"location":"pipeline/text/similarity/#methods","title":"Methods","text":"<p>Python documentation for the pipeline.</p>"},{"location":"pipeline/text/similarity/#txtai.pipeline.text.similarity.Similarity.__init__","title":"<code>__init__(self, path=None, quantize=False, gpu=True, model=None, dynamic=True, crossencode=False)</code>  <code>special</code>","text":"Source code in <code>txtai/pipeline/text/similarity.py</code> <pre><code>def __init__(self, path=None, quantize=False, gpu=True, model=None, dynamic=True, crossencode=False):\n    # Use zero-shot classification if dynamic is True and crossencode is False, otherwise use standard text classification\n    super().__init__(path, quantize, gpu, model, False if crossencode else dynamic)\n\n    # Load as a cross-encoder if crossencode set to True\n    self.crossencoder = CrossEncoder(model=self.pipeline) if crossencode else None\n</code></pre>"},{"location":"pipeline/text/similarity/#txtai.pipeline.text.similarity.Similarity.__call__","title":"<code>__call__(self, query, texts, multilabel=True)</code>  <code>special</code>","text":"<p>Computes the similarity between query and list of text. Returns a list of (id, score) sorted by highest score, where id is the index in texts.</p> <p>This method supports query as a string or a list. If the input is a string, the return type is a 1D list of (id, score). If text is a list, a 2D list of (id, score) is returned with a row per string.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <p>query text|list</p> required <code>texts</code> <p>list of text</p> required <code>multilabel</code> <p>labels are independent if True, scores are normalized to sum to 1 per text item if False, raw scores returned if None</p> <code>True</code> <p>Returns:</p> Type Description <p>list of (id, score)</p> Source code in <code>txtai/pipeline/text/similarity.py</code> <pre><code>def __call__(self, query, texts, multilabel=True):\n\"\"\"\n    Computes the similarity between query and list of text. Returns a list of\n    (id, score) sorted by highest score, where id is the index in texts.\n\n    This method supports query as a string or a list. If the input is a string,\n    the return type is a 1D list of (id, score). If text is a list, a 2D list\n    of (id, score) is returned with a row per string.\n\n    Args:\n        query: query text|list\n        texts: list of text\n        multilabel: labels are independent if True, scores are normalized to sum to 1 per text item if False, raw scores returned if None\n\n    Returns:\n        list of (id, score)\n    \"\"\"\n\n    if self.crossencoder:\n        # pylint: disable=E1102\n        return self.crossencoder(query, texts, multilabel)\n\n    # Call Labels pipeline for texts using input query as the candidate label\n    scores = super().__call__(texts, [query] if isinstance(query, str) else query, multilabel)\n\n    # Sort on query index id\n    scores = [[score for _, score in sorted(row)] for row in scores]\n\n    # Transpose axes to get a list of text scores for each query\n    scores = np.array(scores).T.tolist()\n\n    # Build list of (id, score) per query sorted by highest score\n    scores = [sorted(enumerate(row), key=lambda x: x[1], reverse=True) for row in scores]\n\n    return scores[0] if isinstance(query, str) else scores\n</code></pre>"},{"location":"pipeline/text/summary/","title":"Summary","text":"<p>The Summary pipeline summarizes text. This pipeline runs a text2text model that abstractively creates a summary of the input text.</p>"},{"location":"pipeline/text/summary/#example","title":"Example","text":"<p>The following shows a simple example using this pipeline.</p> <pre><code>from txtai.pipeline import Summary\n\n# Create and run pipeline\nsummary = Summary()\nsummary(\"Enter long, detailed text to summarize here\")\n</code></pre> <p>See the link below for a more detailed example.</p> Notebook Description Building abstractive text summaries Run abstractive text summarization"},{"location":"pipeline/text/summary/#configuration-driven-example","title":"Configuration-driven example","text":"<p>Pipelines are run with Python or configuration. Pipelines can be instantiated in configuration using the lower case name of the pipeline. Configuration-driven pipelines are run with workflows or the API.</p>"},{"location":"pipeline/text/summary/#configyml","title":"config.yml","text":"<pre><code># Create pipeline using lower case class name\nsummary:\n\n# Run pipeline with workflow\nworkflow:\nsummary:\ntasks:\n- action: summary\n</code></pre>"},{"location":"pipeline/text/summary/#run-with-workflows","title":"Run with Workflows","text":"<pre><code>from txtai.app import Application\n\n# Create and run pipeline with workflow\napp = Application(\"config.yml\")\nlist(app.workflow(\"summary\", [\"Enter long, detailed text to summarize here\"]))\n</code></pre>"},{"location":"pipeline/text/summary/#run-with-api","title":"Run with API","text":"<pre><code>CONFIG=config.yml uvicorn \"txtai.api:app\" &amp;\n\ncurl \\\n-X POST \"http://localhost:8000/workflow\" \\\n-H \"Content-Type: application/json\" \\\n-d '{\"name\":\"summary\", \"elements\":[\"Enter long, detailed text to summarize here\"]}'\n</code></pre>"},{"location":"pipeline/text/summary/#methods","title":"Methods","text":"<p>Python documentation for the pipeline.</p>"},{"location":"pipeline/text/summary/#txtai.pipeline.text.summary.Summary.__init__","title":"<code>__init__(self, path=None, quantize=False, gpu=True, model=None)</code>  <code>special</code>","text":"Source code in <code>txtai/pipeline/text/summary.py</code> <pre><code>def __init__(self, path=None, quantize=False, gpu=True, model=None):\n    super().__init__(\"summarization\", path, quantize, gpu, model)\n</code></pre>"},{"location":"pipeline/text/summary/#txtai.pipeline.text.summary.Summary.__call__","title":"<code>__call__(self, text, minlength=None, maxlength=None, workers=0)</code>  <code>special</code>","text":"<p>Runs a summarization model against a block of text.</p> <p>This method supports text as a string or a list. If the input is a string, the return type is text. If text is a list, a list of text is returned with a row per block of text.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <p>text|list</p> required <code>minlength</code> <p>minimum length for summary</p> <code>None</code> <code>maxlength</code> <p>maximum length for summary</p> <code>None</code> <code>workers</code> <p>number of concurrent workers to use for processing data, defaults to None</p> <code>0</code> <p>Returns:</p> Type Description <p>summary text</p> Source code in <code>txtai/pipeline/text/summary.py</code> <pre><code>def __call__(self, text, minlength=None, maxlength=None, workers=0):\n\"\"\"\n    Runs a summarization model against a block of text.\n\n    This method supports text as a string or a list. If the input is a string, the return\n    type is text. If text is a list, a list of text is returned with a row per block of text.\n\n    Args:\n        text: text|list\n        minlength: minimum length for summary\n        maxlength: maximum length for summary\n        workers: number of concurrent workers to use for processing data, defaults to None\n\n    Returns:\n        summary text\n    \"\"\"\n\n    # Validate text length greater than max length\n    check = maxlength if maxlength else self.maxlength()\n\n    # Skip text shorter than max length\n    texts = text if isinstance(text, list) else [text]\n    params = [(x, text if len(text) &gt;= check else None) for x, text in enumerate(texts)]\n\n    # Build keyword arguments\n    kwargs = self.args(minlength, maxlength)\n\n    inputs = [text for _, text in params if text]\n    if inputs:\n        # Run summarization pipeline\n        results = self.pipeline(inputs, num_workers=workers, **kwargs)\n\n        # Pull out summary text\n        results = iter([self.clean(x[\"summary_text\"]) for x in results])\n        results = [next(results) if text else texts[x] for x, text in params]\n    else:\n        # Return original\n        results = texts\n\n    return results[0] if isinstance(text, str) else results\n</code></pre>"},{"location":"pipeline/text/translation/","title":"Translation","text":"<p>The Translation pipeline translates text between languages. It supports over 100+ languages. Automatic source language detection is built-in. This pipeline detects the language of each input text row, loads a model for the source-target combination and translates text to the target language.</p>"},{"location":"pipeline/text/translation/#example","title":"Example","text":"<p>The following shows a simple example using this pipeline.</p> <pre><code>from txtai.pipeline import Translation\n\n# Create and run pipeline\ntranslate = Translation()\ntranslate(\"This is a test translation into Spanish\", \"es\")\n</code></pre> <p>See the link below for a more detailed example.</p> Notebook Description Translate text between languages Streamline machine translation and language detection"},{"location":"pipeline/text/translation/#configuration-driven-example","title":"Configuration-driven example","text":"<p>Pipelines are run with Python or configuration. Pipelines can be instantiated in configuration using the lower case name of the pipeline. Configuration-driven pipelines are run with workflows or the API.</p>"},{"location":"pipeline/text/translation/#configyml","title":"config.yml","text":"<pre><code># Create pipeline using lower case class name\ntranslation:\n\n# Run pipeline with workflow\nworkflow:\ntranslate:\ntasks:\n- action: translation\nargs: [\"es\"]\n</code></pre>"},{"location":"pipeline/text/translation/#run-with-workflows","title":"Run with Workflows","text":"<pre><code>from txtai.app import Application\n\n# Create and run pipeline with workflow\napp = Application(\"config.yml\")\nlist(app.workflow(\"translate\", [\"This is a test translation into Spanish\"]))\n</code></pre>"},{"location":"pipeline/text/translation/#run-with-api","title":"Run with API","text":"<pre><code>CONFIG=config.yml uvicorn \"txtai.api:app\" &amp;\n\ncurl \\\n-X POST \"http://localhost:8000/workflow\" \\\n-H \"Content-Type: application/json\" \\\n-d '{\"name\":\"translate\", \"elements\":[\"This is a test translation into Spanish\"]}'\n</code></pre>"},{"location":"pipeline/text/translation/#methods","title":"Methods","text":"<p>Python documentation for the pipeline.</p>"},{"location":"pipeline/text/translation/#txtai.pipeline.text.translation.Translation.__init__","title":"<code>__init__(self, path='facebook/m2m100_418M', quantize=False, gpu=True, batch=64, langdetect='https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.ftz', findmodels=True)</code>  <code>special</code>","text":"<p>Constructs a new language translation pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <p>optional path to model, accepts Hugging Face model hub id or local path,   uses default model for task if not provided</p> <code>'facebook/m2m100_418M'</code> <code>quantize</code> <p>if model should be quantized, defaults to False</p> <code>False</code> <code>gpu</code> <p>True/False if GPU should be enabled, also supports a GPU device id</p> <code>True</code> <code>batch</code> <p>batch size used to incrementally process content</p> <code>64</code> <code>langdetect</code> <p>path to language detection model, uses a default path if not provided</p> <code>'https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.ftz'</code> <code>findmodels</code> <p>True/False if the Hugging Face Hub will be searched for source-target translation models</p> <code>True</code> Source code in <code>txtai/pipeline/text/translation.py</code> <pre><code>def __init__(self, path=\"facebook/m2m100_418M\", quantize=False, gpu=True, batch=64, langdetect=DEFAULT_LANG_DETECT, findmodels=True):\n\"\"\"\n    Constructs a new language translation pipeline.\n\n    Args:\n        path: optional path to model, accepts Hugging Face model hub id or local path,\n              uses default model for task if not provided\n        quantize: if model should be quantized, defaults to False\n        gpu: True/False if GPU should be enabled, also supports a GPU device id\n        batch: batch size used to incrementally process content\n        langdetect: path to language detection model, uses a default path if not provided\n        findmodels: True/False if the Hugging Face Hub will be searched for source-target translation models\n    \"\"\"\n\n    # Call parent constructor\n    super().__init__(path, quantize, gpu, batch)\n\n    # Language detection\n    self.detector = None\n    self.langdetect = langdetect\n    self.findmodels = findmodels\n\n    # Language models\n    self.models = {}\n    self.ids = self.modelids()\n</code></pre>"},{"location":"pipeline/text/translation/#txtai.pipeline.text.translation.Translation.__call__","title":"<code>__call__(self, texts, target='en', source=None)</code>  <code>special</code>","text":"<p>Translates text from source language into target language.</p> <p>This method supports texts as a string or a list. If the input is a string, the return type is string. If text is a list, the return type is a list.</p> <p>Parameters:</p> Name Type Description Default <code>texts</code> <p>text|list</p> required <code>target</code> <p>target language code, defaults to \"en\"</p> <code>'en'</code> <code>source</code> <p>source language code, detects language if not provided</p> <code>None</code> <p>Returns:</p> Type Description <p>list of translated text</p> Source code in <code>txtai/pipeline/text/translation.py</code> <pre><code>def __call__(self, texts, target=\"en\", source=None):\n\"\"\"\n    Translates text from source language into target language.\n\n    This method supports texts as a string or a list. If the input is a string,\n    the return type is string. If text is a list, the return type is a list.\n\n    Args:\n        texts: text|list\n        target: target language code, defaults to \"en\"\n        source: source language code, detects language if not provided\n\n    Returns:\n        list of translated text\n    \"\"\"\n\n    values = [texts] if not isinstance(texts, list) else texts\n\n    # Detect source languages\n    languages = self.detect(values) if not source else [source] * len(values)\n    unique = set(languages)\n\n    # Build list of (index, language, text)\n    values = [(x, lang, values[x]) for x, lang in enumerate(languages)]\n\n    results = {}\n    for language in unique:\n        # Get all text values for language\n        inputs = [(x, text) for x, lang, text in values if lang == language]\n\n        # Translate text in batches\n        outputs = []\n        for chunk in self.batch([text for _, text in inputs], self.batchsize):\n            outputs.extend(self.translate(chunk, language, target))\n\n        # Store output value\n        for y, (x, _) in enumerate(inputs):\n            results[x] = outputs[y].strip()\n\n    # Return results in same order as input\n    results = [results[x] for x in sorted(results)]\n    return results[0] if isinstance(texts, str) else results\n</code></pre>"},{"location":"pipeline/train/hfonnx/","title":"HFOnnx","text":"<p>Exports a Hugging Face Transformer model to ONNX. Currently, this works best with classification/pooling/qa models. Work is ongoing for sequence to sequence models (summarization, transcription, translation).</p>"},{"location":"pipeline/train/hfonnx/#example","title":"Example","text":"<p>The following shows a simple example using this pipeline.</p> <pre><code>from txtai.pipeline import HFOnnx, Labels\n\n# Model path\npath = \"distilbert-base-uncased-finetuned-sst-2-english\"\n\n# Export model to ONNX\nonnx = HFOnnx()\nmodel = onnx(path, \"text-classification\", \"model.onnx\", True)\n\n# Run inference and validate\nlabels = Labels((model, path), dynamic=False)\nlabels(\"I am happy\")\n</code></pre> <p>See the link below for a more detailed example.</p> Notebook Description Export and run models with ONNX Export models with ONNX, run natively in JavaScript, Java and Rust"},{"location":"pipeline/train/hfonnx/#methods","title":"Methods","text":"<p>Python documentation for the pipeline.</p>"},{"location":"pipeline/train/hfonnx/#txtai.pipeline.train.hfonnx.HFOnnx.__call__","title":"<code>__call__(self, path, task='default', output=None, quantize=False, opset=12)</code>  <code>special</code>","text":"<p>Exports a Hugging Face Transformer model to ONNX.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <p>path to model, accepts Hugging Face model hub id, local path or (model, tokenizer) tuple</p> required <code>task</code> <p>optional model task or category, determines the model type and outputs, defaults to export hidden state</p> <code>'default'</code> <code>output</code> <p>optional output model path, defaults to return byte array if None</p> <code>None</code> <code>quantize</code> <p>if model should be quantized (requires onnx to be installed), defaults to False</p> <code>False</code> <code>opset</code> <p>onnx opset, defaults to 12</p> <code>12</code> <p>Returns:</p> Type Description <p>path to model output or model as bytes depending on output parameter</p> Source code in <code>txtai/pipeline/train/hfonnx.py</code> <pre><code>def __call__(self, path, task=\"default\", output=None, quantize=False, opset=12):\n\"\"\"\n    Exports a Hugging Face Transformer model to ONNX.\n\n    Args:\n        path: path to model, accepts Hugging Face model hub id, local path or (model, tokenizer) tuple\n        task: optional model task or category, determines the model type and outputs, defaults to export hidden state\n        output: optional output model path, defaults to return byte array if None\n        quantize: if model should be quantized (requires onnx to be installed), defaults to False\n        opset: onnx opset, defaults to 12\n\n    Returns:\n        path to model output or model as bytes depending on output parameter\n    \"\"\"\n\n    inputs, outputs, model = self.parameters(task)\n\n    if isinstance(path, (list, tuple)):\n        model, tokenizer = path\n        model = model.cpu()\n    else:\n        model = model(path)\n        tokenizer = AutoTokenizer.from_pretrained(path)\n\n    # Generate dummy inputs\n    dummy = dict(tokenizer([\"test inputs\"], return_tensors=\"pt\"))\n\n    # Default to BytesIO if no output file provided\n    output = output if output else BytesIO()\n\n    # Export model to ONNX\n    export(\n        model,\n        (dummy,),\n        output,\n        opset_version=opset,\n        do_constant_folding=True,\n        input_names=list(inputs.keys()),\n        output_names=list(outputs.keys()),\n        dynamic_axes=dict(chain(inputs.items(), outputs.items())),\n    )\n\n    # Quantize model\n    if quantize:\n        if not ONNX_RUNTIME:\n            raise ImportError('onnxruntime is not available - install \"pipeline\" extra to enable')\n\n        output = self.quantization(output)\n\n    if isinstance(output, BytesIO):\n        # Reset stream and return bytes\n        output.seek(0)\n        output = output.read()\n\n    return output\n</code></pre>"},{"location":"pipeline/train/mlonnx/","title":"MLOnnx","text":"<p>Exports a traditional machine learning model (i.e. scikit-learn) to ONNX.</p>"},{"location":"pipeline/train/mlonnx/#example","title":"Example","text":"<p>See the link below for a detailed example.</p> Notebook Description Export and run other machine learning models Export and run models from scikit-learn, PyTorch and more"},{"location":"pipeline/train/mlonnx/#methods","title":"Methods","text":"<p>Python documentation for the pipeline.</p>"},{"location":"pipeline/train/mlonnx/#txtai.pipeline.train.mlonnx.MLOnnx.__call__","title":"<code>__call__(self, model, task='default', output=None, opset=12)</code>  <code>special</code>","text":"<p>Exports a machine learning model to ONNX using ONNXMLTools.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <p>model to export</p> required <code>task</code> <p>optional model task or category</p> <code>'default'</code> <code>output</code> <p>optional output model path, defaults to return byte array if None</p> <code>None</code> <code>opset</code> <p>onnx opset, defaults to 12</p> <code>12</code> <p>Returns:</p> Type Description <p>path to model output or model as bytes depending on output parameter</p> Source code in <code>txtai/pipeline/train/mlonnx.py</code> <pre><code>def __call__(self, model, task=\"default\", output=None, opset=12):\n\"\"\"\n    Exports a machine learning model to ONNX using ONNXMLTools.\n\n    Args:\n        model: model to export\n        task: optional model task or category\n        output: optional output model path, defaults to return byte array if None\n        opset: onnx opset, defaults to 12\n\n    Returns:\n        path to model output or model as bytes depending on output parameter\n    \"\"\"\n\n    # Convert scikit-learn model to ONNX\n    model = convert_sklearn(model, task, initial_types=[(\"input_ids\", StringTensorType([None, None]))], target_opset=opset)\n\n    # Prune model graph down to only output probabilities\n    model = select_model_inputs_outputs(model, outputs=\"probabilities\")\n\n    # pylint: disable=E1101\n    # Rename output to logits for consistency with other models\n    model.graph.output[0].name = \"logits\"\n    model.graph.node[0].output[0] = \"logits\"\n\n    # Save model to specified output path or return bytes\n    model = save_onnx_model(model, output)\n    return output if output else model\n</code></pre>"},{"location":"pipeline/train/trainer/","title":"HFTrainer","text":"<p>Trains a new Hugging Face Transformer model using the Trainer framework.</p>"},{"location":"pipeline/train/trainer/#example","title":"Example","text":"<p>The following shows a simple example using this pipeline.</p> <pre><code>import pandas as pd\n\nfrom datasets import load_dataset\n\nfrom txtai.pipeline import HFTrainer\n\ntrainer = HFTrainer()\n\n# Pandas DataFrame\ndf = pd.read_csv(\"training.csv\")\nmodel, tokenizer = trainer(\"bert-base-uncased\", df)\n\n# Hugging Face dataset\nds = load_dataset(\"glue\", \"sst2\")\nmodel, tokenizer = trainer(\"bert-base-uncased\", ds[\"train\"], columns=(\"sentence\", \"label\"))\n\n# List of dicts\ndt = [{\"text\": \"sentence 1\", \"label\": 0}, {\"text\": \"sentence 2\", \"label\": 1}]]\nmodel, tokenizer = trainer(\"bert-base-uncased\", dt)\n\n# Support additional TrainingArguments\nmodel, tokenizer = trainer(\"bert-base-uncased\", dt, \n                            learning_rate=3e-5, num_train_epochs=5)\n</code></pre> <p>All TrainingArguments are supported as function arguments to the trainer call.</p> <p>See the links below for more detailed examples.</p> Notebook Description Train a text labeler Build text sequence classification models Train without labels Use zero-shot classifiers to train new models Train a QA model Build and fine-tune question-answering models Train a language model from scratch Build new language models"},{"location":"pipeline/train/trainer/#training-tasks","title":"Training tasks","text":"<p>The HFTrainer pipeline builds and/or fine-tunes models for following training tasks.</p> Task Description language-generation Causal language model for text generation (e.g. GPT) language-modeling Masked language model for general tasks (e.g. BERT) question-answering Extractive question-answering model, typically with the SQuAD dataset sequence-sequence Sequence-Sequence model (e.g. T5) text-classification Classify text with a set of labels token-detection ELECTRA-style pre-training with replaced token detection"},{"location":"pipeline/train/trainer/#methods","title":"Methods","text":"<p>Python documentation for the pipeline.</p>"},{"location":"pipeline/train/trainer/#txtai.pipeline.train.hftrainer.HFTrainer.__call__","title":"<code>__call__(self, base, train, validation=None, columns=None, maxlength=None, stride=128, task='text-classification', prefix=None, metrics=None, tokenizers=None, checkpoint=None, **args)</code>  <code>special</code>","text":"<p>Builds a new model using arguments.</p> <p>Parameters:</p> Name Type Description Default <code>base</code> <p>path to base model, accepts Hugging Face model hub id, local path or (model, tokenizer) tuple</p> required <code>train</code> <p>training data</p> required <code>validation</code> <p>validation data</p> <code>None</code> <code>columns</code> <p>tuple of columns to use for text/label, defaults to (text, None, label)</p> <code>None</code> <code>maxlength</code> <p>maximum sequence length, defaults to tokenizer.model_max_length</p> <code>None</code> <code>stride</code> <p>chunk size for splitting data for QA tasks</p> <code>128</code> <code>task</code> <p>optional model task or category, determines the model type, defaults to \"text-classification\"</p> <code>'text-classification'</code> <code>prefix</code> <p>optional source prefix</p> <code>None</code> <code>metrics</code> <p>optional function that computes and returns a dict of evaluation metrics</p> <code>None</code> <code>tokenizers</code> <p>optional number of concurrent tokenizers, defaults to None</p> <code>None</code> <code>checkpoint</code> <p>optional resume from checkpoint flag or path to checkpoint directory, defaults to None</p> <code>None</code> <code>args</code> <p>training arguments</p> <code>{}</code> <p>Returns:</p> Type Description <p>(model, tokenizer)</p> Source code in <code>txtai/pipeline/train/hftrainer.py</code> <pre><code>def __call__(\n    self,\n    base,\n    train,\n    validation=None,\n    columns=None,\n    maxlength=None,\n    stride=128,\n    task=\"text-classification\",\n    prefix=None,\n    metrics=None,\n    tokenizers=None,\n    checkpoint=None,\n    **args\n):\n\"\"\"\n    Builds a new model using arguments.\n\n    Args:\n        base: path to base model, accepts Hugging Face model hub id, local path or (model, tokenizer) tuple\n        train: training data\n        validation: validation data\n        columns: tuple of columns to use for text/label, defaults to (text, None, label)\n        maxlength: maximum sequence length, defaults to tokenizer.model_max_length\n        stride: chunk size for splitting data for QA tasks\n        task: optional model task or category, determines the model type, defaults to \"text-classification\"\n        prefix: optional source prefix\n        metrics: optional function that computes and returns a dict of evaluation metrics\n        tokenizers: optional number of concurrent tokenizers, defaults to None\n        checkpoint: optional resume from checkpoint flag or path to checkpoint directory, defaults to None\n        args: training arguments\n\n    Returns:\n        (model, tokenizer)\n    \"\"\"\n\n    # Parse TrainingArguments\n    args = self.parse(args)\n\n    # Set seed for model reproducibility\n    set_seed(args.seed)\n\n    # Load model configuration, tokenizer and max sequence length\n    config, tokenizer, maxlength = self.load(base, maxlength)\n\n    # Data collator and list of labels (only for classification models)\n    collator, labels = None, None\n\n    # Prepare datasets\n    if task == \"language-generation\":\n        # Default tokenizer pad token if it's not set\n        tokenizer.pad_token = tokenizer.pad_token if tokenizer.pad_token is not None else tokenizer.eos_token\n\n        process = Texts(tokenizer, columns, maxlength)\n        collator = DataCollatorForLanguageModeling(tokenizer, mlm=False, pad_to_multiple_of=8 if args.fp16 else None)\n    elif task in (\"language-modeling\", \"token-detection\"):\n        process = Texts(tokenizer, columns, maxlength)\n        collator = DataCollatorForLanguageModeling(tokenizer, pad_to_multiple_of=8 if args.fp16 else None)\n    elif task == \"question-answering\":\n        process = Questions(tokenizer, columns, maxlength, stride)\n    elif task == \"sequence-sequence\":\n        process = Sequences(tokenizer, columns, maxlength, prefix)\n        collator = DataCollatorForSeq2Seq(tokenizer, pad_to_multiple_of=8 if args.fp16 else None)\n    else:\n        process = Labels(tokenizer, columns, maxlength)\n        labels = process.labels(train)\n\n    # Tokenize training and validation data\n    train, validation = process(train, validation, os.cpu_count() if tokenizers and isinstance(tokenizers, bool) else tokenizers)\n\n    # Create model to train\n    model = self.model(task, base, config, labels, tokenizer)\n\n    # Add model to collator\n    if collator:\n        collator.model = model\n\n    # Build trainer\n    trainer = Trainer(\n        model=model,\n        tokenizer=tokenizer,\n        data_collator=collator,\n        args=args,\n        train_dataset=train,\n        eval_dataset=validation if validation else None,\n        compute_metrics=metrics,\n    )\n\n    # Run training\n    trainer.train(resume_from_checkpoint=checkpoint)\n\n    # Run evaluation\n    if validation:\n        trainer.evaluate()\n\n    # Save model outputs\n    if args.should_save:\n        trainer.save_model()\n        trainer.save_state()\n\n    # Put model in eval mode to disable weight updates and return (model, tokenizer)\n    return (model.eval(), tokenizer)\n</code></pre>"},{"location":"workflow/","title":"Workflow","text":"<p>Workflows are a simple yet powerful construct that takes a callable and returns elements. Workflows operate well with pipelines but can work with any callable object. Workflows are streaming and work on data in batches, allowing large volumes of data to be processed efficiently.</p> <p>Given that pipelines are callable objects, workflows enable efficient processing of pipeline data. Transformers models typically work with smaller batches of data, workflows are well suited to feed a series of transformers pipelines. </p> <p>An example of the most basic workflow:</p> <pre><code>workflow = Workflow([Task(lambda x: [y * 2 for y in x])])\nlist(workflow([1, 2, 3]))\n</code></pre> <p>This example multiplies each input value by 2 and returns transformed elements via a generator.</p> <p>Since workflows run as generators, output must be consumed for execution to occur. The following snippets show how output can be consumed.</p> <pre><code># Small dataset where output fits in memory\nlist(workflow(elements))\n\n# Large dataset\nfor output in workflow(elements):\n    function(output)\n\n# Large dataset where output is discarded\nfor _ in workflow(elements):\n    pass\n</code></pre> <p>Workflows are run with Python or configuration. Examples of both methods are shown below.</p>"},{"location":"workflow/#example","title":"Example","text":"<p>A full-featured example is shown below in Python. This workflow transcribes a set of audio files, translates the text into French and indexes the data.</p> <pre><code>from txtai.embeddings import Embeddings\nfrom txtai.pipeline import Transcription, Translation\nfrom txtai.workflow import FileTask, Task, Workflow\n\n# Embeddings instance\nembeddings = Embeddings({\n    \"path\": \"sentence-transformers/paraphrase-MiniLM-L3-v2\",\n    \"content\": True\n})\n\n# Transcription instance\ntranscribe = Transcription()\n\n# Translation instance\ntranslate = Translation()\n\ntasks = [\n    FileTask(transcribe, r\"\\.wav$\"),\n    Task(lambda x: translate(x, \"fr\"))\n]\n\n# List of files to process\ndata = [\n  \"US_tops_5_million.wav\",\n  \"Canadas_last_fully.wav\",\n  \"Beijing_mobilises.wav\",\n  \"The_National_Park.wav\",\n  \"Maine_man_wins_1_mil.wav\",\n  \"Make_huge_profits.wav\"\n]\n\n# Workflow that translate text to French\nworkflow = Workflow(tasks)\n\n# Index data\nembeddings.index((uid, text, None) for uid, text in enumerate(workflow(data)))\n\n# Search\nembeddings.search(\"wildlife\", 1)\n</code></pre>"},{"location":"workflow/#configuration-driven-example","title":"Configuration-driven example","text":"<p>Workflows can be defined using Python as shown above but they can also run with YAML configuration.</p> <pre><code>writable: true\nembeddings:\npath: sentence-transformers/paraphrase-MiniLM-L3-v2\ncontent: true\n\n# Transcribe audio to text\ntranscription:\n\n# Translate text between languages\ntranslation:\n\nworkflow:\nindex:\ntasks:\n- action: transcription\nselect: \"\\\\.wav$\"\ntask: file\n- action: translation\nargs: [\"fr\"]\n- action: index\n</code></pre> <pre><code># Create and run the workflow\nfrom txtai.app import Application\n\n# Create and run the workflow\napp = Application(\"workflow.yml\")\nlist(app.workflow(\"index\", [\n  \"US_tops_5_million.wav\",\n  \"Canadas_last_fully.wav\",\n  \"Beijing_mobilises.wav\",\n  \"The_National_Park.wav\",\n  \"Maine_man_wins_1_mil.wav\",\n  \"Make_huge_profits.wav\"\n]))\n\n# Search\napp.search(\"wildlife\")\n</code></pre> <p>The code above executes a workflow defined in the file <code>workflow.yml</code>. The API is used to run the workflow locally, there is minimal overhead running workflows in this manner. It's a matter of preference.</p> <p>See the following links for more information.</p> <ul> <li>Workflow Demo</li> <li>Workflow YAML Examples</li> <li>Workflow YAML Guide</li> </ul>"},{"location":"workflow/#methods","title":"Methods","text":"<p>Workflows are callable objects. Workflows take an input of iterable data elements and output iterable data elements. </p>"},{"location":"workflow/#txtai.workflow.base.Workflow.__init__","title":"<code>__init__(self, tasks, batch=100, workers=None, name=None)</code>  <code>special</code>","text":"<p>Creates a new workflow. Workflows are lists of tasks to execute.</p> <p>Parameters:</p> Name Type Description Default <code>tasks</code> <p>list of workflow tasks</p> required <code>batch</code> <p>how many items to process at a time, defaults to 100</p> <code>100</code> <code>workers</code> <p>number of concurrent workers</p> <code>None</code> <code>name</code> <p>workflow name</p> <code>None</code> Source code in <code>txtai/workflow/base.py</code> <pre><code>def __init__(self, tasks, batch=100, workers=None, name=None):\n\"\"\"\n    Creates a new workflow. Workflows are lists of tasks to execute.\n\n    Args:\n        tasks: list of workflow tasks\n        batch: how many items to process at a time, defaults to 100\n        workers: number of concurrent workers\n        name: workflow name\n    \"\"\"\n\n    self.tasks = tasks\n    self.batch = batch\n    self.workers = workers\n    self.name = name\n\n    # Set default number of executor workers to max number of actions in a task\n    self.workers = max(len(task.action) for task in self.tasks) if not self.workers else self.workers\n</code></pre>"},{"location":"workflow/#txtai.workflow.base.Workflow.__call__","title":"<code>__call__(self, elements)</code>  <code>special</code>","text":"<p>Executes a workflow for input elements. This method returns a generator that yields transformed data elements.</p> <p>Parameters:</p> Name Type Description Default <code>elements</code> <p>iterable data elements</p> required <p>Returns:</p> Type Description <p>generator that yields transformed data elements</p> Source code in <code>txtai/workflow/base.py</code> <pre><code>def __call__(self, elements):\n\"\"\"\n    Executes a workflow for input elements. This method returns a generator that yields transformed\n    data elements.\n\n    Args:\n        elements: iterable data elements\n\n    Returns:\n        generator that yields transformed data elements\n    \"\"\"\n\n    # Create execute instance for this run\n    with Execute(self.workers) as executor:\n        # Run task initializers\n        self.initialize()\n\n        # Process elements in batches\n        for batch in self.chunk(elements):\n            yield from self.process(batch, executor)\n\n        # Run task finalizers\n        self.finalize()\n</code></pre>"},{"location":"workflow/#txtai.workflow.base.Workflow.schedule","title":"<code>schedule(self, cron, elements, iterations=None)</code>","text":"<p>Schedules a workflow using a cron expression and elements.</p> <p>Parameters:</p> Name Type Description Default <code>cron</code> <p>cron expression</p> required <code>elements</code> <p>iterable data elements passed to workflow each call</p> required <code>iterations</code> <p>number of times to run workflow, defaults to run indefinitely</p> <code>None</code> Source code in <code>txtai/workflow/base.py</code> <pre><code>def schedule(self, cron, elements, iterations=None):\n\"\"\"\n    Schedules a workflow using a cron expression and elements.\n\n    Args:\n        cron: cron expression\n        elements: iterable data elements passed to workflow each call\n        iterations: number of times to run workflow, defaults to run indefinitely\n    \"\"\"\n\n    # Check that croniter is installed\n    if not CRONITER:\n        raise ImportError('Workflow scheduling is not available - install \"workflow\" extra to enable')\n\n    logger.info(\"'%s' scheduler started with schedule %s\", self.name, cron)\n\n    maxiterations = iterations\n    while iterations is None or iterations &gt; 0:\n        # Schedule using localtime\n        schedule = croniter(cron, datetime.now().astimezone()).get_next(datetime)\n        logger.info(\"'%s' next run scheduled for %s\", self.name, schedule.isoformat())\n        time.sleep(schedule.timestamp() - time.time())\n\n        # Run workflow\n        # pylint: disable=W0703\n        try:\n            for _ in self(elements):\n                pass\n        except Exception:\n            logger.error(traceback.format_exc())\n\n        # Decrement iterations remaining, if necessary\n        if iterations is not None:\n            iterations -= 1\n\n    logger.info(\"'%s' max iterations (%d) reached\", self.name, maxiterations)\n</code></pre>"},{"location":"workflow/#more-examples","title":"More examples","text":"<p>See this link for a full list of workflow examples.</p>"},{"location":"workflow/schedule/","title":"Schedule","text":"<p>Workflows can run on a repeating basis with schedules. This is suitable in cases where a workflow is run against a dynamically expanding input, like an API service or directory of files. </p> <p>The schedule method takes a cron expression, list of static elements (which dynamically expand i.e. API service, directory listing) and an optional maximum number of iterations.</p> <p>Below are a couple example cron expressions.</p> <pre><code># \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 minute (0 - 59)\n# | \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 hour (0 - 23)\n# | | \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 day of the month (1 - 31)\n# | | | \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 month (1 - 12)\n# | | | | \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500 day of the week (0 - 6)\n# | | | | | \u250c\u2500\u2500\u2500\u2500\u2500 second (0 - 59)\n# | | | | | |\n* * * * * *      # Run every second\n0/5 * * * *        # Run every 5 minutes\n0 0 1 * *        # Run monthly on 1st\n0 0 1 1 *        # Run on Jan 1 at 12am\n0 0 * * mon,wed  # Run Monday and Wednesday\n</code></pre>"},{"location":"workflow/schedule/#python","title":"Python","text":"<p>Simple workflow scheduled with Python.</p> <pre><code>workflow = Workflow(tasks)\nworkflow.schedule(\"0/5 * * * *\", elements)\n</code></pre> <p>See the link below for a more detailed example.</p> Notebook Description Workflow Scheduling Schedule workflows with cron expressions"},{"location":"workflow/schedule/#configuration","title":"Configuration","text":"<p>Simple workflow scheduled with configuration.</p> <pre><code>workflow:\nindex:\nschedule:\ncron: 0/5 * * * *\nelements: [...]\ntasks: [...]\n</code></pre> <pre><code># Create and run the workflow\nfrom txtai.app import Application\n\n# Create and run the workflow\napp = Application(\"workflow.yml\")\n\n# Wait for scheduled workflows\napp.wait()\n</code></pre> <p>See the links below for more information on cron expressions.</p> <ul> <li>cron overview</li> <li>croniter - library used by txtai</li> </ul>"},{"location":"workflow/task/","title":"Tasks","text":"<p>Workflows execute tasks. Tasks are callable objects with a number of parameters to control the processing of data at a given step. While similar to pipelines, tasks encapsulate processing and don't perform signficant transformations on their own. Tasks perform logic to prepare content for the underlying action(s).</p> <p>A simple task is shown below.</p> <pre><code>Task(lambda x: [y * 2 for y in x])\n</code></pre> <p>The task above executes the function above for all input elements.</p> <p>Tasks work well with pipelines, since pipelines are callable objects. The example below will summarize each input element.</p> <pre><code>summary = Summary()\nTask(summary)\n</code></pre> <p>Tasks can operate independently but work best with workflows, as workflows add large-scale stream processing.</p> <pre><code>summary = Summary()\ntask = Task(summary)\ntask([\"Very long text here\"])\n\nworkflow = Workflow([task])\nlist(workflow([\"Very long text here\"]))\n</code></pre> <p>Tasks can also be created with configuration as part of a workflow.</p> <pre><code>workflow:\ntasks:\n- action: summary </code></pre>"},{"location":"workflow/task/#txtai.workflow.task.base.Task.__init__","title":"<code>__init__(self, action=None, select=None, unpack=True, column=None, merge='hstack', initialize=None, finalize=None, concurrency=None, onetomany=True, **kwargs)</code>  <code>special</code>","text":"<p>Creates a new task. A task defines two methods, type of data it accepts and the action to execute for each data element. Action is a callable function or list of callable functions.</p> <p>Parameters:</p> Name Type Description Default <code>action</code> <p>action(s) to execute on each data element</p> <code>None</code> <code>select</code> <p>filter(s) used to select data to process</p> <code>None</code> <code>unpack</code> <p>if data elements should be unpacked or unwrapped from (id, data, tag) tuples</p> <code>True</code> <code>column</code> <p>column index to select if element is a tuple, defaults to all</p> <code>None</code> <code>merge</code> <p>merge mode for joining multi-action outputs, defaults to hstack</p> <code>'hstack'</code> <code>initialize</code> <p>action to execute before processing</p> <code>None</code> <code>finalize</code> <p>action to execute after processing</p> <code>None</code> <code>concurrency</code> <p>sets concurrency method when execute instance available          valid values: \"thread\" for thread-based concurrency, \"process\" for process-based concurrency</p> <code>None</code> <code>onetomany</code> <p>if one-to-many data transformations should be enabled, defaults to True</p> <code>True</code> <code>kwargs</code> <p>additional keyword arguments</p> <code>{}</code> Source code in <code>txtai/workflow/task/base.py</code> <pre><code>def __init__(\n    self,\n    action=None,\n    select=None,\n    unpack=True,\n    column=None,\n    merge=\"hstack\",\n    initialize=None,\n    finalize=None,\n    concurrency=None,\n    onetomany=True,\n    **kwargs,\n):\n\"\"\"\n    Creates a new task. A task defines two methods, type of data it accepts and the action to execute\n    for each data element. Action is a callable function or list of callable functions.\n\n    Args:\n        action: action(s) to execute on each data element\n        select: filter(s) used to select data to process\n        unpack: if data elements should be unpacked or unwrapped from (id, data, tag) tuples\n        column: column index to select if element is a tuple, defaults to all\n        merge: merge mode for joining multi-action outputs, defaults to hstack\n        initialize: action to execute before processing\n        finalize: action to execute after processing\n        concurrency: sets concurrency method when execute instance available\n                     valid values: \"thread\" for thread-based concurrency, \"process\" for process-based concurrency\n        onetomany: if one-to-many data transformations should be enabled, defaults to True\n        kwargs: additional keyword arguments\n    \"\"\"\n\n    # Standardize into list of actions\n    if not action:\n        action = []\n    elif not isinstance(action, list):\n        action = [action]\n\n    self.action = action\n    self.select = select\n    self.unpack = unpack\n    self.column = column\n    self.merge = merge\n    self.initialize = initialize\n    self.finalize = finalize\n    self.concurrency = concurrency\n    self.onetomany = onetomany\n\n    # Check for custom registration. Adds additional instance members and validates required dependencies available.\n    if hasattr(self, \"register\"):\n        self.register(**kwargs)\n    elif kwargs:\n        # Raise error if additional keyword arguments passed in without register method\n        kwargs = \", \".join(f\"'{kw}'\" for kw in kwargs)\n        raise TypeError(f\"__init__() got unexpected keyword arguments: {kwargs}\")\n</code></pre>"},{"location":"workflow/task/#multi-action-task-concurrency","title":"Multi-action task concurrency","text":"<p>The default processing mode is to run actions sequentially. Multiprocessing support is already built in at a number of levels. Any of the GPU models will maximize GPU utilization for example and even in CPU mode, concurrency is utilized. But there are still use cases for task action concurrency. For example, if the system has multiple GPUs, the task runs external sequential code, or the task has a large number of I/O tasks.</p> <p>In addition to sequential processing, multi-action tasks can run either multithreaded or with multiple processes. The advantages of each approach are discussed below.</p> <ul> <li> <p>multithreading - no overhead of creating separate processes or pickling data. But Python can only execute a single thread due the GIL, so this approach won't help with CPU bound actions. This method works well with I/O bound actions and GPU actions.</p> </li> <li> <p>multiprocessing - separate subprocesses are created and data is exchanged via pickling. This method can fully utilize all CPU cores since each process runs independently. This method works well with CPU bound actions.</p> </li> </ul> <p>More information on multiprocessing can be found in the Python documentation.</p>"},{"location":"workflow/task/#multi-action-task-merges","title":"Multi-action task merges","text":"<p>Multi-action tasks will generate parallel outputs for the input data. The task output can be merged together in a couple different ways.</p>"},{"location":"workflow/task/#txtai.workflow.task.base.Task.hstack","title":"<code>hstack(self, outputs)</code>","text":"<p>Merges outputs column-wise. Returns a list of tuples which will be interpreted as a one to one transformation.</p> <p>Column-wise merge example (2 actions)</p> <p>Inputs: [a, b, c]</p> <p>Outputs =&gt; [[a1, b1, c1], [a2, b2, c2]]</p> <p>Column Merge =&gt; [(a1, a2), (b1, b2), (c1, c2)]</p> <p>Parameters:</p> Name Type Description Default <code>outputs</code> <p>task outputs</p> required <p>Returns:</p> Type Description <p>list of aggregated/zipped outputs as tuples (column-wise)</p> Source code in <code>txtai/workflow/task/base.py</code> <pre><code>def hstack(self, outputs):\n\"\"\"\n    Merges outputs column-wise. Returns a list of tuples which will be interpreted as a one to one transformation.\n\n    Column-wise merge example (2 actions)\n\n      Inputs: [a, b, c]\n\n      Outputs =&gt; [[a1, b1, c1], [a2, b2, c2]]\n\n      Column Merge =&gt; [(a1, a2), (b1, b2), (c1, c2)]\n\n    Args:\n        outputs: task outputs\n\n    Returns:\n        list of aggregated/zipped outputs as tuples (column-wise)\n    \"\"\"\n\n    # If all outputs are numpy arrays, use native method\n    if all(isinstance(output, np.ndarray) for output in outputs):\n        return np.stack(outputs, axis=1)\n\n    # If all outputs are torch tensors, use native method\n    # pylint: disable=E1101\n    if all(torch.is_tensor(output) for output in outputs):\n        return torch.stack(outputs, axis=1)\n\n    return list(zip(*outputs))\n</code></pre>"},{"location":"workflow/task/#txtai.workflow.task.base.Task.vstack","title":"<code>vstack(self, outputs)</code>","text":"<p>Merges outputs row-wise. Returns a list of lists which will be interpreted as a one to many transformation.</p> <p>Row-wise merge example (2 actions)</p> <p>Inputs: [a, b, c]</p> <p>Outputs =&gt; [[a1, b1, c1], [a2, b2, c2]]</p> <p>Row Merge =&gt; [[a1, a2], [b1, b2], [c1, c2]] = [a1, a2, b1, b2, c1, c2]</p> <p>Parameters:</p> Name Type Description Default <code>outputs</code> <p>task outputs</p> required <p>Returns:</p> Type Description <p>list of aggregated/zipped outputs as one to many transforms (row-wise)</p> Source code in <code>txtai/workflow/task/base.py</code> <pre><code>def vstack(self, outputs):\n\"\"\"\n    Merges outputs row-wise. Returns a list of lists which will be interpreted as a one to many transformation.\n\n    Row-wise merge example (2 actions)\n\n      Inputs: [a, b, c]\n\n      Outputs =&gt; [[a1, b1, c1], [a2, b2, c2]]\n\n      Row Merge =&gt; [[a1, a2], [b1, b2], [c1, c2]] = [a1, a2, b1, b2, c1, c2]\n\n    Args:\n        outputs: task outputs\n\n    Returns:\n        list of aggregated/zipped outputs as one to many transforms (row-wise)\n    \"\"\"\n\n    # If all outputs are numpy arrays, use native method\n    if all(isinstance(output, np.ndarray) for output in outputs):\n        return np.concatenate(np.stack(outputs, axis=1))\n\n    # If all outputs are torch tensors, use native method\n    # pylint: disable=E1101\n    if all(torch.is_tensor(output) for output in outputs):\n        return torch.cat(tuple(torch.stack(outputs, axis=1)))\n\n    # Flatten into lists of outputs per input row. Wrap as one to many transformation.\n    merge = []\n    for x in zip(*outputs):\n        combine = []\n        for y in x:\n            if isinstance(y, list):\n                combine.extend(y)\n            else:\n                combine.append(y)\n\n        merge.append(OneToMany(combine))\n\n    return merge\n</code></pre>"},{"location":"workflow/task/#txtai.workflow.task.base.Task.concat","title":"<code>concat(self, outputs)</code>","text":"<p>Merges outputs column-wise and concats values together into a string. Returns a list of strings.</p> <p>Concat merge example (2 actions)</p> <p>Inputs: [a, b, c]</p> <p>Outputs =&gt; [[a1, b1, c1], [a2, b2, c2]]</p> <p>Concat Merge =&gt; [(a1, a2), (b1, b2), (c1, c2)] =&gt; [\"a1. a2\", \"b1. b2\", \"c1. c2\"]</p> <p>Parameters:</p> Name Type Description Default <code>outputs</code> <p>task outputs</p> required <p>Returns:</p> Type Description <p>list of concat outputs</p> Source code in <code>txtai/workflow/task/base.py</code> <pre><code>def concat(self, outputs):\n\"\"\"\n    Merges outputs column-wise and concats values together into a string. Returns a list of strings.\n\n    Concat merge example (2 actions)\n\n      Inputs: [a, b, c]\n\n      Outputs =&gt; [[a1, b1, c1], [a2, b2, c2]]\n\n      Concat Merge =&gt; [(a1, a2), (b1, b2), (c1, c2)] =&gt; [\"a1. a2\", \"b1. b2\", \"c1. c2\"]\n\n    Args:\n        outputs: task outputs\n\n    Returns:\n        list of concat outputs\n    \"\"\"\n\n    return [\". \".join([str(y) for y in x if y]) for x in self.hstack(outputs)]\n</code></pre>"},{"location":"workflow/task/#extract-task-output-columns","title":"Extract task output columns","text":"<p>With column-wise merging, each output row will be a tuple of output values for each task action. This can be fed as input to a downstream task and that task can have separate tasks work with each element.</p> <p>A simple example:</p> <pre><code>workflow = Workflow([Task(lambda x: [y * 3 for y in x], unpack=False, column=0)])\nlist(workflow([(2, 8)]))\n</code></pre> <p>For the example input tuple of (2, 2), the workflow will only select the first element (2) and run the task against that element. </p> <pre><code>workflow = Workflow([Task([lambda x: [y * 3 for y in x], \n                           lambda x: [y - 1 for y in x]],\n                           unpack=False, column={0:0, 1:1})])\nlist(workflow([(2, 8)]))\n</code></pre> <p>The example above applies a separate action to each input column. This simple construct can help build extremely powerful workflow graphs!</p>"},{"location":"workflow/task/console/","title":"Console Task","text":"<p>The Console Task prints task inputs and outputs to standard output. This task is mainly used for debugging and can be added at any point in a workflow.</p>"},{"location":"workflow/task/console/#example","title":"Example","text":"<p>The following shows a simple example using this task as part of a workflow.</p> <pre><code>from txtai.workflow import FileTask, Workflow\n\nworkflow = Workflow([ConsoleTask()])\nworkflow([\"Input 1\", \"Input2\"])\n</code></pre>"},{"location":"workflow/task/console/#configuration-driven-example","title":"Configuration-driven example","text":"<p>This task can also be created with workflow configuration.</p> <pre><code>workflow:\ntasks:\n- task: console\n</code></pre>"},{"location":"workflow/task/console/#methods","title":"Methods","text":"<p>Python documentation for the task.</p>"},{"location":"workflow/task/console/#txtai.workflow.task.base.ConsoleTask.__init__","title":"<code>__init__(self, action=None, select=None, unpack=True, column=None, merge='hstack', initialize=None, finalize=None, concurrency=None, onetomany=True, **kwargs)</code>  <code>special</code>","text":"Source code in <code>txtai/workflow/task/base.py</code> <pre><code>def __init__(\n    self,\n    action=None,\n    select=None,\n    unpack=True,\n    column=None,\n    merge=\"hstack\",\n    initialize=None,\n    finalize=None,\n    concurrency=None,\n    onetomany=True,\n    **kwargs,\n):\n\"\"\"\n    Creates a new task. A task defines two methods, type of data it accepts and the action to execute\n    for each data element. Action is a callable function or list of callable functions.\n\n    Args:\n        action: action(s) to execute on each data element\n        select: filter(s) used to select data to process\n        unpack: if data elements should be unpacked or unwrapped from (id, data, tag) tuples\n        column: column index to select if element is a tuple, defaults to all\n        merge: merge mode for joining multi-action outputs, defaults to hstack\n        initialize: action to execute before processing\n        finalize: action to execute after processing\n        concurrency: sets concurrency method when execute instance available\n                     valid values: \"thread\" for thread-based concurrency, \"process\" for process-based concurrency\n        onetomany: if one-to-many data transformations should be enabled, defaults to True\n        kwargs: additional keyword arguments\n    \"\"\"\n\n    # Standardize into list of actions\n    if not action:\n        action = []\n    elif not isinstance(action, list):\n        action = [action]\n\n    self.action = action\n    self.select = select\n    self.unpack = unpack\n    self.column = column\n    self.merge = merge\n    self.initialize = initialize\n    self.finalize = finalize\n    self.concurrency = concurrency\n    self.onetomany = onetomany\n\n    # Check for custom registration. Adds additional instance members and validates required dependencies available.\n    if hasattr(self, \"register\"):\n        self.register(**kwargs)\n    elif kwargs:\n        # Raise error if additional keyword arguments passed in without register method\n        kwargs = \", \".join(f\"'{kw}'\" for kw in kwargs)\n        raise TypeError(f\"__init__() got unexpected keyword arguments: {kwargs}\")\n</code></pre>"},{"location":"workflow/task/export/","title":"Export Task","text":"<p>The Export Task exports task outputs to CSV or Excel.</p>"},{"location":"workflow/task/export/#example","title":"Example","text":"<p>The following shows a simple example using this task as part of a workflow.</p> <pre><code>from txtai.workflow import FileTask, Workflow\n\nworkflow = Workflow([ExportTask()])\nworkflow([\"Input 1\", \"Input2\"])\n</code></pre>"},{"location":"workflow/task/export/#configuration-driven-example","title":"Configuration-driven example","text":"<p>This task can also be created with workflow configuration.</p> <pre><code>workflow:\ntasks:\n- task: export\n</code></pre>"},{"location":"workflow/task/export/#methods","title":"Methods","text":"<p>Python documentation for the task.</p>"},{"location":"workflow/task/export/#txtai.workflow.task.base.ExportTask.__init__","title":"<code>__init__(self, action=None, select=None, unpack=True, column=None, merge='hstack', initialize=None, finalize=None, concurrency=None, onetomany=True, **kwargs)</code>  <code>special</code>","text":"Source code in <code>txtai/workflow/task/base.py</code> <pre><code>def __init__(\n    self,\n    action=None,\n    select=None,\n    unpack=True,\n    column=None,\n    merge=\"hstack\",\n    initialize=None,\n    finalize=None,\n    concurrency=None,\n    onetomany=True,\n    **kwargs,\n):\n\"\"\"\n    Creates a new task. A task defines two methods, type of data it accepts and the action to execute\n    for each data element. Action is a callable function or list of callable functions.\n\n    Args:\n        action: action(s) to execute on each data element\n        select: filter(s) used to select data to process\n        unpack: if data elements should be unpacked or unwrapped from (id, data, tag) tuples\n        column: column index to select if element is a tuple, defaults to all\n        merge: merge mode for joining multi-action outputs, defaults to hstack\n        initialize: action to execute before processing\n        finalize: action to execute after processing\n        concurrency: sets concurrency method when execute instance available\n                     valid values: \"thread\" for thread-based concurrency, \"process\" for process-based concurrency\n        onetomany: if one-to-many data transformations should be enabled, defaults to True\n        kwargs: additional keyword arguments\n    \"\"\"\n\n    # Standardize into list of actions\n    if not action:\n        action = []\n    elif not isinstance(action, list):\n        action = [action]\n\n    self.action = action\n    self.select = select\n    self.unpack = unpack\n    self.column = column\n    self.merge = merge\n    self.initialize = initialize\n    self.finalize = finalize\n    self.concurrency = concurrency\n    self.onetomany = onetomany\n\n    # Check for custom registration. Adds additional instance members and validates required dependencies available.\n    if hasattr(self, \"register\"):\n        self.register(**kwargs)\n    elif kwargs:\n        # Raise error if additional keyword arguments passed in without register method\n        kwargs = \", \".join(f\"'{kw}'\" for kw in kwargs)\n        raise TypeError(f\"__init__() got unexpected keyword arguments: {kwargs}\")\n</code></pre>"},{"location":"workflow/task/export/#txtai.workflow.task.export.ExportTask.register","title":"<code>register(self, output=None, timestamp=None)</code>","text":"<p>Add export parameters to task. Checks if required dependencies are installed.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <p>output file path</p> <code>None</code> <code>timestamp</code> <p>true if output file should be timestamped</p> <code>None</code> Source code in <code>txtai/workflow/task/export.py</code> <pre><code>def register(self, output=None, timestamp=None):\n\"\"\"\n    Add export parameters to task. Checks if required dependencies are installed.\n\n    Args:\n        output: output file path\n        timestamp: true if output file should be timestamped\n    \"\"\"\n\n    if not PANDAS:\n        raise ImportError('ExportTask is not available - install \"workflow\" extra to enable')\n\n    # pylint: disable=W0201\n    self.output = output\n    self.timestamp = timestamp\n</code></pre>"},{"location":"workflow/task/file/","title":"File Task","text":"<p>The File Task validates a file exists. It handles both file paths and local file urls. Note that this task only works with local files.</p>"},{"location":"workflow/task/file/#example","title":"Example","text":"<p>The following shows a simple example using this task as part of a workflow.</p> <pre><code>from txtai.workflow import FileTask, Workflow\n\nworkflow = Workflow([FileTask()])\nworkflow([\"/path/to/file\", \"file:///path/to/file\"])\n</code></pre>"},{"location":"workflow/task/file/#configuration-driven-example","title":"Configuration-driven example","text":"<p>This task can also be created with workflow configuration.</p> <pre><code>workflow:\ntasks:\n- task: file\n</code></pre>"},{"location":"workflow/task/file/#methods","title":"Methods","text":"<p>Python documentation for the task.</p>"},{"location":"workflow/task/file/#txtai.workflow.task.base.FileTask.__init__","title":"<code>__init__(self, action=None, select=None, unpack=True, column=None, merge='hstack', initialize=None, finalize=None, concurrency=None, onetomany=True, **kwargs)</code>  <code>special</code>","text":"Source code in <code>txtai/workflow/task/base.py</code> <pre><code>def __init__(\n    self,\n    action=None,\n    select=None,\n    unpack=True,\n    column=None,\n    merge=\"hstack\",\n    initialize=None,\n    finalize=None,\n    concurrency=None,\n    onetomany=True,\n    **kwargs,\n):\n\"\"\"\n    Creates a new task. A task defines two methods, type of data it accepts and the action to execute\n    for each data element. Action is a callable function or list of callable functions.\n\n    Args:\n        action: action(s) to execute on each data element\n        select: filter(s) used to select data to process\n        unpack: if data elements should be unpacked or unwrapped from (id, data, tag) tuples\n        column: column index to select if element is a tuple, defaults to all\n        merge: merge mode for joining multi-action outputs, defaults to hstack\n        initialize: action to execute before processing\n        finalize: action to execute after processing\n        concurrency: sets concurrency method when execute instance available\n                     valid values: \"thread\" for thread-based concurrency, \"process\" for process-based concurrency\n        onetomany: if one-to-many data transformations should be enabled, defaults to True\n        kwargs: additional keyword arguments\n    \"\"\"\n\n    # Standardize into list of actions\n    if not action:\n        action = []\n    elif not isinstance(action, list):\n        action = [action]\n\n    self.action = action\n    self.select = select\n    self.unpack = unpack\n    self.column = column\n    self.merge = merge\n    self.initialize = initialize\n    self.finalize = finalize\n    self.concurrency = concurrency\n    self.onetomany = onetomany\n\n    # Check for custom registration. Adds additional instance members and validates required dependencies available.\n    if hasattr(self, \"register\"):\n        self.register(**kwargs)\n    elif kwargs:\n        # Raise error if additional keyword arguments passed in without register method\n        kwargs = \", \".join(f\"'{kw}'\" for kw in kwargs)\n        raise TypeError(f\"__init__() got unexpected keyword arguments: {kwargs}\")\n</code></pre>"},{"location":"workflow/task/image/","title":"Image Task","text":"<p>The Image Task reads file paths, check the file is an image and opens it as an Image object. Note that this task only works with local files.</p>"},{"location":"workflow/task/image/#example","title":"Example","text":"<p>The following shows a simple example using this task as part of a workflow.</p> <pre><code>from txtai.workflow import ImageTask, Workflow\n\nworkflow = Workflow([ImageTask()])\nworkflow([\"image.jpg\", \"image.gif\"])\n</code></pre>"},{"location":"workflow/task/image/#configuration-driven-example","title":"Configuration-driven example","text":"<p>This task can also be created with workflow configuration.</p> <pre><code>workflow:\ntasks:\n- task: image\n</code></pre>"},{"location":"workflow/task/image/#methods","title":"Methods","text":"<p>Python documentation for the task.</p>"},{"location":"workflow/task/image/#txtai.workflow.task.base.ImageTask.__init__","title":"<code>__init__(self, action=None, select=None, unpack=True, column=None, merge='hstack', initialize=None, finalize=None, concurrency=None, onetomany=True, **kwargs)</code>  <code>special</code>","text":"Source code in <code>txtai/workflow/task/base.py</code> <pre><code>def __init__(\n    self,\n    action=None,\n    select=None,\n    unpack=True,\n    column=None,\n    merge=\"hstack\",\n    initialize=None,\n    finalize=None,\n    concurrency=None,\n    onetomany=True,\n    **kwargs,\n):\n\"\"\"\n    Creates a new task. A task defines two methods, type of data it accepts and the action to execute\n    for each data element. Action is a callable function or list of callable functions.\n\n    Args:\n        action: action(s) to execute on each data element\n        select: filter(s) used to select data to process\n        unpack: if data elements should be unpacked or unwrapped from (id, data, tag) tuples\n        column: column index to select if element is a tuple, defaults to all\n        merge: merge mode for joining multi-action outputs, defaults to hstack\n        initialize: action to execute before processing\n        finalize: action to execute after processing\n        concurrency: sets concurrency method when execute instance available\n                     valid values: \"thread\" for thread-based concurrency, \"process\" for process-based concurrency\n        onetomany: if one-to-many data transformations should be enabled, defaults to True\n        kwargs: additional keyword arguments\n    \"\"\"\n\n    # Standardize into list of actions\n    if not action:\n        action = []\n    elif not isinstance(action, list):\n        action = [action]\n\n    self.action = action\n    self.select = select\n    self.unpack = unpack\n    self.column = column\n    self.merge = merge\n    self.initialize = initialize\n    self.finalize = finalize\n    self.concurrency = concurrency\n    self.onetomany = onetomany\n\n    # Check for custom registration. Adds additional instance members and validates required dependencies available.\n    if hasattr(self, \"register\"):\n        self.register(**kwargs)\n    elif kwargs:\n        # Raise error if additional keyword arguments passed in without register method\n        kwargs = \", \".join(f\"'{kw}'\" for kw in kwargs)\n        raise TypeError(f\"__init__() got unexpected keyword arguments: {kwargs}\")\n</code></pre>"},{"location":"workflow/task/retrieve/","title":"Retrieve Task","text":"<p>The Retrieve Task connects to a url and downloads the content locally. This task is helpful when working with actions that require data to be available locally.</p>"},{"location":"workflow/task/retrieve/#example","title":"Example","text":"<p>The following shows a simple example using this task as part of a workflow.</p> <pre><code>from txtai.workflow import RetrieveTask, Workflow\n\nworkflow = Workflow([RetrieveTask(directory=\"/tmp\")])\nworkflow([\"https://file.to.download\", \"/local/file/to/copy\"])\n</code></pre>"},{"location":"workflow/task/retrieve/#configuration-driven-example","title":"Configuration-driven example","text":"<p>This task can also be created with workflow configuration.</p> <pre><code>workflow:\ntasks:\n- task: retrieve\ndirectory: /tmp\n</code></pre>"},{"location":"workflow/task/retrieve/#methods","title":"Methods","text":"<p>Python documentation for the task.</p>"},{"location":"workflow/task/retrieve/#txtai.workflow.task.base.RetrieveTask.__init__","title":"<code>__init__(self, action=None, select=None, unpack=True, column=None, merge='hstack', initialize=None, finalize=None, concurrency=None, onetomany=True, **kwargs)</code>  <code>special</code>","text":"Source code in <code>txtai/workflow/task/base.py</code> <pre><code>def __init__(\n    self,\n    action=None,\n    select=None,\n    unpack=True,\n    column=None,\n    merge=\"hstack\",\n    initialize=None,\n    finalize=None,\n    concurrency=None,\n    onetomany=True,\n    **kwargs,\n):\n\"\"\"\n    Creates a new task. A task defines two methods, type of data it accepts and the action to execute\n    for each data element. Action is a callable function or list of callable functions.\n\n    Args:\n        action: action(s) to execute on each data element\n        select: filter(s) used to select data to process\n        unpack: if data elements should be unpacked or unwrapped from (id, data, tag) tuples\n        column: column index to select if element is a tuple, defaults to all\n        merge: merge mode for joining multi-action outputs, defaults to hstack\n        initialize: action to execute before processing\n        finalize: action to execute after processing\n        concurrency: sets concurrency method when execute instance available\n                     valid values: \"thread\" for thread-based concurrency, \"process\" for process-based concurrency\n        onetomany: if one-to-many data transformations should be enabled, defaults to True\n        kwargs: additional keyword arguments\n    \"\"\"\n\n    # Standardize into list of actions\n    if not action:\n        action = []\n    elif not isinstance(action, list):\n        action = [action]\n\n    self.action = action\n    self.select = select\n    self.unpack = unpack\n    self.column = column\n    self.merge = merge\n    self.initialize = initialize\n    self.finalize = finalize\n    self.concurrency = concurrency\n    self.onetomany = onetomany\n\n    # Check for custom registration. Adds additional instance members and validates required dependencies available.\n    if hasattr(self, \"register\"):\n        self.register(**kwargs)\n    elif kwargs:\n        # Raise error if additional keyword arguments passed in without register method\n        kwargs = \", \".join(f\"'{kw}'\" for kw in kwargs)\n        raise TypeError(f\"__init__() got unexpected keyword arguments: {kwargs}\")\n</code></pre>"},{"location":"workflow/task/retrieve/#txtai.workflow.task.retrieve.RetrieveTask.register","title":"<code>register(self, directory=None)</code>","text":"<p>Adds retrieve parameters to task.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <p>local directory used to store retrieved files</p> <code>None</code> Source code in <code>txtai/workflow/task/retrieve.py</code> <pre><code>def register(self, directory=None):\n\"\"\"\n    Adds retrieve parameters to task.\n\n    Args:\n        directory: local directory used to store retrieved files\n    \"\"\"\n\n    # pylint: disable=W0201\n    # Create default temporary directory if not specified\n    if not directory:\n        # Save tempdir to prevent content from being deleted until this task is out of scope\n        # pylint: disable=R1732\n        self.tempdir = tempfile.TemporaryDirectory()\n        directory = self.tempdir.name\n\n    # Create output directory if necessary\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n\n    self.directory = directory\n</code></pre>"},{"location":"workflow/task/service/","title":"Service Task","text":"<p>The Service Task extracts content from a http service.</p>"},{"location":"workflow/task/service/#example","title":"Example","text":"<p>The following shows a simple example using this task as part of a workflow.</p> <pre><code>from txtai.workflow import ServiceTask, Workflow\n\nworkflow = Workflow([ServiceTask(url=\"https://service.url/action)])\nworkflow([\"parameter\"])\n</code></pre>"},{"location":"workflow/task/service/#configuration-driven-example","title":"Configuration-driven example","text":"<p>This task can also be created with workflow configuration.</p> <pre><code>workflow:\ntasks:\n- task: service\nurl: https://service.url/action\n</code></pre>"},{"location":"workflow/task/service/#methods","title":"Methods","text":"<p>Python documentation for the task.</p>"},{"location":"workflow/task/service/#txtai.workflow.task.base.ServiceTask.__init__","title":"<code>__init__(self, action=None, select=None, unpack=True, column=None, merge='hstack', initialize=None, finalize=None, concurrency=None, onetomany=True, **kwargs)</code>  <code>special</code>","text":"Source code in <code>txtai/workflow/task/base.py</code> <pre><code>def __init__(\n    self,\n    action=None,\n    select=None,\n    unpack=True,\n    column=None,\n    merge=\"hstack\",\n    initialize=None,\n    finalize=None,\n    concurrency=None,\n    onetomany=True,\n    **kwargs,\n):\n\"\"\"\n    Creates a new task. A task defines two methods, type of data it accepts and the action to execute\n    for each data element. Action is a callable function or list of callable functions.\n\n    Args:\n        action: action(s) to execute on each data element\n        select: filter(s) used to select data to process\n        unpack: if data elements should be unpacked or unwrapped from (id, data, tag) tuples\n        column: column index to select if element is a tuple, defaults to all\n        merge: merge mode for joining multi-action outputs, defaults to hstack\n        initialize: action to execute before processing\n        finalize: action to execute after processing\n        concurrency: sets concurrency method when execute instance available\n                     valid values: \"thread\" for thread-based concurrency, \"process\" for process-based concurrency\n        onetomany: if one-to-many data transformations should be enabled, defaults to True\n        kwargs: additional keyword arguments\n    \"\"\"\n\n    # Standardize into list of actions\n    if not action:\n        action = []\n    elif not isinstance(action, list):\n        action = [action]\n\n    self.action = action\n    self.select = select\n    self.unpack = unpack\n    self.column = column\n    self.merge = merge\n    self.initialize = initialize\n    self.finalize = finalize\n    self.concurrency = concurrency\n    self.onetomany = onetomany\n\n    # Check for custom registration. Adds additional instance members and validates required dependencies available.\n    if hasattr(self, \"register\"):\n        self.register(**kwargs)\n    elif kwargs:\n        # Raise error if additional keyword arguments passed in without register method\n        kwargs = \", \".join(f\"'{kw}'\" for kw in kwargs)\n        raise TypeError(f\"__init__() got unexpected keyword arguments: {kwargs}\")\n</code></pre>"},{"location":"workflow/task/service/#txtai.workflow.task.service.ServiceTask.register","title":"<code>register(self, url=None, method=None, params=None, batch=True, extract=None)</code>","text":"<p>Adds service parameters to task. Checks if required dependencies are installed.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <p>url to connect to</p> <code>None</code> <code>method</code> <p>http method, GET or POST</p> <code>None</code> <code>params</code> <p>default query parameters</p> <code>None</code> <code>batch</code> <p>if True, all elements are passed in a single batch request, otherwise a service call is executed per element</p> <code>True</code> <code>extract</code> <p>list of sections to extract from response</p> <code>None</code> Source code in <code>txtai/workflow/task/service.py</code> <pre><code>def register(self, url=None, method=None, params=None, batch=True, extract=None):\n\"\"\"\n    Adds service parameters to task. Checks if required dependencies are installed.\n\n    Args:\n        url: url to connect to\n        method: http method, GET or POST\n        params: default query parameters\n        batch: if True, all elements are passed in a single batch request, otherwise a service call is executed per element\n        extract: list of sections to extract from response\n    \"\"\"\n\n    if not XML_TO_DICT:\n        raise ImportError('ServiceTask is not available - install \"workflow\" extra to enable')\n\n    # pylint: disable=W0201\n    # Save URL, method and parameter defaults\n    self.url = url\n    self.method = method\n    self.params = params\n\n    # If True, all elements are passed in a single batch request, otherwise a service call is executed per element\n    self.batch = batch\n\n    # Save sections to extract. Supports both a single string and a hierarchical list of sections.\n    self.extract = extract\n    if self.extract:\n        self.extract = [self.extract] if isinstance(self.extract, str) else self.extract\n</code></pre>"},{"location":"workflow/task/storage/","title":"Storage Task","text":"<p>The Storage Task expands a local directory or cloud storage bucket into a list of URLs to process.</p>"},{"location":"workflow/task/storage/#example","title":"Example","text":"<p>The following shows a simple example using this task as part of a workflow.</p> <pre><code>from txtai.workflow import StorageTask, Workflow\n\nworkflow = Workflow([StorageTask()])\nworkflow([\"s3://path/to/bucket\", \"local://local/directory\"])\n</code></pre>"},{"location":"workflow/task/storage/#configuration-driven-example","title":"Configuration-driven example","text":"<p>This task can also be created with workflow configuration.</p> <pre><code>workflow:\ntasks:\n- task: storage\n</code></pre>"},{"location":"workflow/task/storage/#methods","title":"Methods","text":"<p>Python documentation for the task.</p>"},{"location":"workflow/task/storage/#txtai.workflow.task.base.StorageTask.__init__","title":"<code>__init__(self, action=None, select=None, unpack=True, column=None, merge='hstack', initialize=None, finalize=None, concurrency=None, onetomany=True, **kwargs)</code>  <code>special</code>","text":"Source code in <code>txtai/workflow/task/base.py</code> <pre><code>def __init__(\n    self,\n    action=None,\n    select=None,\n    unpack=True,\n    column=None,\n    merge=\"hstack\",\n    initialize=None,\n    finalize=None,\n    concurrency=None,\n    onetomany=True,\n    **kwargs,\n):\n\"\"\"\n    Creates a new task. A task defines two methods, type of data it accepts and the action to execute\n    for each data element. Action is a callable function or list of callable functions.\n\n    Args:\n        action: action(s) to execute on each data element\n        select: filter(s) used to select data to process\n        unpack: if data elements should be unpacked or unwrapped from (id, data, tag) tuples\n        column: column index to select if element is a tuple, defaults to all\n        merge: merge mode for joining multi-action outputs, defaults to hstack\n        initialize: action to execute before processing\n        finalize: action to execute after processing\n        concurrency: sets concurrency method when execute instance available\n                     valid values: \"thread\" for thread-based concurrency, \"process\" for process-based concurrency\n        onetomany: if one-to-many data transformations should be enabled, defaults to True\n        kwargs: additional keyword arguments\n    \"\"\"\n\n    # Standardize into list of actions\n    if not action:\n        action = []\n    elif not isinstance(action, list):\n        action = [action]\n\n    self.action = action\n    self.select = select\n    self.unpack = unpack\n    self.column = column\n    self.merge = merge\n    self.initialize = initialize\n    self.finalize = finalize\n    self.concurrency = concurrency\n    self.onetomany = onetomany\n\n    # Check for custom registration. Adds additional instance members and validates required dependencies available.\n    if hasattr(self, \"register\"):\n        self.register(**kwargs)\n    elif kwargs:\n        # Raise error if additional keyword arguments passed in without register method\n        kwargs = \", \".join(f\"'{kw}'\" for kw in kwargs)\n        raise TypeError(f\"__init__() got unexpected keyword arguments: {kwargs}\")\n</code></pre>"},{"location":"workflow/task/url/","title":"Url Task","text":"<p>The Url Task validates that inputs start with a url prefix.</p>"},{"location":"workflow/task/url/#example","title":"Example","text":"<p>The following shows a simple example using this task as part of a workflow.</p> <pre><code>from txtai.workflow import UrlTask, Workflow\n\nworkflow = Workflow([UrlTask()])\nworkflow([\"https://file.to.download\", \"file:////local/file/to/copy\"])\n</code></pre>"},{"location":"workflow/task/url/#configuration-driven-example","title":"Configuration-driven example","text":"<p>This task can also be created with workflow configuration.</p> <pre><code>workflow:\ntasks:\n- task: url\n</code></pre>"},{"location":"workflow/task/url/#methods","title":"Methods","text":"<p>Python documentation for the task.</p>"},{"location":"workflow/task/url/#txtai.workflow.task.base.UrlTask.__init__","title":"<code>__init__(self, action=None, select=None, unpack=True, column=None, merge='hstack', initialize=None, finalize=None, concurrency=None, onetomany=True, **kwargs)</code>  <code>special</code>","text":"Source code in <code>txtai/workflow/task/base.py</code> <pre><code>def __init__(\n    self,\n    action=None,\n    select=None,\n    unpack=True,\n    column=None,\n    merge=\"hstack\",\n    initialize=None,\n    finalize=None,\n    concurrency=None,\n    onetomany=True,\n    **kwargs,\n):\n\"\"\"\n    Creates a new task. A task defines two methods, type of data it accepts and the action to execute\n    for each data element. Action is a callable function or list of callable functions.\n\n    Args:\n        action: action(s) to execute on each data element\n        select: filter(s) used to select data to process\n        unpack: if data elements should be unpacked or unwrapped from (id, data, tag) tuples\n        column: column index to select if element is a tuple, defaults to all\n        merge: merge mode for joining multi-action outputs, defaults to hstack\n        initialize: action to execute before processing\n        finalize: action to execute after processing\n        concurrency: sets concurrency method when execute instance available\n                     valid values: \"thread\" for thread-based concurrency, \"process\" for process-based concurrency\n        onetomany: if one-to-many data transformations should be enabled, defaults to True\n        kwargs: additional keyword arguments\n    \"\"\"\n\n    # Standardize into list of actions\n    if not action:\n        action = []\n    elif not isinstance(action, list):\n        action = [action]\n\n    self.action = action\n    self.select = select\n    self.unpack = unpack\n    self.column = column\n    self.merge = merge\n    self.initialize = initialize\n    self.finalize = finalize\n    self.concurrency = concurrency\n    self.onetomany = onetomany\n\n    # Check for custom registration. Adds additional instance members and validates required dependencies available.\n    if hasattr(self, \"register\"):\n        self.register(**kwargs)\n    elif kwargs:\n        # Raise error if additional keyword arguments passed in without register method\n        kwargs = \", \".join(f\"'{kw}'\" for kw in kwargs)\n        raise TypeError(f\"__init__() got unexpected keyword arguments: {kwargs}\")\n</code></pre>"},{"location":"workflow/task/workflow/","title":"Workflow Task","text":"<p>The Workflow Task runs a workflow. Allows creating workflows of workflows.</p>"},{"location":"workflow/task/workflow/#example","title":"Example","text":"<p>The following shows a simple example using this task as part of a workflow.</p> <pre><code>from txtai.workflow import WorkflowTask, Workflow\n\nworkflow = Workflow([WorkflowTask(otherworkflow)])\nworkflow([\"input data\"])\n</code></pre>"},{"location":"workflow/task/workflow/#methods","title":"Methods","text":"<p>Python documentation for the task.</p>"},{"location":"workflow/task/workflow/#txtai.workflow.task.base.WorkflowTask.__init__","title":"<code>__init__(self, action=None, select=None, unpack=True, column=None, merge='hstack', initialize=None, finalize=None, concurrency=None, onetomany=True, **kwargs)</code>  <code>special</code>","text":"Source code in <code>txtai/workflow/task/base.py</code> <pre><code>def __init__(\n    self,\n    action=None,\n    select=None,\n    unpack=True,\n    column=None,\n    merge=\"hstack\",\n    initialize=None,\n    finalize=None,\n    concurrency=None,\n    onetomany=True,\n    **kwargs,\n):\n\"\"\"\n    Creates a new task. A task defines two methods, type of data it accepts and the action to execute\n    for each data element. Action is a callable function or list of callable functions.\n\n    Args:\n        action: action(s) to execute on each data element\n        select: filter(s) used to select data to process\n        unpack: if data elements should be unpacked or unwrapped from (id, data, tag) tuples\n        column: column index to select if element is a tuple, defaults to all\n        merge: merge mode for joining multi-action outputs, defaults to hstack\n        initialize: action to execute before processing\n        finalize: action to execute after processing\n        concurrency: sets concurrency method when execute instance available\n                     valid values: \"thread\" for thread-based concurrency, \"process\" for process-based concurrency\n        onetomany: if one-to-many data transformations should be enabled, defaults to True\n        kwargs: additional keyword arguments\n    \"\"\"\n\n    # Standardize into list of actions\n    if not action:\n        action = []\n    elif not isinstance(action, list):\n        action = [action]\n\n    self.action = action\n    self.select = select\n    self.unpack = unpack\n    self.column = column\n    self.merge = merge\n    self.initialize = initialize\n    self.finalize = finalize\n    self.concurrency = concurrency\n    self.onetomany = onetomany\n\n    # Check for custom registration. Adds additional instance members and validates required dependencies available.\n    if hasattr(self, \"register\"):\n        self.register(**kwargs)\n    elif kwargs:\n        # Raise error if additional keyword arguments passed in without register method\n        kwargs = \", \".join(f\"'{kw}'\" for kw in kwargs)\n        raise TypeError(f\"__init__() got unexpected keyword arguments: {kwargs}\")\n</code></pre>"}]}